{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Over the years, LinkedIn's data infrastructure team built custom solutions for ingesting diverse data entities into our Hadoop eco-system. At one point, we were running 15 types of ingestion pipelines which created significant data quality, metadata management, development, and operation challenges. Our experiences and challenges motivated us to build Gobblin . Gobblin is a universal data ingestion framework for extracting, transforming, and loading large volume of data from a variety of data sources, e.g., databases, rest APIs, FTP/SFTP servers, filers, etc., onto Hadoop. Gobblin handles the common routine tasks required for all data ingestion ETLs, including job/task scheduling, task partitioning, error handling, state management, data quality checking, data publishing, etc. Gobblin ingests data from different data sources in the same execution framework, and manages metadata of different sources all in one place. This, combined with other features such as auto scalability, fault tolerance, data quality assurance, extensibility, and the ability of handling data model evolution, makes Gobblin an easy-to-use, self-serving, and efficient data ingestion framework. You can find a lot of useful resources in our wiki pages, including how to get started with Gobblin , an architecture overview of Gobblin , and the Gobblin user guide . We also provide a discussion group: Google Gobblin-Users Group . Please feel free to post any questions or comments. For a detailed overview, please take a look at the VLDB 2015 paper and the LinkedIn's Gobblin blog post .","title":"Home"},{"location":"Getting-Started/","text":"Table of Contents Table of Contents Introduction Getting a Gobblin Release Building a Distribution Run Your First Job Steps Running Gobblin as a Daemon Preliminary Steps Other Example Jobs Introduction This guide will help you setup Gobblin, and run your first job. Currently, Gobblin requires JDK 7 or later to run. Getting a Gobblin Release All steps in this page assume you are using a Apache Gobblin source distribution. Download source distribution from here . Building a Distribution Build a distribution: cd /path/to/gobblin/source ./gradlew :gobblin-distribution:buildDistributionTar Note: A full build takes time because it runs other tasks like test, javadoc, findMainBugs, etc, which impacts the build performance. For a quick usage, building distribution is good enough. However a full build can be easily made by running: ./gradlew build The generated distribution contains the binary in a specific directory structure, which is different from source. After the build is done, there should be a tarball (if there are multiple, use the newest one) at build/gobblin-distribution/distributions/ Distributions built from source are generated as *.tar.gz files. After getting the tarball, unpackage it locally: tar -xvf gobblin-distribution-[VERSION].tar.gz . Run Your First Job Note: the following two sections are only applicable to newer versions of Gobblin. If you are running version 0.8.0 or earlier, skip to Gobblin daemon . Here we illustrate how to run a simple job. This job will pull revisions for the last ten days of each of the two Wikipedia pages: Linkedin, Wikipedia:Sandbox (a page with frequent edits). The records will be written to stdout. Gobblin can run either in standalone mode or on MapReduce. In this example we will run Gobblin in standalone mode. This page explains how to run the job from the terminal. You may also run this job from your favorite IDE (IntelliJ is recommended). Steps cd to the unpacked Gobblin distribution and run bin/gobblin cli run to get usage. Running bin/gobblin cli run listQuickApps will list the available easy-to-configure apps. Note the line with the wikipedia example: wikipedia - Gobblin example that downloads revisions from Wikipedia. Running bin/gobblin cli run wikipedia will show the usage of this application. Notice the usage and one of the options listed for this job: usage: gobblin cli run wikipedia [OPTIONS] article-title [ article-title ...] -lookback arg Sets the period for which articles should be pulled in ISO time format (e.g. P2D, PT1H) Run bin/gobblin cli run wikipedia -lookback P10D LinkedIn Wikipedia:Sandbox . This will print a lot of logs, but somewhere in there you will see a few json entries with the revisions for those articles. For example: { revid :746260034, parentid :745444076, user : 2605:8D80:580:5824:B108:82BD:693D:CFA1 , anon : , userid :0, timestamp : 2016-10-26T08:12:09Z , size :69527, pageid :970755, title : LinkedIn } In the usage, there is also an option to instead write the output to an avro file: -avroOutput arg Write output to Avro files. Specify the output directory as argument. Running bin/gobblin cli run wikipedia -lookback P10D -avroOutput /tmp/wikiSample LinkedIn Wikipedia:Sandbox will create a directory /tmp/wikiSample with two subdirectories LinkedIn and Wikipedia_Sandbox each one with one avro file. Running Gobblin as a Daemon Here we show how to run a Gobblin daemon. A Gobblin daemon tracks a directory and finds job configuration files in it (jobs with extensions *.pull ). Job files can be either run once or scheduled jobs. Gobblin will automatically execute this jobs as they are received following the schedule. For this example, we will once again run the Wikipedia example. The records will be stored as Avro files. Preliminary Each Gobblin job minimally involves several constructs, e.g. Source , Extractor , DataWriter and DataPublisher . As the names suggest, Source defines the source to pull data from, Extractor implements the logic to extract data records, DataWriter defines the way the extracted records are output, and DataPublisher publishes the data to the final output location. A job may optionally have one or more Converters, which transform the extracted records, as well as one or more PolicyCheckers that check the quality of the extracted records and determine whether they conform to certain policies. Some of the classes relevant to this example include WikipediaSource , WikipediaExtractor , WikipediaConverter , AvroHdfsDataWriter and BaseDataPublisher . To run Gobblin in standalone daemon mode we need a Gobblin configuration file (such as uses application.conf ). And for each job we wish to run, we also need a job configuration file (such as wikipedia.pull ). The Gobblin configuration file, which is passed to Gobblin as a command line argument, should contain a property jobconf.dir which specifies where the job configuration files are located. By default, jobconf.dir points to environment variable GOBBLIN_JOB_CONFIG_DIR . Each file in jobconf.dir with extension .job or .pull is considered a job configuration file, and Gobblin will launch a job for each such file. For more information on Gobblin deployment in standalone mode, refer to the Standalone Deployment page. A list of commonly used configuration properties can be found here: Configuration Properties Glossary . Steps Create a folder to store the job configuration file. Put wikipedia.pull in this folder, and set environment variable GOBBLIN_JOB_CONFIG_DIR to point to this folder. Also, make sure that the environment variable JAVA_HOME is set correctly. Create a folder as Gobblin's working directory. Gobblin will write job output as well as other information there, such as locks and state-store (for more information, see the Standalone Deployment page). Set environment variable GOBBLIN_WORK_DIR to point to that folder. Unpack Gobblin distribution: Launch Gobblin in one of the execution mode [for more info refer: Gobblin-CLI ] : gobblin service standalone start Stdout and the job log, which contains the progress and status of the job, will be written into logs/ execution-mode .out logs/ execution-mode .err (to change where the log is written, modify the Log4j configuration file conf/log4j.xml ). Among the job logs there should be the following information: INFO JobScheduler - Loaded 1 job configuration INFO AbstractJobLauncher - Starting job job_PullFromWikipedia_1422040355678 INFO TaskExecutor - Starting the task executor INFO LocalTaskStateTracker2 - Starting the local task state tracker INFO AbstractJobLauncher - Submitting task task_PullFromWikipedia_1422040355678_0 to run INFO TaskExecutor - Submitting task task_PullFromWikipedia_1422040355678_0 INFO AbstractJobLauncher - Waiting for submitted tasks of job job_PullFromWikipedia_1422040355678 to complete... to complete... INFO AbstractJobLauncher - 1 out of 1 tasks of job job_PullFromWikipedia_1422040355678 are running INFO WikipediaExtractor - 5 record(s) retrieved for title NASA INFO WikipediaExtractor - 5 record(s) retrieved for title LinkedIn INFO WikipediaExtractor - 5 record(s) retrieved for title Parris_Cues INFO WikipediaExtractor - 5 record(s) retrieved for title Barbara_Corcoran INFO Task - Extracted 20 data records INFO Fork-0 - Committing data of branch 0 of task task_PullFromWikipedia_1422040355678_0 INFO LocalTaskStateTracker2 - Task task_PullFromWikipedia_1422040355678_0 completed in 2334ms with state SUCCESSFUL INFO AbstractJobLauncher - All tasks of job job_PullFromWikipedia_1422040355678 have completed INFO TaskExecutor - Stopping the task executor INFO LocalTaskStateTracker2 - Stopping the local task state tracker INFO AbstractJobLauncher - Publishing job data of job job_PullFromWikipedia_1422040355678 with commit policy COMMIT_ON_FULL_SUCCESS INFO AbstractJobLauncher - Persisting job/task states of job job_PullFromWikipedia_1422040355678 After the job is done, stop Gobblin by running gobblin service standalone stop The job output is written in GOBBLIN_WORK_DIR/job-output folder as an Avro file. To see the content of the job output, use the Avro tools to convert Avro to JSON. Download the latest version of Avro tools (e.g. avro-tools-1.8.1.jar): curl -O http://central.maven.org/maven2/org/apache/avro/avro-tools/1.8.1/avro-tools-1.8.1.jar and run java -jar avro-tools-1.8.1.jar tojson --pretty [job_output].avro output.json output.json will contain all retrieved records in JSON format. Note that since this job configuration file we used ( wikipedia.pull ) doesn't specify a job schedule, the job will run immediately and will run only once. To schedule a job to run at a certain time and/or repeatedly, set the job.schedule property with a cron-based syntax. For example, job.schedule=0 0/2 * * * ? will run the job every two minutes. See this link (Quartz CronTrigger) for more details. Other Example Jobs Besides the Wikipedia example, we have another example job SimpleJson , which extracts records from JSON files and store them in Avro files. To create your own jobs, simply implement the relevant interfaces such as Source , Extractor , Converter and DataWriter . In the job configuration file, set properties such as source.class and converter.class to point to these classes. On a side note: while users are free to directly implement the Extractor interface (e.g., WikipediaExtractor), Gobblin also provides several extractor implementations based on commonly used protocols, e.g., KafkaExtractor , RestApiExtractor , JdbcExtractor , SftpExtractor , etc. Users are encouraged to extend these classes to take advantage of existing implementations.","title":"Getting Started"},{"location":"Getting-Started/#table-of-contents","text":"Table of Contents Introduction Getting a Gobblin Release Building a Distribution Run Your First Job Steps Running Gobblin as a Daemon Preliminary Steps Other Example Jobs","title":"Table of Contents"},{"location":"Getting-Started/#introduction","text":"This guide will help you setup Gobblin, and run your first job. Currently, Gobblin requires JDK 7 or later to run.","title":"Introduction"},{"location":"Getting-Started/#getting-a-gobblin-release","text":"All steps in this page assume you are using a Apache Gobblin source distribution. Download source distribution from here .","title":"Getting a Gobblin Release"},{"location":"Getting-Started/#building-a-distribution","text":"Build a distribution: cd /path/to/gobblin/source ./gradlew :gobblin-distribution:buildDistributionTar Note: A full build takes time because it runs other tasks like test, javadoc, findMainBugs, etc, which impacts the build performance. For a quick usage, building distribution is good enough. However a full build can be easily made by running: ./gradlew build The generated distribution contains the binary in a specific directory structure, which is different from source. After the build is done, there should be a tarball (if there are multiple, use the newest one) at build/gobblin-distribution/distributions/ Distributions built from source are generated as *.tar.gz files. After getting the tarball, unpackage it locally: tar -xvf gobblin-distribution-[VERSION].tar.gz .","title":"Building a Distribution"},{"location":"Getting-Started/#run-your-first-job","text":"Note: the following two sections are only applicable to newer versions of Gobblin. If you are running version 0.8.0 or earlier, skip to Gobblin daemon . Here we illustrate how to run a simple job. This job will pull revisions for the last ten days of each of the two Wikipedia pages: Linkedin, Wikipedia:Sandbox (a page with frequent edits). The records will be written to stdout. Gobblin can run either in standalone mode or on MapReduce. In this example we will run Gobblin in standalone mode. This page explains how to run the job from the terminal. You may also run this job from your favorite IDE (IntelliJ is recommended).","title":"Run Your First Job"},{"location":"Getting-Started/#steps","text":"cd to the unpacked Gobblin distribution and run bin/gobblin cli run to get usage. Running bin/gobblin cli run listQuickApps will list the available easy-to-configure apps. Note the line with the wikipedia example: wikipedia - Gobblin example that downloads revisions from Wikipedia. Running bin/gobblin cli run wikipedia will show the usage of this application. Notice the usage and one of the options listed for this job: usage: gobblin cli run wikipedia [OPTIONS] article-title [ article-title ...] -lookback arg Sets the period for which articles should be pulled in ISO time format (e.g. P2D, PT1H) Run bin/gobblin cli run wikipedia -lookback P10D LinkedIn Wikipedia:Sandbox . This will print a lot of logs, but somewhere in there you will see a few json entries with the revisions for those articles. For example: { revid :746260034, parentid :745444076, user : 2605:8D80:580:5824:B108:82BD:693D:CFA1 , anon : , userid :0, timestamp : 2016-10-26T08:12:09Z , size :69527, pageid :970755, title : LinkedIn } In the usage, there is also an option to instead write the output to an avro file: -avroOutput arg Write output to Avro files. Specify the output directory as argument. Running bin/gobblin cli run wikipedia -lookback P10D -avroOutput /tmp/wikiSample LinkedIn Wikipedia:Sandbox will create a directory /tmp/wikiSample with two subdirectories LinkedIn and Wikipedia_Sandbox each one with one avro file.","title":"Steps"},{"location":"Getting-Started/#running-gobblin-as-a-daemon","text":"Here we show how to run a Gobblin daemon. A Gobblin daemon tracks a directory and finds job configuration files in it (jobs with extensions *.pull ). Job files can be either run once or scheduled jobs. Gobblin will automatically execute this jobs as they are received following the schedule. For this example, we will once again run the Wikipedia example. The records will be stored as Avro files.","title":"Running Gobblin as a Daemon"},{"location":"Getting-Started/#preliminary","text":"Each Gobblin job minimally involves several constructs, e.g. Source , Extractor , DataWriter and DataPublisher . As the names suggest, Source defines the source to pull data from, Extractor implements the logic to extract data records, DataWriter defines the way the extracted records are output, and DataPublisher publishes the data to the final output location. A job may optionally have one or more Converters, which transform the extracted records, as well as one or more PolicyCheckers that check the quality of the extracted records and determine whether they conform to certain policies. Some of the classes relevant to this example include WikipediaSource , WikipediaExtractor , WikipediaConverter , AvroHdfsDataWriter and BaseDataPublisher . To run Gobblin in standalone daemon mode we need a Gobblin configuration file (such as uses application.conf ). And for each job we wish to run, we also need a job configuration file (such as wikipedia.pull ). The Gobblin configuration file, which is passed to Gobblin as a command line argument, should contain a property jobconf.dir which specifies where the job configuration files are located. By default, jobconf.dir points to environment variable GOBBLIN_JOB_CONFIG_DIR . Each file in jobconf.dir with extension .job or .pull is considered a job configuration file, and Gobblin will launch a job for each such file. For more information on Gobblin deployment in standalone mode, refer to the Standalone Deployment page. A list of commonly used configuration properties can be found here: Configuration Properties Glossary .","title":"Preliminary"},{"location":"Getting-Started/#steps_1","text":"Create a folder to store the job configuration file. Put wikipedia.pull in this folder, and set environment variable GOBBLIN_JOB_CONFIG_DIR to point to this folder. Also, make sure that the environment variable JAVA_HOME is set correctly. Create a folder as Gobblin's working directory. Gobblin will write job output as well as other information there, such as locks and state-store (for more information, see the Standalone Deployment page). Set environment variable GOBBLIN_WORK_DIR to point to that folder. Unpack Gobblin distribution: Launch Gobblin in one of the execution mode [for more info refer: Gobblin-CLI ] : gobblin service standalone start Stdout and the job log, which contains the progress and status of the job, will be written into logs/ execution-mode .out logs/ execution-mode .err (to change where the log is written, modify the Log4j configuration file conf/log4j.xml ). Among the job logs there should be the following information: INFO JobScheduler - Loaded 1 job configuration INFO AbstractJobLauncher - Starting job job_PullFromWikipedia_1422040355678 INFO TaskExecutor - Starting the task executor INFO LocalTaskStateTracker2 - Starting the local task state tracker INFO AbstractJobLauncher - Submitting task task_PullFromWikipedia_1422040355678_0 to run INFO TaskExecutor - Submitting task task_PullFromWikipedia_1422040355678_0 INFO AbstractJobLauncher - Waiting for submitted tasks of job job_PullFromWikipedia_1422040355678 to complete... to complete... INFO AbstractJobLauncher - 1 out of 1 tasks of job job_PullFromWikipedia_1422040355678 are running INFO WikipediaExtractor - 5 record(s) retrieved for title NASA INFO WikipediaExtractor - 5 record(s) retrieved for title LinkedIn INFO WikipediaExtractor - 5 record(s) retrieved for title Parris_Cues INFO WikipediaExtractor - 5 record(s) retrieved for title Barbara_Corcoran INFO Task - Extracted 20 data records INFO Fork-0 - Committing data of branch 0 of task task_PullFromWikipedia_1422040355678_0 INFO LocalTaskStateTracker2 - Task task_PullFromWikipedia_1422040355678_0 completed in 2334ms with state SUCCESSFUL INFO AbstractJobLauncher - All tasks of job job_PullFromWikipedia_1422040355678 have completed INFO TaskExecutor - Stopping the task executor INFO LocalTaskStateTracker2 - Stopping the local task state tracker INFO AbstractJobLauncher - Publishing job data of job job_PullFromWikipedia_1422040355678 with commit policy COMMIT_ON_FULL_SUCCESS INFO AbstractJobLauncher - Persisting job/task states of job job_PullFromWikipedia_1422040355678 After the job is done, stop Gobblin by running gobblin service standalone stop The job output is written in GOBBLIN_WORK_DIR/job-output folder as an Avro file. To see the content of the job output, use the Avro tools to convert Avro to JSON. Download the latest version of Avro tools (e.g. avro-tools-1.8.1.jar): curl -O http://central.maven.org/maven2/org/apache/avro/avro-tools/1.8.1/avro-tools-1.8.1.jar and run java -jar avro-tools-1.8.1.jar tojson --pretty [job_output].avro output.json output.json will contain all retrieved records in JSON format. Note that since this job configuration file we used ( wikipedia.pull ) doesn't specify a job schedule, the job will run immediately and will run only once. To schedule a job to run at a certain time and/or repeatedly, set the job.schedule property with a cron-based syntax. For example, job.schedule=0 0/2 * * * ? will run the job every two minutes. See this link (Quartz CronTrigger) for more details.","title":"Steps"},{"location":"Getting-Started/#other-example-jobs","text":"Besides the Wikipedia example, we have another example job SimpleJson , which extracts records from JSON files and store them in Avro files. To create your own jobs, simply implement the relevant interfaces such as Source , Extractor , Converter and DataWriter . In the job configuration file, set properties such as source.class and converter.class to point to these classes. On a side note: while users are free to directly implement the Extractor interface (e.g., WikipediaExtractor), Gobblin also provides several extractor implementations based on commonly used protocols, e.g., KafkaExtractor , RestApiExtractor , JdbcExtractor , SftpExtractor , etc. Users are encouraged to extend these classes to take advantage of existing implementations.","title":"Other Example Jobs"},{"location":"Gobblin-Architecture/","text":"Table of Contents Table of Contents Gobblin Architecture Overview Gobblin Job Flow Gobblin Constructs Source and Extractor Converter Quality Checker Fork Operator Data Writer Data Publisher Gobblin Task Flow Job State Management Handling of Failures Job Scheduling Gobblin Architecture Overview Gobblin is built around the idea of extensibility, i.e., it should be easy for users to add new adapters or extend existing adapters to work with new sources and start extracting data from the new sources in any deployment settings. The architecture of Gobblin reflects this idea, as shown in Fig. 1 below: Figure 1: Gobblin Architecture Overview A Gobblin job is built on a set of constructs (illustrated by the light green boxes in the diagram above) that work together in a certain way and get the data extraction work done. All the constructs are pluggable through the job configuration and extensible by adding new or extending existing implementations. The constructs will be discussed in Gobblin Constructs . A Gobblin job consists of a set of tasks, each of which corresponds to a unit of work to be done and is responsible for extracting a portion of the data. The tasks of a Gobblin job are executed by the Gobblin runtime (illustrated by the orange boxes in the diagram above) on the deployment setting of choice (illustrated by the red boxes in the diagram above). The Gobblin runtime is responsible for running user-defined Gobblin jobs on the deployment setting of choice. It handles the common tasks including job and task scheduling, error handling and task retries, resource negotiation and management, state management, data quality checking, data publishing, etc. Gobblin currently supports two deployment modes: the standalone mode on a single node and the Hadoop MapReduce mode on a Hadoop cluster. We are also working on adding support for deploying and running Gobblin as a native application on YARN . Details on deployment of Gobblin can be found in Gobblin Deployment . The running and operation of Gobblin are supported by a few components and utilities (illustrated by the blue boxes in the diagram above) that handle important things such as metadata management, state management, metric collection and reporting, and monitoring. Gobblin Job Flow A Gobblin job is responsible for extracting data in a defined scope/range from a data source and writing data to a sink such as HDFS. It manages the entire lifecycle of data ingestion in a certain flow as illustrated by Fig. 2 below. Figure 2: Gobblin Job Flow A Gobblin job starts with an optional phase of acquiring a job lock. The purpose of doing this is to prevent the next scheduled run of the same job from starting until the current run finishes. This phase is optional because some job schedulers such as Azkaban is already doing this. The next thing the job does is to create an instance of the Source class specified in the job configuration. A Source is responsible for partitioning the data ingestion work into a set of WorkUnit s, each of which represents a logic unit of work for extracting a portion of the data from a data source. A Source is also responsible for creating a Extractor for each WorkUnit . A Extractor , as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's Source is modeled after Hadoop's InputFormat , which is responsible for partitioning the input into Split s as well as creating a RecordReader for each Split . From the set of WorkUnit s given by the Source , the job creates a set of tasks. A task is a runtime counterpart of a WorkUnit , which represents a logic unit of work. Normally, a task is created per WorkUnit . However, there is a special type of WorkUnit s called MultiWorkUnit that wraps multiple WorkUnit s for which multiple tasks may be created, one per wrapped WorkUnit . The next phase is to launch and run the tasks. How tasks are executed and where they run depend on the deployment setting. In the standalone mode on a single node, tasks are running in a thread pool dedicated to that job, the size of which is configurable on a per-job basis. In the Hadoop MapReduce mode on a Hadoop cluster, tasks are running in the mappers (used purely as containers to run tasks). After all tasks of the job finish (either successfully or unsuccessfully), the job publishes the data if it is OK to do so. Whether extracted data should be published is determined by the task states and the JobCommitPolicy used (configurable). More specifically, extracted data should be published if and only if any one of the following two conditions holds: JobCommitPolicy.COMMIT_ON_PARTIAL_SUCCESS is specified in the job configuration. JobCommitPolicy.COMMIT_ON_FULL_SUCCESS is specified in the job configuration and all tasks were successful. After the data extracted is published, the job persists the job/task states into the state store. When the next scheduled run of the job starts, it will load the job/task states of the previous run to get things like watermarks so it knows where to start. During its execution, the job may create some temporary working data that is no longer needed after the job is done. So the job will cleanup such temporary work data before exiting. Finally, an optional phase of the job is to release the job lock if it is acquired at the beginning. This gives green light to the next scheduled run of the same job to proceed. If a Gobblin job is cancelled before it finishes, the job will not persist any job/task state nor commit and publish any data (as the dotted line shows in the diagram). Gobblin Constructs As described above, a Gobblin job creates and runs tasks, each of which is responsible for extracting a portion of the data to be pulled by the job. A Gobblin task is created from a WorkUnit that represents a unit of work and serves as a container of job configuration at runtime. A task composes the Gobblin constructs into a flow to extract, transform, checks data quality on, and finally writes each extracted data record to the specified sink. Fig. 3 below gives an overview on the Gobblin constructs that constitute the task flows in a Gobblin job. Figure 3: Gobblin Constructs Source and Extractor A Source represents an adapter between a data source and Gobblin and is used by a Gobblin job at the beginning of the job flow. A Source is responsible for partitioning the data ingestion work into a set of WorkUnit s, each of which represents a logic unit of work for extracting a portion of the data from a data source. A Source is also responsible for creating an Extractor for each WorkUnit . An Extractor , as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's Source is modeled after Hadoop's InputFormat , which is responsible for partitioning the input into Split s as well as creating a RecordReader for each Split . Gobblin out-of-the-box provides some built-in Source and Extractor implementations that work with various types of of data sources, e.g., web services offering some Rest APIs, databases supporting JDBC, FTP/SFTP servers, etc. Currently, Extractor s are record-oriented, i.e., an Extractor reads one data record at a time, although internally it may choose to pull and cache a batch of data records. We are planning to add options for Extractor s to support byte-oriented and file-oriented processing. Converter A Converter is responsible for converting both schema and data records and is the core construct for data transformation. Converter s are composible and can be chained together as long as each adjacent pair of Converter s are compatible in the input and output schema and data record types. This allows building complex data transformation from simple Converter s. Note that a Converter converts an input schema to one output schema. It may, however, convert an input data record to zero ( 1:0 mapping), one ( 1:1 mapping), or many ( 1:N mapping) output data records. Each Converter converts every output records of the previous Converter , except for the first one that converts the original extracted data record. When converting a data record, a Converter also takes in the output converted schema of itself, except for the first one that takes in the original input schema. So each converter first converts the input schema and then uses the output schema in the conversion of each data record. The output schema of each converter is fed into both the converter itself for data record conversion and also the next converter. Fig. 4 explains how Converter chaining works using three example converters that have 1:1 , 1:N , and 1:1 mappings for data record conversion, respectively. Figure 4: How Converter Chaining Works Quality Checker A QualityChecker , as the name suggests, is responsible for data quality checking. There are two types of QualityChecker s: one that checks individual data records and decides if each record should proceed to the next phase in the task flow and the other one that checks the entire task output and decides if data can be committed. We call the two types row-level QualityChecker s and task-level QualityChecker s, respectively. A QualityChecker can be MANDATORY or OPTIONAL and will participate in the decision on if quality checking passes if and only if it is MANDATORY . OPTIONAL QualityChecker s are informational only. Similarly to Converter s, more than one QualityChecker can be specified and in this case, quality checking passes if and only if all MANDATORY QualityChecker s give a PASS . Fork Operator A ForkOperator is a type of control operators that allow a task flow to branch into multiple streams, each of which goes to a separately configured sink. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers. Data Writer A DataWriter is responsible for writing data records to the sink it is associated to. Gobblin out-of-the-box provides an AvroHdfsDataWriter for writing data in Avro format onto HDFS. Users can plugin their own DataWriter s by specifying a DataWriterBuilder class in the job configuration that Gobblin uses to build DataWriter s. Data Publisher A DataPublisher is responsible for publishing extracted data of a Gobblin job. Gobblin ships with a default DataPublisher that works with file-based DataWriter s such as the AvroHdfsDataWriter and moves data from the output directory of each task to a final job output directory. Gobblin Task Flow Fig. 5 below zooms in further and shows the details on how different constructs are connected and composed to form a task flow. The same task flow is employed regardless of the deployment setting and where tasks are running. Figure 5: Gobblin Task Flow A Gobblin task flow consists of a main branch and a number of forked branches coming out of a ForkOperator . It is optional to specify a ForkOperator in the job configuration. When no ForkOperator is specified in the job configuration, a Gobblin task flow uses a IdentityForkOperator by default with a single forked branch. The IdentityForkOperator simply connects the master branch and the single forked branch and passes schema and data records between them. The reason behind this is it avoids special logic from being introduced into the task flow when a ForkOperator is indeed specified in the job configuration. The master branch of a Gobblin task starts with schema extraction from the source. The extracted schema will go through a schema transformation phase if at least one Converter class is specified in the job configuration. The next phase is to repeatedly extract data records one at a time. Each extracted data record will also go through a transformation phase if at least one Converter class is specified. Each extracted (or converted if applicable) data record is fed into an optional list of row-level QualityChecker s. Data records that pass the row-level QualityChecker s will go through the ForkOperator and be further processed in the forked branches. The ForkOperator allows users to specify if the input schema or data record should go to a specific forked branch. If the input schema is specified not to go into a particular branch, that branch will be ignored. If the input schema or data record is specified to go into more than one forked branch, Gobblin assumes that the schema or data record class implements the Copyable interface and will attempt to make a copy before passing it to each forked branch. So it is very important to make sure the input schema or data record to the ForkOperator is an instance of Copyable if it is going into more than one branch. Similarly to the master branch, a forked branch also processes the input schema and each input data record (one at a time) through an optional transformation phase and a row-level quality checking phase. Data records that pass the branch's row-level QualityChecker s will be written out to a sink by a DataWriter . Each forked branch has its own sink configuration and a separate DataWriter . Upon successful processing of the last record, a forked branch applies an optional list of task-level QualityChecker s to the data processed by the branch in its entirety. If this quality checking passes, the branch commits the data and exits. A task flow completes its execution once every forked branches commit and exit. During the execution of a task, a TaskStateTracker keeps track of the task's state and a core set of task metrics, e.g., total records extracted, records extracted per second, total bytes extracted, bytes extracted per second, etc. Job State Management Typically a Gobblin job runs periodically on some schedule and each run of the job is extracting data incrementally, i.e., extracting new data or changes to existing data within a specific range since the last run of the job. To make incremental extraction possible, Gobblin must persist the state of the job upon the completion of each run and load the state of the previous run so the next run knows where to start extracting. Gobblin maintains a state store that is responsible for job state persistence. Each run of a Gobblin job reads the state store for the state of the previous run and writes the state of itself to the state store upon its completion. The state of a run of a Gobblin job consists of the job configuration and any properties set at runtime at the job or task level. Out-of-the-box, Gobblin uses an implementation of the state store that serializes job and task states into Hadoop SequenceFile s, one per job run. Each job has a separate directory where its job and task state SequenceFile s are stored. The file system on which the SequenceFile -based state store resides is configurable. Handling of Failures As a fault tolerance data ingestion framework, Gobblin employs multiple level of defenses against job and task failures. For job failures, Gobblin keeps track of the number of times a job fails consecutively and optionally sends out an alert email if the number exceeds a defined threshold so the owner of the job can jump in and investigate the failures. For task failures, Gobblin retries failed tasks in a job run up to a configurable maximum number of times. In addition to that, Gobblin also provides an option to enable retries of WorkUnit s corresponding to failed tasks across job runs. The idea is that if a task fails after all retries fail, the WorkUnit based on which the task gets created will be automatically included in the next run of the job if this type of retries is enabled. This type of retries is very useful in handling intermittent failures such as those due to temporary data source outrage. Job Scheduling Like mentioned above, a Gobblin job typically runs periodically on some schedule. Gobblin can be integrated with job schedulers such as Azkaban , Oozie , or Crontab. Out-of-the-box, Gobblin also ships with a built-in job scheduler backed by a Quartz scheduler, which is used as the default job scheduler in the standalone deployment and it supports cron-based triggers using the configuration property job.schedule for defining the cron schedule. An important feature of Gobblin is that it decouples the job scheduler and the jobs scheduled by the scheduler such that different jobs may run in different deployment settings. This is achieved using the abstraction JobLauncher that has different implementations for different deployment settings. For example, a job scheduler may have 5 jobs scheduled: 2 of them run locally on the same host as the scheduler using the LocalJobLauncher , whereas the rest 3 run on a Hadoop cluster somewhere using the MRJobLauncher . Which JobLauncher to use can be simply configured using the property launcher.type . Please refer to this tutorial for more information on how to use and configure a cron-based trigger.","title":"Architecture"},{"location":"Gobblin-Architecture/#table-of-contents","text":"Table of Contents Gobblin Architecture Overview Gobblin Job Flow Gobblin Constructs Source and Extractor Converter Quality Checker Fork Operator Data Writer Data Publisher Gobblin Task Flow Job State Management Handling of Failures Job Scheduling","title":"Table of Contents"},{"location":"Gobblin-Architecture/#gobblin-architecture-overview","text":"Gobblin is built around the idea of extensibility, i.e., it should be easy for users to add new adapters or extend existing adapters to work with new sources and start extracting data from the new sources in any deployment settings. The architecture of Gobblin reflects this idea, as shown in Fig. 1 below: Figure 1: Gobblin Architecture Overview A Gobblin job is built on a set of constructs (illustrated by the light green boxes in the diagram above) that work together in a certain way and get the data extraction work done. All the constructs are pluggable through the job configuration and extensible by adding new or extending existing implementations. The constructs will be discussed in Gobblin Constructs . A Gobblin job consists of a set of tasks, each of which corresponds to a unit of work to be done and is responsible for extracting a portion of the data. The tasks of a Gobblin job are executed by the Gobblin runtime (illustrated by the orange boxes in the diagram above) on the deployment setting of choice (illustrated by the red boxes in the diagram above). The Gobblin runtime is responsible for running user-defined Gobblin jobs on the deployment setting of choice. It handles the common tasks including job and task scheduling, error handling and task retries, resource negotiation and management, state management, data quality checking, data publishing, etc. Gobblin currently supports two deployment modes: the standalone mode on a single node and the Hadoop MapReduce mode on a Hadoop cluster. We are also working on adding support for deploying and running Gobblin as a native application on YARN . Details on deployment of Gobblin can be found in Gobblin Deployment . The running and operation of Gobblin are supported by a few components and utilities (illustrated by the blue boxes in the diagram above) that handle important things such as metadata management, state management, metric collection and reporting, and monitoring.","title":"Gobblin Architecture Overview"},{"location":"Gobblin-Architecture/#gobblin-job-flow","text":"A Gobblin job is responsible for extracting data in a defined scope/range from a data source and writing data to a sink such as HDFS. It manages the entire lifecycle of data ingestion in a certain flow as illustrated by Fig. 2 below. Figure 2: Gobblin Job Flow A Gobblin job starts with an optional phase of acquiring a job lock. The purpose of doing this is to prevent the next scheduled run of the same job from starting until the current run finishes. This phase is optional because some job schedulers such as Azkaban is already doing this. The next thing the job does is to create an instance of the Source class specified in the job configuration. A Source is responsible for partitioning the data ingestion work into a set of WorkUnit s, each of which represents a logic unit of work for extracting a portion of the data from a data source. A Source is also responsible for creating a Extractor for each WorkUnit . A Extractor , as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's Source is modeled after Hadoop's InputFormat , which is responsible for partitioning the input into Split s as well as creating a RecordReader for each Split . From the set of WorkUnit s given by the Source , the job creates a set of tasks. A task is a runtime counterpart of a WorkUnit , which represents a logic unit of work. Normally, a task is created per WorkUnit . However, there is a special type of WorkUnit s called MultiWorkUnit that wraps multiple WorkUnit s for which multiple tasks may be created, one per wrapped WorkUnit . The next phase is to launch and run the tasks. How tasks are executed and where they run depend on the deployment setting. In the standalone mode on a single node, tasks are running in a thread pool dedicated to that job, the size of which is configurable on a per-job basis. In the Hadoop MapReduce mode on a Hadoop cluster, tasks are running in the mappers (used purely as containers to run tasks). After all tasks of the job finish (either successfully or unsuccessfully), the job publishes the data if it is OK to do so. Whether extracted data should be published is determined by the task states and the JobCommitPolicy used (configurable). More specifically, extracted data should be published if and only if any one of the following two conditions holds: JobCommitPolicy.COMMIT_ON_PARTIAL_SUCCESS is specified in the job configuration. JobCommitPolicy.COMMIT_ON_FULL_SUCCESS is specified in the job configuration and all tasks were successful. After the data extracted is published, the job persists the job/task states into the state store. When the next scheduled run of the job starts, it will load the job/task states of the previous run to get things like watermarks so it knows where to start. During its execution, the job may create some temporary working data that is no longer needed after the job is done. So the job will cleanup such temporary work data before exiting. Finally, an optional phase of the job is to release the job lock if it is acquired at the beginning. This gives green light to the next scheduled run of the same job to proceed. If a Gobblin job is cancelled before it finishes, the job will not persist any job/task state nor commit and publish any data (as the dotted line shows in the diagram).","title":"Gobblin Job Flow"},{"location":"Gobblin-Architecture/#gobblin-constructs","text":"As described above, a Gobblin job creates and runs tasks, each of which is responsible for extracting a portion of the data to be pulled by the job. A Gobblin task is created from a WorkUnit that represents a unit of work and serves as a container of job configuration at runtime. A task composes the Gobblin constructs into a flow to extract, transform, checks data quality on, and finally writes each extracted data record to the specified sink. Fig. 3 below gives an overview on the Gobblin constructs that constitute the task flows in a Gobblin job. Figure 3: Gobblin Constructs","title":"Gobblin Constructs"},{"location":"Gobblin-Architecture/#source-and-extractor","text":"A Source represents an adapter between a data source and Gobblin and is used by a Gobblin job at the beginning of the job flow. A Source is responsible for partitioning the data ingestion work into a set of WorkUnit s, each of which represents a logic unit of work for extracting a portion of the data from a data source. A Source is also responsible for creating an Extractor for each WorkUnit . An Extractor , as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's Source is modeled after Hadoop's InputFormat , which is responsible for partitioning the input into Split s as well as creating a RecordReader for each Split . Gobblin out-of-the-box provides some built-in Source and Extractor implementations that work with various types of of data sources, e.g., web services offering some Rest APIs, databases supporting JDBC, FTP/SFTP servers, etc. Currently, Extractor s are record-oriented, i.e., an Extractor reads one data record at a time, although internally it may choose to pull and cache a batch of data records. We are planning to add options for Extractor s to support byte-oriented and file-oriented processing.","title":"Source and Extractor"},{"location":"Gobblin-Architecture/#converter","text":"A Converter is responsible for converting both schema and data records and is the core construct for data transformation. Converter s are composible and can be chained together as long as each adjacent pair of Converter s are compatible in the input and output schema and data record types. This allows building complex data transformation from simple Converter s. Note that a Converter converts an input schema to one output schema. It may, however, convert an input data record to zero ( 1:0 mapping), one ( 1:1 mapping), or many ( 1:N mapping) output data records. Each Converter converts every output records of the previous Converter , except for the first one that converts the original extracted data record. When converting a data record, a Converter also takes in the output converted schema of itself, except for the first one that takes in the original input schema. So each converter first converts the input schema and then uses the output schema in the conversion of each data record. The output schema of each converter is fed into both the converter itself for data record conversion and also the next converter. Fig. 4 explains how Converter chaining works using three example converters that have 1:1 , 1:N , and 1:1 mappings for data record conversion, respectively. Figure 4: How Converter Chaining Works","title":"Converter"},{"location":"Gobblin-Architecture/#quality-checker","text":"A QualityChecker , as the name suggests, is responsible for data quality checking. There are two types of QualityChecker s: one that checks individual data records and decides if each record should proceed to the next phase in the task flow and the other one that checks the entire task output and decides if data can be committed. We call the two types row-level QualityChecker s and task-level QualityChecker s, respectively. A QualityChecker can be MANDATORY or OPTIONAL and will participate in the decision on if quality checking passes if and only if it is MANDATORY . OPTIONAL QualityChecker s are informational only. Similarly to Converter s, more than one QualityChecker can be specified and in this case, quality checking passes if and only if all MANDATORY QualityChecker s give a PASS .","title":"Quality Checker"},{"location":"Gobblin-Architecture/#fork-operator","text":"A ForkOperator is a type of control operators that allow a task flow to branch into multiple streams, each of which goes to a separately configured sink. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers.","title":"Fork Operator"},{"location":"Gobblin-Architecture/#data-writer","text":"A DataWriter is responsible for writing data records to the sink it is associated to. Gobblin out-of-the-box provides an AvroHdfsDataWriter for writing data in Avro format onto HDFS. Users can plugin their own DataWriter s by specifying a DataWriterBuilder class in the job configuration that Gobblin uses to build DataWriter s.","title":"Data Writer"},{"location":"Gobblin-Architecture/#data-publisher","text":"A DataPublisher is responsible for publishing extracted data of a Gobblin job. Gobblin ships with a default DataPublisher that works with file-based DataWriter s such as the AvroHdfsDataWriter and moves data from the output directory of each task to a final job output directory.","title":"Data Publisher"},{"location":"Gobblin-Architecture/#gobblin-task-flow","text":"Fig. 5 below zooms in further and shows the details on how different constructs are connected and composed to form a task flow. The same task flow is employed regardless of the deployment setting and where tasks are running. Figure 5: Gobblin Task Flow A Gobblin task flow consists of a main branch and a number of forked branches coming out of a ForkOperator . It is optional to specify a ForkOperator in the job configuration. When no ForkOperator is specified in the job configuration, a Gobblin task flow uses a IdentityForkOperator by default with a single forked branch. The IdentityForkOperator simply connects the master branch and the single forked branch and passes schema and data records between them. The reason behind this is it avoids special logic from being introduced into the task flow when a ForkOperator is indeed specified in the job configuration. The master branch of a Gobblin task starts with schema extraction from the source. The extracted schema will go through a schema transformation phase if at least one Converter class is specified in the job configuration. The next phase is to repeatedly extract data records one at a time. Each extracted data record will also go through a transformation phase if at least one Converter class is specified. Each extracted (or converted if applicable) data record is fed into an optional list of row-level QualityChecker s. Data records that pass the row-level QualityChecker s will go through the ForkOperator and be further processed in the forked branches. The ForkOperator allows users to specify if the input schema or data record should go to a specific forked branch. If the input schema is specified not to go into a particular branch, that branch will be ignored. If the input schema or data record is specified to go into more than one forked branch, Gobblin assumes that the schema or data record class implements the Copyable interface and will attempt to make a copy before passing it to each forked branch. So it is very important to make sure the input schema or data record to the ForkOperator is an instance of Copyable if it is going into more than one branch. Similarly to the master branch, a forked branch also processes the input schema and each input data record (one at a time) through an optional transformation phase and a row-level quality checking phase. Data records that pass the branch's row-level QualityChecker s will be written out to a sink by a DataWriter . Each forked branch has its own sink configuration and a separate DataWriter . Upon successful processing of the last record, a forked branch applies an optional list of task-level QualityChecker s to the data processed by the branch in its entirety. If this quality checking passes, the branch commits the data and exits. A task flow completes its execution once every forked branches commit and exit. During the execution of a task, a TaskStateTracker keeps track of the task's state and a core set of task metrics, e.g., total records extracted, records extracted per second, total bytes extracted, bytes extracted per second, etc.","title":"Gobblin Task Flow"},{"location":"Gobblin-Architecture/#job-state-management","text":"Typically a Gobblin job runs periodically on some schedule and each run of the job is extracting data incrementally, i.e., extracting new data or changes to existing data within a specific range since the last run of the job. To make incremental extraction possible, Gobblin must persist the state of the job upon the completion of each run and load the state of the previous run so the next run knows where to start extracting. Gobblin maintains a state store that is responsible for job state persistence. Each run of a Gobblin job reads the state store for the state of the previous run and writes the state of itself to the state store upon its completion. The state of a run of a Gobblin job consists of the job configuration and any properties set at runtime at the job or task level. Out-of-the-box, Gobblin uses an implementation of the state store that serializes job and task states into Hadoop SequenceFile s, one per job run. Each job has a separate directory where its job and task state SequenceFile s are stored. The file system on which the SequenceFile -based state store resides is configurable.","title":"Job State Management"},{"location":"Gobblin-Architecture/#handling-of-failures","text":"As a fault tolerance data ingestion framework, Gobblin employs multiple level of defenses against job and task failures. For job failures, Gobblin keeps track of the number of times a job fails consecutively and optionally sends out an alert email if the number exceeds a defined threshold so the owner of the job can jump in and investigate the failures. For task failures, Gobblin retries failed tasks in a job run up to a configurable maximum number of times. In addition to that, Gobblin also provides an option to enable retries of WorkUnit s corresponding to failed tasks across job runs. The idea is that if a task fails after all retries fail, the WorkUnit based on which the task gets created will be automatically included in the next run of the job if this type of retries is enabled. This type of retries is very useful in handling intermittent failures such as those due to temporary data source outrage.","title":"Handling of Failures"},{"location":"Gobblin-Architecture/#job-scheduling","text":"Like mentioned above, a Gobblin job typically runs periodically on some schedule. Gobblin can be integrated with job schedulers such as Azkaban , Oozie , or Crontab. Out-of-the-box, Gobblin also ships with a built-in job scheduler backed by a Quartz scheduler, which is used as the default job scheduler in the standalone deployment and it supports cron-based triggers using the configuration property job.schedule for defining the cron schedule. An important feature of Gobblin is that it decouples the job scheduler and the jobs scheduled by the scheduler such that different jobs may run in different deployment settings. This is achieved using the abstraction JobLauncher that has different implementations for different deployment settings. For example, a job scheduler may have 5 jobs scheduled: 2 of them run locally on the same host as the scheduler using the LocalJobLauncher , whereas the rest 3 run on a Hadoop cluster somewhere using the MRJobLauncher . Which JobLauncher to use can be simply configured using the property launcher.type . Please refer to this tutorial for more information on how to use and configure a cron-based trigger.","title":"Job Scheduling"},{"location":"Powered-By/","text":"A few companies known to be powered by Gobblin: LinkedIn Intel Paypal Microsoft IBM CERN Apple Stunlock Studios Swisscom Prezi Cleverdata AppLift Nerdwallet Sandia National Laboratories BPU Holdings","title":"Companies Powered By Gobblin"},{"location":"adaptors/Gobblin-Distcp/","text":"Table of Contents Table of Contents Introduction Problem Statement Existing Solutions Proposed Design Design Overview Example Classes CopyableDataset DatasetFinder CopyableFile Distcp Constructs CopySource FileAwareInputStreamExtractor DistcpConverter FileAwareInputStreamDataWriter TarArchiveInputStreamDataWriter CopyPublisher Recovery of unpublished files Splitting files into block level granularity work units Leverage Performance, Scalability and Provisioning Monitoring and Alerting Future Work Introduction Gobblin Distcp is a rebuilding of Distcp on top of Gobblin. It is still currently a work in progress, but an Alpha version of the code is available. The document mainly outlines the design of Gobblin Distcp, including the high level design goals and core APIs. Gobblin Distcp benefits from many features in Gobblin: Dataset awareness Configurability/customization of replication flows (Planned) Isolation (Implemented) Support for flexible copy triggering semantics (data triggers, dataset descriptors, etc.) (Planned) Future support for self-serve replication (Planned) Operability Metrics (Implemented) Customizable publish semantics Data triggers (Implemented) Hive registration (Implemented) Auditing (Planned) Exactly-once publishing (Planned) Future support for continuous execution (near-real-time replication) (Planned) Inline byte stream processing Archiving/unarchiving (Implemented) Encryption/decryption (Implemented) The effort uses a regular Gobblin workflow with specific constructs that handle input streams as records. We use gobblin data management to have dataset awareness, and to optimize copy listings where possible. We use gobblin metrics to emit data availability notifications and operational metrics. Problem Statement We need an application for copying from a FileSystem compatible source to another FileSystem compatible destination. The application must be able to: Find files in source FileSystem A that need to be copied. Determine locations in FileSystem B where the new files will be created. Do byte level copy from file in A to file in B efficiently. Be simple enough for other users to use it instead of distcp. Set owner, group, and permissions of newly created files, as well as newly created ancestors. On user request, override default attributes of new files like block size, replication factor, etc. Allow for on-the-fly byte level transformations like UnGZippping, PGP decrypting, etc. Allow for on-the-fly unpacking of byte streams, like expanding tarballs, zips, etc. Perform quality checks on the destination files if requested. Emit real-time operational metrics (transfer speed, files completed, etc.) and allow for creating post-job summaries. Emit data availability notifications. Copy listings should be pluggable and fully dataset aware. Datasets can annotate data availability notifications, or modify aspects of the copy operation (like preserve attributes). Publishing should be pluggable and allow for easy extensions. Default publishing will simply place files in correct target locations. Extensions can register new files with Hive, etc. Reuse previously copied files that didn\u00e2\u0080\u0099t get published due to errors in the previous flow. Use other Gobblin features (e.g. proxying, password management). Existing Solutions Distcp : Tool maintained by Hadoop. Allows copying files and syncing directories between FileSystem implementations (including HDFS, S3, and local file systems). Uses MapReduce to perform the copy. Has various features like preserving permissions and setting replication factors. Uses some heuristics to accelerate file listing generation (e.g. using directory mod time to determine if new files are likely to exist). Minimally dataset aware: e.g. can treat tracking and databases data differently. Can recover files that failed to publish in previous runs. Gobblin: Regular Gobblin can be used to read every record and re-write it to the new location. However, this involves actually deserializing records and has significant overhead. Proposed Design Design Overview The core of Gobblin distcp is simply a traditional Gobblin flow with sources, converters, and writers that work directly with input streams. The work units are CopyableFiles, which contain all the metadata necessary to copy a single file, and the records are FileAwareInputStream, which is an input stream + CopyableFile. Example CopySource runs a DatasetFinder . DatasetFinder searches for all Dataset s. It creates a CopyableDataset for each Dataset . Each Dataset creates a copy listing for itself. CopySource creates a Gobblin WorkUnit for each CopyableFile . InputStreamExtractor opens an InputStream for each CopyableFile . InputStreamWriter creates the necessary file in destination and dumps the bytes of the InputStream . InputStreamWriter sets the correct owner and permissions, and puts files in writer-output location in the same directory structure as they will be published. DataPublisher groups work units by partition string and, for each partition string, moves the files to the destination. If a partition of a dataset failed to copy, all other successful partitions and datasets are published either way. The failed partition is staged for recovery on next run. DataPublisher emits notifications, performs Hive registration, etc. Classes CopyableDataset An abstraction of a Dataset , i.e. a set of related files (for example a database table). Generates copy listings for that dataset. Example: if I want to replicate DB.Table to a new location, which files should I copy. Generates partitioning of copy listing into atomic units called file sets. A file set will be published nearly atomically. All files in the listing will be copied. It is the responsibility of the CopyableDataset to do a diff with the target (because it might have optimizations for performing the diff). Implementations: RecursiveCopyableDataset : copies all files under an root directory. StreamDataset : copies date-partitioned directories for Kafka topics. /** * Interface representing a dataset. */ public interface Dataset { /** * Deepest {@link org.apache.hadoop.fs.Path} that contains all files in the dataset. */ public Path datasetRoot(); } /** * {@link Dataset} that supports finding {@link CopyableFile}s. */ public interface CopyableDataset extends Dataset { /** * Find all {@link CopyableFile}s in this dataset. * * p * This method should return a collection of {@link CopyableFile}, each describing one file that should be copied * to the target. The returned collection should contain exactly one {@link CopyableFile} per file that should * be copied. Directories are created automatically, the returned collection should not include any directories. * See {@link CopyableFile} for explanation of the information contained in the {@link CopyableFile}s. * /p * * @param targetFs target {@link FileSystem} where copied files will be placed. * @param configuration {@link CopyConfiguration} for this job. See {@link CopyConfiguration}. * @return List of {@link CopyableFile}s in this dataset. * @throws IOException */ public Collection CopyableFile getCopyableFiles(FileSystem targetFs, CopyConfiguration configuration) throws IOException; } DatasetFinder Finds CopyableDataset s in the file system. Implementations: CopyableGlobDatasetFinder : Uses a glob and creates a RecursiveCopyableDataset for each matching directory. StreamDatasetFinder : Creates a StreamDataset for each directory in input directory. /** * Finds {@link Dataset}s in the file system. * * p * Concrete subclasses should have a constructor with signature * ({@link org.apache.hadoop.fs.FileSystem}, {@link java.util.Properties}). * /p */ public interface DatasetsFinder T extends Dataset { /** * Find all {@link Dataset}s in the file system. * @return List of {@link Dataset}s in the file system. * @throws IOException */ public List T findDatasets() throws IOException; /** * @return The deepest common root shared by all {@link Dataset}s root paths returned by this finder. */ public Path commonDatasetRoot(); } CopyableFile Structure containing information about a file that needs to be copied: Origin FileStatus . Destination path. Desired owner and permission. Attributes to be preserved (e.g. replication, block size). FileSet file belongs to (atomic units). Checksum. Metadata. Built with a builder with sensible defaults. Has a replicable guid that uniquely identifies origin file. Guid is a hash (sha1) of: Origin path. Origin length. Origin timestamp. Checksum if available. Distcp Constructs Distcp runs as a Gobblin flow with special distcp constructs. CopySource Source for Gobblin distcp. Flow: Instantiate a DatasetFinder . Use DatasetFinder to find CopyableDatasets . For each CopyableDataset get file listing. For each CopyableFile create a Gobblin WorkUnit . Serialize the CopyableFile into the WorkUnit . For each WorkUnit create a FileAwareInputStreamExtractor . FileAwareInputStreamExtractor Extractor for Gobblin distcp. Opens origin file and creates FileAwareInputStream containing the InputStream and the corresponding CopyableFile . DistcpConverter Abstract class for distcp converters. Allows transformation of the InputStream (for example decrypting, de-archiving, etc.). Alters extensions to reflect the changes (eg. remove .gz). Implementations: DecryptConverter : Performs GPG decryption of the input. UnGzipConverter : Un-gzips the input. EncryptConverter : Performs GPG encryption of the input. FileAwareInputStreamDataWriter Gobblin writer for distcp. Takes a FileAwareInputStream and performs the copy of the file. Currently using a single DirectByteBuffer . Possible optimizations: Use two DirectByteBuffer s, while one is reading, the other one is writing. Sets target file attributes and permissions. Performs recovery of previous unpublished work. TarArchiveInputStreamDataWriter Extension of FileAwareInputStreamDataWriter . Takes a tar input stream and writes the contained sequence of files to the file system. Allows for automatic untaring on write. Example: a tarball containing files root/path/to/file, root/file2 will be expanded on the fly to get output/path/to/file and output/file2. CopyPublisher Groups work units by file set. For each file set, move the output files from staging location to final location in as few operations as possible (for near-atomicity). Recheck permissions of the output. Emit events indicating availability of published data. One event per file. One event per file set. Recovery of unpublished files Copied files may fail to be published even after the copy has succeeded. Some reasons: Failed to set permissions. Other files in the same file set failed, preventing atomic publish. Wrong permissions for destination. Transient file system issues. When distcp detects a failure on the write step (e.g. setting owner and permissions), it will persist the uncommitted file to a separate location (Gobblin automatically deletes staging locations on exit). On the next run, distcp can identify files that were previously copied, and re-use them instead of repeating the data copy. The publish step uses \"exactly once\" feature: The publisher generates a set of publish steps (e.g. 1. move file to this location, 2. send event notifications, 3. commit watermark). The publish steps are written to a write-ahead log. The publish steps are executed. If the publish steps are successful, the write-ahead log is deleted. If the publish steps fail, the write-ahead log is preserved, and Gobblin will attempt to run them on the next execution. Relevant directories will not be deleted on exit. Eventually, write step should also use exactly-once feature. Splitting files into block level granularity work units Gobblin Distcp has an option to enable splitting of files into block level granularity work units, which involves the use of a helper class, DistcpFileSplitter , which has methods for: Splitting of files into block level work units, which is done at the CopySource ; the block level granularity is represented by an additional Split construct within each work unit that contains offset and ordering information. Merging of block level work units/splits, which is done at the CopyDataPublisher ; this uses calls to the FileSystem#concat API to append the separately copied entities of each file back together. Leverage Gobblin Distcp leverages Gobblin as its running framework, and most features available to Gobblin: Gobblin execution implementation Gobblin publishing implementation Gobblin metrics Gobblin on YARN Exactly once semantics Automatic Hive registration Performance, Scalability and Provisioning There are two components in the flow: File listing and work unit generation: slow if there are too many files. Dataset aware optimizations are possible, as well as using services other than Hadoop ls call (like lsr or HDFS edit log), so this can be improved and should scale with the correct optimizations. Work unit generation is currently a serial process handled by Gobblin and could be a bottleneck. If we find it is a bottleneck, that process is parallelizable. Actual copy tasks: massively parallel using MR or many containers in YARN. Generally, it is the most expensive part of the flow. Although inputs can be split, HDFS does not support parallel writing to the same file, so large files will be a bottleneck (but this is true with distcp2 as well). This issue will be alleviated with the YARN executing model, where WorkUnits are allocated dynamically to containers (multiple small files can be copied in one container will another container copies a large file), and datasets can be publishes as soon as they are ready (remove impact from slow datasets). If this is an issue for a job in MR/with HDFS, Gobblin Distcp provides an option to enable splitting of files into block level granularity work units to be copied independently, then merged back together before publishing, which may help to reduce the mapper skew and alleviate the bottleneck. In direct byte copies, we have observed speeds that saturate the available network speed. Byte level transformations (e.g. decrypting) slow down the process, and also cannot be used with jobs that enable splitting. Monitoring and Alerting Monitoring and alerting will be done through Gobblin metrics. We will have real-time operational metrics available. Gobblin metrics automatically emits notifications for any failures as well as whenever data is available. Better SLAs can be achieved in the future through the use of continuous ingestion with priority queues. Future Work There is currently work in progress to implement Gobblin Distcp on top of Hive. Gobblin Distcp will be capable of copying Hive tables and databases within and between Hadoop clusters.","title":"Gobblin Distcp"},{"location":"adaptors/Gobblin-Distcp/#table-of-contents","text":"Table of Contents Introduction Problem Statement Existing Solutions Proposed Design Design Overview Example Classes CopyableDataset DatasetFinder CopyableFile Distcp Constructs CopySource FileAwareInputStreamExtractor DistcpConverter FileAwareInputStreamDataWriter TarArchiveInputStreamDataWriter CopyPublisher Recovery of unpublished files Splitting files into block level granularity work units Leverage Performance, Scalability and Provisioning Monitoring and Alerting Future Work","title":"Table of Contents"},{"location":"adaptors/Gobblin-Distcp/#introduction","text":"Gobblin Distcp is a rebuilding of Distcp on top of Gobblin. It is still currently a work in progress, but an Alpha version of the code is available. The document mainly outlines the design of Gobblin Distcp, including the high level design goals and core APIs. Gobblin Distcp benefits from many features in Gobblin: Dataset awareness Configurability/customization of replication flows (Planned) Isolation (Implemented) Support for flexible copy triggering semantics (data triggers, dataset descriptors, etc.) (Planned) Future support for self-serve replication (Planned) Operability Metrics (Implemented) Customizable publish semantics Data triggers (Implemented) Hive registration (Implemented) Auditing (Planned) Exactly-once publishing (Planned) Future support for continuous execution (near-real-time replication) (Planned) Inline byte stream processing Archiving/unarchiving (Implemented) Encryption/decryption (Implemented) The effort uses a regular Gobblin workflow with specific constructs that handle input streams as records. We use gobblin data management to have dataset awareness, and to optimize copy listings where possible. We use gobblin metrics to emit data availability notifications and operational metrics.","title":"Introduction"},{"location":"adaptors/Gobblin-Distcp/#problem-statement","text":"We need an application for copying from a FileSystem compatible source to another FileSystem compatible destination. The application must be able to: Find files in source FileSystem A that need to be copied. Determine locations in FileSystem B where the new files will be created. Do byte level copy from file in A to file in B efficiently. Be simple enough for other users to use it instead of distcp. Set owner, group, and permissions of newly created files, as well as newly created ancestors. On user request, override default attributes of new files like block size, replication factor, etc. Allow for on-the-fly byte level transformations like UnGZippping, PGP decrypting, etc. Allow for on-the-fly unpacking of byte streams, like expanding tarballs, zips, etc. Perform quality checks on the destination files if requested. Emit real-time operational metrics (transfer speed, files completed, etc.) and allow for creating post-job summaries. Emit data availability notifications. Copy listings should be pluggable and fully dataset aware. Datasets can annotate data availability notifications, or modify aspects of the copy operation (like preserve attributes). Publishing should be pluggable and allow for easy extensions. Default publishing will simply place files in correct target locations. Extensions can register new files with Hive, etc. Reuse previously copied files that didn\u00e2\u0080\u0099t get published due to errors in the previous flow. Use other Gobblin features (e.g. proxying, password management).","title":"Problem Statement"},{"location":"adaptors/Gobblin-Distcp/#existing-solutions","text":"Distcp : Tool maintained by Hadoop. Allows copying files and syncing directories between FileSystem implementations (including HDFS, S3, and local file systems). Uses MapReduce to perform the copy. Has various features like preserving permissions and setting replication factors. Uses some heuristics to accelerate file listing generation (e.g. using directory mod time to determine if new files are likely to exist). Minimally dataset aware: e.g. can treat tracking and databases data differently. Can recover files that failed to publish in previous runs. Gobblin: Regular Gobblin can be used to read every record and re-write it to the new location. However, this involves actually deserializing records and has significant overhead.","title":"Existing Solutions"},{"location":"adaptors/Gobblin-Distcp/#proposed-design","text":"","title":"Proposed Design"},{"location":"adaptors/Gobblin-Distcp/#design-overview","text":"The core of Gobblin distcp is simply a traditional Gobblin flow with sources, converters, and writers that work directly with input streams. The work units are CopyableFiles, which contain all the metadata necessary to copy a single file, and the records are FileAwareInputStream, which is an input stream + CopyableFile.","title":"Design Overview"},{"location":"adaptors/Gobblin-Distcp/#example","text":"CopySource runs a DatasetFinder . DatasetFinder searches for all Dataset s. It creates a CopyableDataset for each Dataset . Each Dataset creates a copy listing for itself. CopySource creates a Gobblin WorkUnit for each CopyableFile . InputStreamExtractor opens an InputStream for each CopyableFile . InputStreamWriter creates the necessary file in destination and dumps the bytes of the InputStream . InputStreamWriter sets the correct owner and permissions, and puts files in writer-output location in the same directory structure as they will be published. DataPublisher groups work units by partition string and, for each partition string, moves the files to the destination. If a partition of a dataset failed to copy, all other successful partitions and datasets are published either way. The failed partition is staged for recovery on next run. DataPublisher emits notifications, performs Hive registration, etc.","title":"Example"},{"location":"adaptors/Gobblin-Distcp/#classes","text":"","title":"Classes"},{"location":"adaptors/Gobblin-Distcp/#copyabledataset","text":"An abstraction of a Dataset , i.e. a set of related files (for example a database table). Generates copy listings for that dataset. Example: if I want to replicate DB.Table to a new location, which files should I copy. Generates partitioning of copy listing into atomic units called file sets. A file set will be published nearly atomically. All files in the listing will be copied. It is the responsibility of the CopyableDataset to do a diff with the target (because it might have optimizations for performing the diff). Implementations: RecursiveCopyableDataset : copies all files under an root directory. StreamDataset : copies date-partitioned directories for Kafka topics. /** * Interface representing a dataset. */ public interface Dataset { /** * Deepest {@link org.apache.hadoop.fs.Path} that contains all files in the dataset. */ public Path datasetRoot(); } /** * {@link Dataset} that supports finding {@link CopyableFile}s. */ public interface CopyableDataset extends Dataset { /** * Find all {@link CopyableFile}s in this dataset. * * p * This method should return a collection of {@link CopyableFile}, each describing one file that should be copied * to the target. The returned collection should contain exactly one {@link CopyableFile} per file that should * be copied. Directories are created automatically, the returned collection should not include any directories. * See {@link CopyableFile} for explanation of the information contained in the {@link CopyableFile}s. * /p * * @param targetFs target {@link FileSystem} where copied files will be placed. * @param configuration {@link CopyConfiguration} for this job. See {@link CopyConfiguration}. * @return List of {@link CopyableFile}s in this dataset. * @throws IOException */ public Collection CopyableFile getCopyableFiles(FileSystem targetFs, CopyConfiguration configuration) throws IOException; }","title":"CopyableDataset"},{"location":"adaptors/Gobblin-Distcp/#datasetfinder","text":"Finds CopyableDataset s in the file system. Implementations: CopyableGlobDatasetFinder : Uses a glob and creates a RecursiveCopyableDataset for each matching directory. StreamDatasetFinder : Creates a StreamDataset for each directory in input directory. /** * Finds {@link Dataset}s in the file system. * * p * Concrete subclasses should have a constructor with signature * ({@link org.apache.hadoop.fs.FileSystem}, {@link java.util.Properties}). * /p */ public interface DatasetsFinder T extends Dataset { /** * Find all {@link Dataset}s in the file system. * @return List of {@link Dataset}s in the file system. * @throws IOException */ public List T findDatasets() throws IOException; /** * @return The deepest common root shared by all {@link Dataset}s root paths returned by this finder. */ public Path commonDatasetRoot(); }","title":"DatasetFinder"},{"location":"adaptors/Gobblin-Distcp/#copyablefile","text":"Structure containing information about a file that needs to be copied: Origin FileStatus . Destination path. Desired owner and permission. Attributes to be preserved (e.g. replication, block size). FileSet file belongs to (atomic units). Checksum. Metadata. Built with a builder with sensible defaults. Has a replicable guid that uniquely identifies origin file. Guid is a hash (sha1) of: Origin path. Origin length. Origin timestamp. Checksum if available.","title":"CopyableFile"},{"location":"adaptors/Gobblin-Distcp/#distcp-constructs","text":"Distcp runs as a Gobblin flow with special distcp constructs.","title":"Distcp Constructs"},{"location":"adaptors/Gobblin-Distcp/#copysource","text":"Source for Gobblin distcp. Flow: Instantiate a DatasetFinder . Use DatasetFinder to find CopyableDatasets . For each CopyableDataset get file listing. For each CopyableFile create a Gobblin WorkUnit . Serialize the CopyableFile into the WorkUnit . For each WorkUnit create a FileAwareInputStreamExtractor .","title":"CopySource"},{"location":"adaptors/Gobblin-Distcp/#fileawareinputstreamextractor","text":"Extractor for Gobblin distcp. Opens origin file and creates FileAwareInputStream containing the InputStream and the corresponding CopyableFile .","title":"FileAwareInputStreamExtractor"},{"location":"adaptors/Gobblin-Distcp/#distcpconverter","text":"Abstract class for distcp converters. Allows transformation of the InputStream (for example decrypting, de-archiving, etc.). Alters extensions to reflect the changes (eg. remove .gz). Implementations: DecryptConverter : Performs GPG decryption of the input. UnGzipConverter : Un-gzips the input. EncryptConverter : Performs GPG encryption of the input.","title":"DistcpConverter"},{"location":"adaptors/Gobblin-Distcp/#fileawareinputstreamdatawriter","text":"Gobblin writer for distcp. Takes a FileAwareInputStream and performs the copy of the file. Currently using a single DirectByteBuffer . Possible optimizations: Use two DirectByteBuffer s, while one is reading, the other one is writing. Sets target file attributes and permissions. Performs recovery of previous unpublished work.","title":"FileAwareInputStreamDataWriter"},{"location":"adaptors/Gobblin-Distcp/#tararchiveinputstreamdatawriter","text":"Extension of FileAwareInputStreamDataWriter . Takes a tar input stream and writes the contained sequence of files to the file system. Allows for automatic untaring on write. Example: a tarball containing files root/path/to/file, root/file2 will be expanded on the fly to get output/path/to/file and output/file2.","title":"TarArchiveInputStreamDataWriter"},{"location":"adaptors/Gobblin-Distcp/#copypublisher","text":"Groups work units by file set. For each file set, move the output files from staging location to final location in as few operations as possible (for near-atomicity). Recheck permissions of the output. Emit events indicating availability of published data. One event per file. One event per file set.","title":"CopyPublisher"},{"location":"adaptors/Gobblin-Distcp/#recovery-of-unpublished-files","text":"Copied files may fail to be published even after the copy has succeeded. Some reasons: Failed to set permissions. Other files in the same file set failed, preventing atomic publish. Wrong permissions for destination. Transient file system issues. When distcp detects a failure on the write step (e.g. setting owner and permissions), it will persist the uncommitted file to a separate location (Gobblin automatically deletes staging locations on exit). On the next run, distcp can identify files that were previously copied, and re-use them instead of repeating the data copy. The publish step uses \"exactly once\" feature: The publisher generates a set of publish steps (e.g. 1. move file to this location, 2. send event notifications, 3. commit watermark). The publish steps are written to a write-ahead log. The publish steps are executed. If the publish steps are successful, the write-ahead log is deleted. If the publish steps fail, the write-ahead log is preserved, and Gobblin will attempt to run them on the next execution. Relevant directories will not be deleted on exit. Eventually, write step should also use exactly-once feature.","title":"Recovery of unpublished files"},{"location":"adaptors/Gobblin-Distcp/#splitting-files-into-block-level-granularity-work-units","text":"Gobblin Distcp has an option to enable splitting of files into block level granularity work units, which involves the use of a helper class, DistcpFileSplitter , which has methods for: Splitting of files into block level work units, which is done at the CopySource ; the block level granularity is represented by an additional Split construct within each work unit that contains offset and ordering information. Merging of block level work units/splits, which is done at the CopyDataPublisher ; this uses calls to the FileSystem#concat API to append the separately copied entities of each file back together.","title":"Splitting files into block level granularity work units"},{"location":"adaptors/Gobblin-Distcp/#leverage","text":"Gobblin Distcp leverages Gobblin as its running framework, and most features available to Gobblin: Gobblin execution implementation Gobblin publishing implementation Gobblin metrics Gobblin on YARN Exactly once semantics Automatic Hive registration","title":"Leverage"},{"location":"adaptors/Gobblin-Distcp/#performance-scalability-and-provisioning","text":"There are two components in the flow: File listing and work unit generation: slow if there are too many files. Dataset aware optimizations are possible, as well as using services other than Hadoop ls call (like lsr or HDFS edit log), so this can be improved and should scale with the correct optimizations. Work unit generation is currently a serial process handled by Gobblin and could be a bottleneck. If we find it is a bottleneck, that process is parallelizable. Actual copy tasks: massively parallel using MR or many containers in YARN. Generally, it is the most expensive part of the flow. Although inputs can be split, HDFS does not support parallel writing to the same file, so large files will be a bottleneck (but this is true with distcp2 as well). This issue will be alleviated with the YARN executing model, where WorkUnits are allocated dynamically to containers (multiple small files can be copied in one container will another container copies a large file), and datasets can be publishes as soon as they are ready (remove impact from slow datasets). If this is an issue for a job in MR/with HDFS, Gobblin Distcp provides an option to enable splitting of files into block level granularity work units to be copied independently, then merged back together before publishing, which may help to reduce the mapper skew and alleviate the bottleneck. In direct byte copies, we have observed speeds that saturate the available network speed. Byte level transformations (e.g. decrypting) slow down the process, and also cannot be used with jobs that enable splitting.","title":"Performance, Scalability and Provisioning"},{"location":"adaptors/Gobblin-Distcp/#monitoring-and-alerting","text":"Monitoring and alerting will be done through Gobblin metrics. We will have real-time operational metrics available. Gobblin metrics automatically emits notifications for any failures as well as whenever data is available. Better SLAs can be achieved in the future through the use of continuous ingestion with priority queues.","title":"Monitoring and Alerting"},{"location":"adaptors/Gobblin-Distcp/#future-work","text":"There is currently work in progress to implement Gobblin Distcp on top of Hive. Gobblin Distcp will be capable of copying Hive tables and databases within and between Hadoop clusters.","title":"Future Work"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/","text":"Table of Contents Table of Contents Getting Started Job Constructs Source and Extractor Converter Writer Publisher Job Config Properties Metrics and Events Sample Job Getting Started Gobblin provides ready to use adapters for converting data in Avro to ORC . This page describes the steps to setup such a job. Note: The job requires Avro data to be registered in Hive. Gobblin Avro to ORC job leverages Hive for the conversion. Meaning, Gobblin does not read the Avro data record by record and convert each one of them to ORC, instead Gobblin executes hive queries to perform the conversion. This means that Avro data MUST be registred in hive for the converison to be possible. Below is a sample query. Example Conversion DDL INSERT OVERWRITE TABLE db_name_orc.table_orc PARTITION (year='2016') SELECT header.id, header.time, ... (more columns to select) ... ... FROM db_name_avro.table_avro WHERE year='2016'; Since Hive takes care of scaling the number of mappers/reducers required to perform the conversion, Gobblin does not run this job in MR mode. It runs in standalone mode. Each workunit converts a hive partition or a hive table (non partitioned tables). Each workunit/task executes one or more Hive DDLs. A gobblin task publishes data to a staging table first. The publisher moves data into the final table. The job supports schema evolution. Meaning any schema (compatible) changes on the Avro table are automatically made on the ORC table. By default publishing happens per dataset (dataset = table in this context). If a dataset fails, other datasets will still be published but the job will fail. The commit policy is configurable. Gobblin metrics is used to emit events when ORC data is published or when publish fails. Job Constructs Source and Extractor Gobblin provides HiveSource which is a generic source that connects to the hive metastore and creates WorkUnits for any Hive Partitions and Tables whitelisted. The HiveConvertExtractor is a Gobblin Extractor to extracts work for Avro to ORC conversion. The HiveSource uses the HiveDatasetFinder to find all hive tables and partitions that satisfy a whitelist. For each table/partition it creates a workunit is the updateTime is greater than the lowWatermark . By default a PartitionLevelWatermarker is used. This watermarker tracks watermarks for every partition of the table. Gobblin also provides a TableLevelWatermarker that keeps one watermark per table. The HiveConvertExtractor builds QueryBasedHiveConversionEntity s. The extractor makes necessary calls to the Hive Metastore to get table/partition metadata. The metadata is then wrapped into a QueryBasedHiveConversionEntity . Converter The converter builds the Hive DDLs/DMLs required to perform the Avro to ORC conversion. Gobblin supports conversion of Avro to both flattened ORC and nested ORC. The abstract converter AbstractAvroToOrcConverter builds DDLs/DMLs for any destination ORC format. Concrete subclass HiveAvroToFlattenedOrcConverter provides the configurations required for Avro to flattened ORC conversion and HiveAvroToNestedOrcConverter provides the configurations required for Avro to nested ORC conversion. In the job configurations, both converters can be chained to perform flattend and nested ORC conversion in the same job. Each converter can also be used independent of other. The converter builds the following different DDLs/DMLs Create staging table DDL - ORC data is written to a staging table first. The publisher then publishes them to the final ORC table. These DDLs are to create the staging table. A staging table looks like orc_db_name . orc_table_name _staging_ timestamp Create staging partition DDL - Similar to staging table but for a partition Conversion staging DML - This is the DML to select rows from Avro source table and insert them into the ORC staging table Create final table DDL (Optional) - This is the final ORC destination table. Creates the destination table is it does not exist Evolve final table DDLs (Optional) - Populate the schema evolution queries if required Drop partitions if exist in final table - DDL to drop a partition on destination if it already exists. Create final partition DDL - Create the ORC partition Drop staging table DDL - Cleanup the staging table after data is published from staging to final tables Writer The writer in this context executes the Hive DDLs/DMLs generated by the converter. HiveQueryExecutionWriter uses Hive JDBC connector to execute the DDLs. The DDLs write ORC data into staging tables. After the writer has completed HiveQueryExecutionWriter#write() , ORC data will be available in the staging tables. Publisher The publisher HiveConvertPublisher executes hive DDLs to publish staging ORC tables to final ORC tables. The publisher also cleans up staging tables. By default publishing happens per dataset (dataset = table in this context). If a dataset fails, other datasets will still be published but the job will fail. The commit policy is configurable. Job Config Properties These are some of the job config properties used by HiveAvroToOrcSource and HiveConvertExtractor . Configuration key Description Example value hive.dataset.whitelist Avro hive databases, tables to be converted db1 - any table under db1 passes. db1.table1 - only db1.table1 passes. db1.table* - any table under db1 whose name satisfies the pattern table* passes. db* - all tables from all databases whose names satisfy the pattern db* pass. db*.table* - db and table must satisfy the patterns db* and table* respectively db1.table1,db2.table2 - combine expressions for different databases with comma. db1.table1|table2 - combine expressions for same database with | . hive.dataset.blacklist Avro hive databases, tables not to converted Same as hive.dataset.whitelist examples gobblin.runtime.root.dir Root dir for gobblin state store, staging, output etc. /jobs/user/avroToOrc hive.source.maximum.lookbackDays Partitions older than this value will not be processed. The default value is set to 3. So if an Avro partition older than 3 days gets modified, the job will not convert the new changes. 3 hive.source.watermarker.class The type of watermark to use. Watermark can be per partition or per table. The default is gobblin.data.management.conversion.hive.watermarker.PartitionLevelWatermarker gobblin.data.management.conversion.hive.watermarker.PartitionLevelWatermarker gobblin.data.management.conversion.hive.watermarker.TableLevelWatermarker taskexecutor.threadpool.size Maximum number of parallel conversion hive queries to run. This is the standard gobblin property to control the number of parallel tasks (threads). This is set to a default of 50 because each task queries the hive metastore. So this property also limits the number of parallel metastore connections 50 hive.conversion.avro.flattenedOrc.destination.dbName Name of the ORC database $DB is the Avro database name. E.g. If avro database name is tracking, $DB will be resolved at runtime to tracking. Setting the value to $DB_column will result in a ORC table name of tracking_column hive.conversion.avro.flattenedOrc.destination.tableName Name of the ORC table $TABLE is the Avro table name. E.g. If avro table name is LogEvent, $TABLE will be resolved at runtime to LogEvent. Setting the value of this property to $TABLE will cause the ORC table name to be same as Avro table name. Setting the value to $TABLE_orc will result in a ORC table name of LogEvent_orc hive.conversion.avro.flattenedOrc.destination.dataPath Location on HDFS where ORC data is published /events_orc/$DB/$TABLE hive.conversion.avro.flattenedOrc.evolution.enabled Decides if schema evolution is enabled true/false hive.conversion.avro.flattenedOrc.hiveRuntime.* Additional hive properties to be set while executing the conversion DDL. Prefix any hive standard properties with this key hive.conversion.avro.flattenedOrc.hiveRuntime.mapred.map.tasks=10 hive.conversion.avro.destinationFormats A comma separated list of destination formats. Currently supports nestedOrc and flattenedOrc flattenedOrc,nestedOrc Metrics and Events SLA event is published every time an Avro partition/table is converted to ORC. Each SLA event has the following metadata. { ## Publish timestamp timestamp : 1470229945441 , namespace : gobblin.hive.conversion , name : gobblin.hive.conversion.ConversionSuccessful , metadata : { ## Azkaban metadata (If running on Azkaban) azkabanExecId : 880060 , azkabanFlowId : azkaban_flow_name , azkabanJobId : azkaban_job_name , azkabanProjectName : azkaban_project_name , jobId : job_AvroToOrcConversion_1470227416023 , jobName : AvroToOrcConversion , ## Dataset and Partition metadata datasetUrn : events@logevent , sourceDataLocation : hdfs:// host : port /events/LogEvent/2016/08/03/04 , partition : datepartition=2016-08-03-04 , schemaEvolutionDDLNum : 0 , ## Begin and End time metadata for each phase beginConversionDDLExecuteTime : 1470227453370 , beginDDLBuildTime : 1470227452382 , beginGetWorkunitsTime : 1470227428136 , beginPublishDDLExecuteTime : 1470229944141 , endConversionDDLExecuteTime : 1470227928486 , endDDLBuildTime : 1470227452382 , endPublishDDLExecuteTime : 1470229945440 , originTimestamp : 1470227446703 , previousPublishTs : 1470223843230 , upstreamTimestamp : 1470226593984 , workunitCreateTime : 1470227446703 ## Gobblin metrics metadata class : org.apache.gobblin.data.management.conversion.hive.publisher.HiveConvertPublisher , metricContextID : 20bfb2a2-0592-4f53-9259-c8ee125f90a8 , metricContextName : org.apache.gobblin.data.management.conversion.hive.publisher.HiveConvertPublisher.781426901 , } } The diagram below describes timestamps captured in the SLA event. Sample Job # Avro hive databases and tables to convert hive.dataset.whitelist=events.LogEvent|LoginEvent data.publisher.type=org.apache.gobblin.data.management.conversion.hive.publisher.HiveConvertPublisher source.class=org.apache.gobblin.data.management.conversion.hive.source.HiveAvroToOrcSource writer.builder.class=org.apache.gobblin.data.management.conversion.hive.writer.HiveQueryWriterBuilder converter.classes=org.apache.gobblin.data.management.conversion.hive.converter.HiveAvroToFlattenedOrcConverter,org.apache.gobblin.data.management.conversion.hive.converter.HiveAvroToNestedOrcConverter hive.dataset.finder.class=org.apache.gobblin.data.management.conversion.hive.dataset.ConvertibleHiveDatasetFinder # Only flattened orc is enabled hive.conversion.avro.destinationFormats=flattenedOrc hive.conversion.avro.flattenedOrc.destination.dataPath=/events_orc/ # Avro table name _orc hive.conversion.avro.flattenedOrc.destination.tableName=$TABLE_orc # Same as Avro table name hive.conversion.avro.flattenedOrc.destination.dbName=$DB hive.conversion.avro.flattenedOrc.evolution.enabled=true hive.conversion.avro.flattenedOrc.source.dataPathIdentifier=daily,hourly # No host and port required. Hive starts an embedded hiveserver2 hiveserver.connection.string=jdbc:hive2:// ## Maximum lookback hive.source.maximum.lookbackDays=3 ## Gobblin standard properties ## task.maxretries=1 taskexecutor.threadpool.size=75 workunit.retry.enabled=true # Gobblin framework locations mr.job.root.dir=/jobs/working state.store.dir=/jobs/state_store writer.staging.dir=/jobs/writer_staging writer.output.dir=/jobs/writer_output # Metrics metrics.enabled=true metrics.reporting.kafka.enabled=true metrics.reporting.kafka.format=avro metrics.reporting.kafka.avro.use.schema.registry=true metrics.reporting.kafka.topic.metrics=MetricReport launcher.type=LOCAL classpath=lib/*","title":"Hive Avro-To-Orc Converter"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#table-of-contents","text":"Table of Contents Getting Started Job Constructs Source and Extractor Converter Writer Publisher Job Config Properties Metrics and Events Sample Job","title":"Table of Contents"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#getting-started","text":"Gobblin provides ready to use adapters for converting data in Avro to ORC . This page describes the steps to setup such a job. Note: The job requires Avro data to be registered in Hive. Gobblin Avro to ORC job leverages Hive for the conversion. Meaning, Gobblin does not read the Avro data record by record and convert each one of them to ORC, instead Gobblin executes hive queries to perform the conversion. This means that Avro data MUST be registred in hive for the converison to be possible. Below is a sample query. Example Conversion DDL INSERT OVERWRITE TABLE db_name_orc.table_orc PARTITION (year='2016') SELECT header.id, header.time, ... (more columns to select) ... ... FROM db_name_avro.table_avro WHERE year='2016'; Since Hive takes care of scaling the number of mappers/reducers required to perform the conversion, Gobblin does not run this job in MR mode. It runs in standalone mode. Each workunit converts a hive partition or a hive table (non partitioned tables). Each workunit/task executes one or more Hive DDLs. A gobblin task publishes data to a staging table first. The publisher moves data into the final table. The job supports schema evolution. Meaning any schema (compatible) changes on the Avro table are automatically made on the ORC table. By default publishing happens per dataset (dataset = table in this context). If a dataset fails, other datasets will still be published but the job will fail. The commit policy is configurable. Gobblin metrics is used to emit events when ORC data is published or when publish fails.","title":"Getting Started"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#job-constructs","text":"","title":"Job Constructs"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#source-and-extractor","text":"Gobblin provides HiveSource which is a generic source that connects to the hive metastore and creates WorkUnits for any Hive Partitions and Tables whitelisted. The HiveConvertExtractor is a Gobblin Extractor to extracts work for Avro to ORC conversion. The HiveSource uses the HiveDatasetFinder to find all hive tables and partitions that satisfy a whitelist. For each table/partition it creates a workunit is the updateTime is greater than the lowWatermark . By default a PartitionLevelWatermarker is used. This watermarker tracks watermarks for every partition of the table. Gobblin also provides a TableLevelWatermarker that keeps one watermark per table. The HiveConvertExtractor builds QueryBasedHiveConversionEntity s. The extractor makes necessary calls to the Hive Metastore to get table/partition metadata. The metadata is then wrapped into a QueryBasedHiveConversionEntity .","title":"Source and Extractor"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#converter","text":"The converter builds the Hive DDLs/DMLs required to perform the Avro to ORC conversion. Gobblin supports conversion of Avro to both flattened ORC and nested ORC. The abstract converter AbstractAvroToOrcConverter builds DDLs/DMLs for any destination ORC format. Concrete subclass HiveAvroToFlattenedOrcConverter provides the configurations required for Avro to flattened ORC conversion and HiveAvroToNestedOrcConverter provides the configurations required for Avro to nested ORC conversion. In the job configurations, both converters can be chained to perform flattend and nested ORC conversion in the same job. Each converter can also be used independent of other. The converter builds the following different DDLs/DMLs Create staging table DDL - ORC data is written to a staging table first. The publisher then publishes them to the final ORC table. These DDLs are to create the staging table. A staging table looks like orc_db_name . orc_table_name _staging_ timestamp Create staging partition DDL - Similar to staging table but for a partition Conversion staging DML - This is the DML to select rows from Avro source table and insert them into the ORC staging table Create final table DDL (Optional) - This is the final ORC destination table. Creates the destination table is it does not exist Evolve final table DDLs (Optional) - Populate the schema evolution queries if required Drop partitions if exist in final table - DDL to drop a partition on destination if it already exists. Create final partition DDL - Create the ORC partition Drop staging table DDL - Cleanup the staging table after data is published from staging to final tables","title":"Converter"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#writer","text":"The writer in this context executes the Hive DDLs/DMLs generated by the converter. HiveQueryExecutionWriter uses Hive JDBC connector to execute the DDLs. The DDLs write ORC data into staging tables. After the writer has completed HiveQueryExecutionWriter#write() , ORC data will be available in the staging tables.","title":"Writer"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#publisher","text":"The publisher HiveConvertPublisher executes hive DDLs to publish staging ORC tables to final ORC tables. The publisher also cleans up staging tables. By default publishing happens per dataset (dataset = table in this context). If a dataset fails, other datasets will still be published but the job will fail. The commit policy is configurable.","title":"Publisher"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#job-config-properties","text":"These are some of the job config properties used by HiveAvroToOrcSource and HiveConvertExtractor . Configuration key Description Example value hive.dataset.whitelist Avro hive databases, tables to be converted db1 - any table under db1 passes. db1.table1 - only db1.table1 passes. db1.table* - any table under db1 whose name satisfies the pattern table* passes. db* - all tables from all databases whose names satisfy the pattern db* pass. db*.table* - db and table must satisfy the patterns db* and table* respectively db1.table1,db2.table2 - combine expressions for different databases with comma. db1.table1|table2 - combine expressions for same database with | . hive.dataset.blacklist Avro hive databases, tables not to converted Same as hive.dataset.whitelist examples gobblin.runtime.root.dir Root dir for gobblin state store, staging, output etc. /jobs/user/avroToOrc hive.source.maximum.lookbackDays Partitions older than this value will not be processed. The default value is set to 3. So if an Avro partition older than 3 days gets modified, the job will not convert the new changes. 3 hive.source.watermarker.class The type of watermark to use. Watermark can be per partition or per table. The default is gobblin.data.management.conversion.hive.watermarker.PartitionLevelWatermarker gobblin.data.management.conversion.hive.watermarker.PartitionLevelWatermarker gobblin.data.management.conversion.hive.watermarker.TableLevelWatermarker taskexecutor.threadpool.size Maximum number of parallel conversion hive queries to run. This is the standard gobblin property to control the number of parallel tasks (threads). This is set to a default of 50 because each task queries the hive metastore. So this property also limits the number of parallel metastore connections 50 hive.conversion.avro.flattenedOrc.destination.dbName Name of the ORC database $DB is the Avro database name. E.g. If avro database name is tracking, $DB will be resolved at runtime to tracking. Setting the value to $DB_column will result in a ORC table name of tracking_column hive.conversion.avro.flattenedOrc.destination.tableName Name of the ORC table $TABLE is the Avro table name. E.g. If avro table name is LogEvent, $TABLE will be resolved at runtime to LogEvent. Setting the value of this property to $TABLE will cause the ORC table name to be same as Avro table name. Setting the value to $TABLE_orc will result in a ORC table name of LogEvent_orc hive.conversion.avro.flattenedOrc.destination.dataPath Location on HDFS where ORC data is published /events_orc/$DB/$TABLE hive.conversion.avro.flattenedOrc.evolution.enabled Decides if schema evolution is enabled true/false hive.conversion.avro.flattenedOrc.hiveRuntime.* Additional hive properties to be set while executing the conversion DDL. Prefix any hive standard properties with this key hive.conversion.avro.flattenedOrc.hiveRuntime.mapred.map.tasks=10 hive.conversion.avro.destinationFormats A comma separated list of destination formats. Currently supports nestedOrc and flattenedOrc flattenedOrc,nestedOrc","title":"Job Config Properties"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#metrics-and-events","text":"SLA event is published every time an Avro partition/table is converted to ORC. Each SLA event has the following metadata. { ## Publish timestamp timestamp : 1470229945441 , namespace : gobblin.hive.conversion , name : gobblin.hive.conversion.ConversionSuccessful , metadata : { ## Azkaban metadata (If running on Azkaban) azkabanExecId : 880060 , azkabanFlowId : azkaban_flow_name , azkabanJobId : azkaban_job_name , azkabanProjectName : azkaban_project_name , jobId : job_AvroToOrcConversion_1470227416023 , jobName : AvroToOrcConversion , ## Dataset and Partition metadata datasetUrn : events@logevent , sourceDataLocation : hdfs:// host : port /events/LogEvent/2016/08/03/04 , partition : datepartition=2016-08-03-04 , schemaEvolutionDDLNum : 0 , ## Begin and End time metadata for each phase beginConversionDDLExecuteTime : 1470227453370 , beginDDLBuildTime : 1470227452382 , beginGetWorkunitsTime : 1470227428136 , beginPublishDDLExecuteTime : 1470229944141 , endConversionDDLExecuteTime : 1470227928486 , endDDLBuildTime : 1470227452382 , endPublishDDLExecuteTime : 1470229945440 , originTimestamp : 1470227446703 , previousPublishTs : 1470223843230 , upstreamTimestamp : 1470226593984 , workunitCreateTime : 1470227446703 ## Gobblin metrics metadata class : org.apache.gobblin.data.management.conversion.hive.publisher.HiveConvertPublisher , metricContextID : 20bfb2a2-0592-4f53-9259-c8ee125f90a8 , metricContextName : org.apache.gobblin.data.management.conversion.hive.publisher.HiveConvertPublisher.781426901 , } } The diagram below describes timestamps captured in the SLA event.","title":"Metrics and Events"},{"location":"adaptors/Hive-Avro-To-ORC-Converter/#sample-job","text":"# Avro hive databases and tables to convert hive.dataset.whitelist=events.LogEvent|LoginEvent data.publisher.type=org.apache.gobblin.data.management.conversion.hive.publisher.HiveConvertPublisher source.class=org.apache.gobblin.data.management.conversion.hive.source.HiveAvroToOrcSource writer.builder.class=org.apache.gobblin.data.management.conversion.hive.writer.HiveQueryWriterBuilder converter.classes=org.apache.gobblin.data.management.conversion.hive.converter.HiveAvroToFlattenedOrcConverter,org.apache.gobblin.data.management.conversion.hive.converter.HiveAvroToNestedOrcConverter hive.dataset.finder.class=org.apache.gobblin.data.management.conversion.hive.dataset.ConvertibleHiveDatasetFinder # Only flattened orc is enabled hive.conversion.avro.destinationFormats=flattenedOrc hive.conversion.avro.flattenedOrc.destination.dataPath=/events_orc/ # Avro table name _orc hive.conversion.avro.flattenedOrc.destination.tableName=$TABLE_orc # Same as Avro table name hive.conversion.avro.flattenedOrc.destination.dbName=$DB hive.conversion.avro.flattenedOrc.evolution.enabled=true hive.conversion.avro.flattenedOrc.source.dataPathIdentifier=daily,hourly # No host and port required. Hive starts an embedded hiveserver2 hiveserver.connection.string=jdbc:hive2:// ## Maximum lookback hive.source.maximum.lookbackDays=3 ## Gobblin standard properties ## task.maxretries=1 taskexecutor.threadpool.size=75 workunit.retry.enabled=true # Gobblin framework locations mr.job.root.dir=/jobs/working state.store.dir=/jobs/state_store writer.staging.dir=/jobs/writer_staging writer.output.dir=/jobs/writer_output # Metrics metrics.enabled=true metrics.reporting.kafka.enabled=true metrics.reporting.kafka.format=avro metrics.reporting.kafka.avro.use.schema.registry=true metrics.reporting.kafka.topic.metrics=MetricReport launcher.type=LOCAL classpath=lib/*","title":"Sample Job"},{"location":"case-studies/Hive-Distcp/","text":"Table of Contents Table of Contents Introduction Configure Hive Distcp Job Source and target metastores Database and tables to copy Target path computation Conflicting table and partitions treatment Deregistering tables / partitions Finding copy files Partition Filter Fast partition skip predicate Introduction Gobblin hive distcp is built on top of Gobblin distcp . It uses Hive metastore to find datasets to copy, then performs regular file listings to find the actual files to copy. After finishing the copy, the Hive registrations in the source are replicated on the target. This document will show an sample job config of running Gobblin hive distcp, and explain how it works. Configure Hive Distcp Job Below is the sample job config of running Gobblin hive distcp. Gobblin job constructs and data flow are the same as Gobblin distcp . The only difference is the gobblin.data.profile.class and hive related properties. job.name=SampleHiveDistcp job.group=HiveDistcp job.description=Sample job config for hive distcp extract.namespace=org.apache.gobblin.copy.tracking gobblin.dataset.profile.class=org.apache.gobblin.data.management.copy.hive.HiveDatasetFinder data.publisher.type=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher source.class=org.apache.gobblin.data.management.copy.CopySource writer.builder.class=org.apache.gobblin.data.management.copy.writer.FileAwareInputStreamDataWriterBuilder converter.classes=org.apache.gobblin.converter.IdentityConverter hive.dataset.copy.target.table.prefixToBeReplaced= hive.dataset.copy.target.table.prefixReplacement= data.publisher.final.dir=${hive.dataset.copy.target.table.prefixReplacement} hive.dataset.hive.metastore.uri= hive.dataset.copy.target.metastore.uri= hive.dataset.whitelist= hive.dataset.copy.target.database= hive.dataset.existing.entity.conflict.policy=REPLACE_PARTITIONS hive.dataset.copy.deregister.fileDeleteMethod=NO_DELETE hive.dataset.copy.location.listing.method=RECURSIVE hive.dataset.copy.locations.listing.skipHiddenPaths=true gobblin.copy.preserved.attributes=rgbp Source and target metastores hive.dataset.hive.metastore.uri and hive.dataset.copy.target.metastore.uri specify the source and target metastore uri. Make sure the hive distcp job has access to both hive metastores. Database and tables to copy Use a whitelist and optionally a blacklist using to specify tables to copy using the keys hive.dataset.whitelist and hive.dataset.blacklist . Both whitelist and blacklist accept various patterns, for example: sampleDb.sampleTable - specific table sampleTable in database sampleDb ; sampleDb - all tables in database sampleDb ; sampleDb.samplei* - specific tables starting with sample in database sampleDb . The key hive.dataset.copy.target.database specifies the target database to create tables under. If omitted, will use the same as the source. Target path computation This specifies where copied files should be placed. There are a few options on how the target paths will be computed: Prefix replacement: simply replace a prefix in each file copied, e.g. /a/b to /a/btest. Use the keys hive.dataset.copy.target.table.prefixToBeReplaced and hive.dataset.copy.target.table.prefixReplacement . Any paths that are not a descendant of prefixToBeReplaced will throw an error and will fail the dataset. Note that setting both keys to \"/\" effectively replicates all paths exactly. New table root: Puts files in a new table root. The source table root is its location (which is a Hive registration parameter). This mode will simply do a prefix replacement of the table root for each path in that table. Use the key hive.dataset.copy.target.table.root to specify the replacement. Note there is some primitive token replacement in the value of the key if using the tokens $DB and $TABLE, which will be replaced by the database and table name respectively. If the token $TABLE is not present, however, the table name will be automatically appended to the new table root (see last example below). /data/$DB/$TABLE - /data/databaseName/tableName /data/$TABLE - /data/tableName /data - /data/tableName Relocate files: This mode will move all files in a table to a structure matching Hive's native directory structure. I.e. all files for a partition \"abc\" of table \"myDb.myTable\" will be placed at path \" /abc\" where prefix is specified using the key hive.dataset.copy.target.table.root and processed with the token replacements explained in \"new table root\". To enable this mode set hive.dataset.copy.relocate.data.files to true and set hive.dataset.copy.target.table.root appropriately. Conflicting table and partitions treatment If distcp-ng finds that a partition or table it needs to create already exists it will determine whether the existing table / partition is identical to what it would register (e.g. compare schema, location, etc.). If not, it will use a policy to determine how to proceed. The policy is specified using the key hive.dataset.existing.entity.conflict.policy and can take the following values: ABORT: the conflicting table will not be copied (default) REPLACE_PARTITIONS: replace any conflicting partitions, but not tables REPLACE_TABLES: replace any conflicting tables by deregistrating previous tables first. UPDATE_TABLES: Keep the original-registered table but make modification. Deregistering tables / partitions Sometimes distcp-ng must deregister a table / partition, for example if it doesn't exist in the source, or if it must be replaced. In this case, distcp-ng offers options on what to do with the files under the deregistered partition. Set this policy using the key hive.dataset.copy.deregister.fileDeleteMethod which can take the following values: NO_DELETE: do not delete the files (default) INPUT_FORMAT: use the table / partition input format to infer which files are actually used by that table / partition, and delete only those files. RECURSIVE: delete the entire directory in the table / partition location. Finding copy files To specify the files that distcp will copy for each table / partition, use the key hive.dataset.copy.location.listing.method which can take the values: INPUT_FORMAT: use the table / partition input format to infer which files are actually used by that table / partition. (default) RECURSIVE: copy all files under the directory in the table / partition location recursively. If the recursive method is used, user can additionally specify hive.dataset.copy.locations.listing.skipHiddenPaths , which, if true, will not copy any hidden files. Partition Filter A partition filter can be applied when copying partitioned tables. Filters can only be applied to text partition columns. To speficy a partition filter use the key hive.dataset.copy.partition.filter.generator . gobblin.data.management.copy.hive.filter.LookbackPartitionFilterGenerator : Filters date-representing partitions by a lookback (i.e. only copy recent partitions). Use the keys hive.dataset.partition.filter.datetime.column , hive.dataset.partition.filter.datetime.lookback , and hive.dataset.partition.filter.datetime.format to configure the filter. Fast partition skip predicate A predicate that operates on partitions can be provided to distcp-ng to allow it to quickly skip partitions without having to list all of the source and target files and do a diff on those sets (a costly operation). To set this predicate, provide the class name of the predicate with the key hive.dataset.copy.fast.partition.skip.predicate . Currently only one such predicate exists: RegistrationTimeSkipPredicate : This predicate compares the Hive partition attribute registrationGenerationTimeMillis in the target with the modification time of the partition directory in the source. The partition is skipped unless the directory was modified more recently than the registrationGenerationTime. The attribute registrationGenerationTimeMillis is an attribute set by distcp-ng representing (for all practical purposes) the time at which the distcp-ng job that registered that table started.","title":"Hive Distcp"},{"location":"case-studies/Hive-Distcp/#table-of-contents","text":"Table of Contents Introduction Configure Hive Distcp Job Source and target metastores Database and tables to copy Target path computation Conflicting table and partitions treatment Deregistering tables / partitions Finding copy files Partition Filter Fast partition skip predicate","title":"Table of Contents"},{"location":"case-studies/Hive-Distcp/#introduction","text":"Gobblin hive distcp is built on top of Gobblin distcp . It uses Hive metastore to find datasets to copy, then performs regular file listings to find the actual files to copy. After finishing the copy, the Hive registrations in the source are replicated on the target. This document will show an sample job config of running Gobblin hive distcp, and explain how it works.","title":"Introduction"},{"location":"case-studies/Hive-Distcp/#configure-hive-distcp-job","text":"Below is the sample job config of running Gobblin hive distcp. Gobblin job constructs and data flow are the same as Gobblin distcp . The only difference is the gobblin.data.profile.class and hive related properties. job.name=SampleHiveDistcp job.group=HiveDistcp job.description=Sample job config for hive distcp extract.namespace=org.apache.gobblin.copy.tracking gobblin.dataset.profile.class=org.apache.gobblin.data.management.copy.hive.HiveDatasetFinder data.publisher.type=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher source.class=org.apache.gobblin.data.management.copy.CopySource writer.builder.class=org.apache.gobblin.data.management.copy.writer.FileAwareInputStreamDataWriterBuilder converter.classes=org.apache.gobblin.converter.IdentityConverter hive.dataset.copy.target.table.prefixToBeReplaced= hive.dataset.copy.target.table.prefixReplacement= data.publisher.final.dir=${hive.dataset.copy.target.table.prefixReplacement} hive.dataset.hive.metastore.uri= hive.dataset.copy.target.metastore.uri= hive.dataset.whitelist= hive.dataset.copy.target.database= hive.dataset.existing.entity.conflict.policy=REPLACE_PARTITIONS hive.dataset.copy.deregister.fileDeleteMethod=NO_DELETE hive.dataset.copy.location.listing.method=RECURSIVE hive.dataset.copy.locations.listing.skipHiddenPaths=true gobblin.copy.preserved.attributes=rgbp","title":"Configure Hive Distcp Job"},{"location":"case-studies/Hive-Distcp/#source-and-target-metastores","text":"hive.dataset.hive.metastore.uri and hive.dataset.copy.target.metastore.uri specify the source and target metastore uri. Make sure the hive distcp job has access to both hive metastores.","title":"Source and target metastores"},{"location":"case-studies/Hive-Distcp/#database-and-tables-to-copy","text":"Use a whitelist and optionally a blacklist using to specify tables to copy using the keys hive.dataset.whitelist and hive.dataset.blacklist . Both whitelist and blacklist accept various patterns, for example: sampleDb.sampleTable - specific table sampleTable in database sampleDb ; sampleDb - all tables in database sampleDb ; sampleDb.samplei* - specific tables starting with sample in database sampleDb . The key hive.dataset.copy.target.database specifies the target database to create tables under. If omitted, will use the same as the source.","title":"Database and tables to copy"},{"location":"case-studies/Hive-Distcp/#target-path-computation","text":"This specifies where copied files should be placed. There are a few options on how the target paths will be computed: Prefix replacement: simply replace a prefix in each file copied, e.g. /a/b to /a/btest. Use the keys hive.dataset.copy.target.table.prefixToBeReplaced and hive.dataset.copy.target.table.prefixReplacement . Any paths that are not a descendant of prefixToBeReplaced will throw an error and will fail the dataset. Note that setting both keys to \"/\" effectively replicates all paths exactly. New table root: Puts files in a new table root. The source table root is its location (which is a Hive registration parameter). This mode will simply do a prefix replacement of the table root for each path in that table. Use the key hive.dataset.copy.target.table.root to specify the replacement. Note there is some primitive token replacement in the value of the key if using the tokens $DB and $TABLE, which will be replaced by the database and table name respectively. If the token $TABLE is not present, however, the table name will be automatically appended to the new table root (see last example below). /data/$DB/$TABLE - /data/databaseName/tableName /data/$TABLE - /data/tableName /data - /data/tableName Relocate files: This mode will move all files in a table to a structure matching Hive's native directory structure. I.e. all files for a partition \"abc\" of table \"myDb.myTable\" will be placed at path \" /abc\" where prefix is specified using the key hive.dataset.copy.target.table.root and processed with the token replacements explained in \"new table root\". To enable this mode set hive.dataset.copy.relocate.data.files to true and set hive.dataset.copy.target.table.root appropriately.","title":"Target path computation"},{"location":"case-studies/Hive-Distcp/#conflicting-table-and-partitions-treatment","text":"If distcp-ng finds that a partition or table it needs to create already exists it will determine whether the existing table / partition is identical to what it would register (e.g. compare schema, location, etc.). If not, it will use a policy to determine how to proceed. The policy is specified using the key hive.dataset.existing.entity.conflict.policy and can take the following values: ABORT: the conflicting table will not be copied (default) REPLACE_PARTITIONS: replace any conflicting partitions, but not tables REPLACE_TABLES: replace any conflicting tables by deregistrating previous tables first. UPDATE_TABLES: Keep the original-registered table but make modification.","title":"Conflicting table and partitions treatment"},{"location":"case-studies/Hive-Distcp/#deregistering-tables-partitions","text":"Sometimes distcp-ng must deregister a table / partition, for example if it doesn't exist in the source, or if it must be replaced. In this case, distcp-ng offers options on what to do with the files under the deregistered partition. Set this policy using the key hive.dataset.copy.deregister.fileDeleteMethod which can take the following values: NO_DELETE: do not delete the files (default) INPUT_FORMAT: use the table / partition input format to infer which files are actually used by that table / partition, and delete only those files. RECURSIVE: delete the entire directory in the table / partition location.","title":"Deregistering tables / partitions"},{"location":"case-studies/Hive-Distcp/#finding-copy-files","text":"To specify the files that distcp will copy for each table / partition, use the key hive.dataset.copy.location.listing.method which can take the values: INPUT_FORMAT: use the table / partition input format to infer which files are actually used by that table / partition. (default) RECURSIVE: copy all files under the directory in the table / partition location recursively. If the recursive method is used, user can additionally specify hive.dataset.copy.locations.listing.skipHiddenPaths , which, if true, will not copy any hidden files.","title":"Finding copy files"},{"location":"case-studies/Hive-Distcp/#partition-filter","text":"A partition filter can be applied when copying partitioned tables. Filters can only be applied to text partition columns. To speficy a partition filter use the key hive.dataset.copy.partition.filter.generator . gobblin.data.management.copy.hive.filter.LookbackPartitionFilterGenerator : Filters date-representing partitions by a lookback (i.e. only copy recent partitions). Use the keys hive.dataset.partition.filter.datetime.column , hive.dataset.partition.filter.datetime.lookback , and hive.dataset.partition.filter.datetime.format to configure the filter.","title":"Partition Filter"},{"location":"case-studies/Hive-Distcp/#fast-partition-skip-predicate","text":"A predicate that operates on partitions can be provided to distcp-ng to allow it to quickly skip partitions without having to list all of the source and target files and do a diff on those sets (a costly operation). To set this predicate, provide the class name of the predicate with the key hive.dataset.copy.fast.partition.skip.predicate . Currently only one such predicate exists: RegistrationTimeSkipPredicate : This predicate compares the Hive partition attribute registrationGenerationTimeMillis in the target with the modification time of the partition directory in the source. The partition is skipped unless the directory was modified more recently than the registrationGenerationTime. The attribute registrationGenerationTimeMillis is an attribute set by distcp-ng representing (for all practical purposes) the time at which the distcp-ng job that registered that table started.","title":"Fast partition skip predicate"},{"location":"case-studies/Kafka-HDFS-Ingestion/","text":"Table of Contents Table of Contents Getting Started Standalone MapReduce Job Constructs Source and Extractor Writer and Publisher Job Config Properties Metrics and Events Task Level Metrics Task Level Events Job Level Metrics Job Level Events Grouping Workunits Single-Level Packing Bi-Level Packing Average Record Size-Based Workunit Size Estimator Average Record Time-Based Workunit Size Estimator Topic-Specific Configuration Kafka Deserializer Integration Gobblin Deserializer Implementations KafkaGsonDeserializer Comparison with KafkaSimpleSource Confluent Integration Confluent Schema Registry Confluent Deserializers KafkaAvroDeserializer KafkaJsonDeserializer Getting Started This section helps you set up a quick-start job for ingesting Kafka topics on a single machine. We provide quick start examples in both standalone and MapReduce mode. Standalone Setup a single node Kafka broker by following the Kafka quick start guide . Suppose your broker URI is localhost:9092 , and you've created a topic \"test\" with two events \"This is a message\" and \"This is a another message\". The remaining steps are the same as the Wikipedia example , except using the following job config properties: job.name=GobblinKafkaQuickStart job.group=GobblinKafka job.description=Gobblin quick start job for Kafka job.lock.enabled=false kafka.brokers=localhost:9092 source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource extract.namespace=org.apache.gobblin.extract.kafka writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder writer.file.path.type=tablename writer.destination.type=HDFS writer.output.format=txt data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher mr.job.max.mappers=1 metrics.reporting.file.enabled=true metrics.log.dir=${gobblin.cluster.work.dir}/metrics metrics.reporting.file.suffix=txt bootstrap.with.offset=earliest After the job finishes, the following messages should be in the job log: INFO Pulling topic test INFO Pulling partition test:0 from offset 0 to 2, range=2 INFO Finished pulling partition test:0 INFO Finished pulling topic test INFO Extracted 2 data records INFO Actual high watermark for partition test:0=2, expected=2 INFO Task task_id completed in 31212ms with state SUCCESSFUL The output file will be in {gobblin.cluster.work.dir}/job-output/test , with the two messages you've just created in the Kafka broker. {gobblin.cluster.work.dir}/metrics will contain metrics collected from this run. MapReduce Setup a single node Kafka broker same as in standalone mode. Setup a single node Hadoop cluster by following the steps in Hadoop: Setting up a Single Node Cluster . Suppose your HDFS URI is hdfs://localhost:9000 . Create a job config file with the following properties: job.name=GobblinKafkaQuickStart job.group=GobblinKafka job.description=Gobblin quick start job for Kafka job.lock.enabled=false kafka.brokers=localhost:9092 source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource extract.namespace=org.apache.gobblin.extract.kafka writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder writer.file.path.type=tablename writer.destination.type=HDFS writer.output.format=txt data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher mr.job.max.mappers=1 metrics.reporting.file.enabled=true metrics.log.dir=/gobblin-kafka/metrics metrics.reporting.file.suffix=txt bootstrap.with.offset=earliest fs.uri=hdfs://localhost:9000 writer.fs.uri=hdfs://localhost:9000 state.store.fs.uri=hdfs://localhost:9000 mr.job.root.dir=/gobblin-kafka/working state.store.dir=/gobblin-kafka/state-store task.data.root.dir=/jobs/kafkaetl/gobblin/gobblin-kafka/task-data data.publisher.final.dir=/gobblintest/job-output Run gobblin-mapreduce.sh : gobblin-mapreduce.sh --conf path-to-job-config-file After the job finishes, the job output file will be in /gobblintest/job-output/test in HDFS, and the metrics will be in /gobblin-kafka/metrics . Job Constructs Source and Extractor Gobblin provides two abstract classes, KafkaSource and KafkaExtractor . KafkaSource creates a workunit for each Kafka topic partition to be pulled, then merges and groups the workunits based on the desired number of workunits specified by property mr.job.max.mappers (this property is used in both standalone and MR mode). More details about how workunits are merged and grouped is available here . KafkaExtractor extracts the partitions assigned to a workunit, based on the specified low watermark and high watermark. To use them in a Kafka-HDFS ingestion job, one should subclass KafkaExtractor and implement method decodeRecord(MessageAndOffset) , which takes a MessageAndOffset object pulled from the Kafka broker and decodes it into a desired object. One should also subclass KafkaSource and implement getExtractor(WorkUnitState) which should return an instance of the Extractor class. As examples, take a look at KafkaSimpleSource , KafkaSimpleExtractor , and KafkaAvroExtractor . KafkaSimpleExtractor simply returns the payload of the MessageAndOffset object as a byte array. A job that uses KafkaSimpleExtractor may use a Converter to convert the byte array to whatever format desired. For example, if the desired output format is JSON, one may implement an ByteArrayToJsonConverter to convert the byte array to JSON. Alternatively one may implement a KafkaJsonExtractor , which extends KafkaExtractor and convert the MessageAndOffset object into a JSON object in the decodeRecord method. Both approaches should work equally well. KafkaAvroExtractor decodes the payload of the MessageAndOffset object into an Avro GenericRecord object. Writer and Publisher Any desired writer and publisher can be used, e.g., one may use the AvroHdfsDataWriter and the BaseDataPublisher , similar as the Wikipedia example job . If plain text output file is desired, one may use SimpleDataWriter . Job Config Properties These are some of the job config properties used by KafkaSource and KafkaExtractor . Property Name Semantics topic.whitelist (regex) Kafka topics to be pulled. Default value = .* topic.blacklist (regex) Kafka topics not to be pulled. Default value = empty kafka.brokers Comma separated Kafka brokers to ingest data from. mr.job.max.mappers Number of tasks to launch. In MR mode, this will be the number of mappers launched. If the number of topic partitions to be pulled is larger than the number of tasks, KafkaSource will assign partitions to tasks in a balanced manner. bootstrap.with.offset For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip. Default: latest reset.on.offset.out.of.range This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition). Default: nearest topics.move.to.latest.offset (no regex) Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property should rarely, if ever, be used. It is also possible to set a time limit for each task. For example, to set the time limit to 15 minutes, set the following properties: extract.limit.enabled=true extract.limit.type=time #(other possible values: rate, count, pool) extract.limit.timeLimit=15 extract.limit.timeLimitTimeunit=minutes Metrics and Events Task Level Metrics Task level metrics can be created in Extractor , Converter and Writer by extending InstrumentedExtractor , InstrumentedConverter and InstrumentedDataWriter . For example, KafkaExtractor extends InstrumentedExtractor . So you can do the following in subclasses of KafkaExtractor : Counter decodingErrorCounter = this.getMetricContext().counter( num.of.decoding.errors ); decodingErrorCounter.inc(); Besides Counter, Meter and Histogram are also supported. Task Level Events Task level events can be submitted by creating an EventSubmitter instance and using EventSubmitter.submit() or EventSubmitter.getTimingEvent() . Job Level Metrics To create job level metrics, one may extend AbstractJobLauncher and create metrics there. For example: Optional JobMetrics jobMetrics = this.jobContext.getJobMetricsOptional(); if (!jobMetrics.isPresent()) { LOG.warn( job metrics is absent ); return; } Counter recordsWrittenCounter = jobMetrics.get().getCounter( job.records.written ); recordsWrittenCounter.inc(value); Job level metrics are often aggregations of task level metrics, such as the job.records.written counter above. Since AbstractJobLauncher doesn't have access to task-level metrics, one should set these counters in TaskState s, and override AbstractJobLauncher.postProcessTaskStates() to aggregate them. For example, in AvroHdfsTimePartitionedWriter.close() , property writer.records.written is set for the TaskState . Job Level Events Job level events can be created by extending AbstractJobLauncher and use this.eventSubmitter.submit() or this.eventSubmitter.getTimingEvent() . For more details about metrics, events and reporting them, please see Gobblin Metrics section. Grouping Workunits For each topic partition that should be ingested, KafkaSource first retrieves the last offset pulled by the previous run, which should be the first offset of the current run. It also retrieves the earliest and latest offsets currently available from the Kafka cluster and verifies that the first offset is between the earliest and the latest offsets. The latest offset is the last offset to be pulled by the current workunit. Since new records may be constantly published to Kafka and old records are deleted based on retention policies, the earliest and latest offsets of a partition may change constantly. For each partition, after the first and last offsets are determined, a workunit is created. If the number of Kafka partitions exceeds the desired number of workunits specified by property mr.job.max.mappers , KafkaSource will merge and group them into n MultiWorkUnit s where n=mr.job.max.mappers . This is done using KafkaWorkUnitPacker , which has two implementations: KafkaSingleLevelWorkUnitPacker and KafkaBiLevelWorkUnitPacker . The packer packs workunits based on the estimated size of each workunit, which is obtained from KafkaWorkUnitSizeEstimator , which also has two implementations, KafkaAvgRecordSizeBasedWorkUnitSizeEstimator and KafkaAvgRecordTimeBasedWorkUnitSizeEstimator . Single-Level Packing The single-level packer uses a worst-fit-decreasing approach for assigning workunits to mappers: each workunit goes to the mapper that currently has the lightest load. This approach balances the mappers well. However, multiple partitions of the same topic are usually assigned to different mappers. This may cause two issues: (1) many small output files: if multiple partitions of a topic are assigned to different mappers, they cannot share output files. (2) task overhead: when multiple partitions of a topic are assigned to different mappers, a task is created for each partition, which may lead to a large number of tasks and large overhead. Bi-Level Packing The bi-level packer packs workunits in two steps. In the first step, all workunits are grouped into approximately 3n groups, each of which contains partitions of the same topic. The max group size is set as maxGroupSize = totalWorkunitSize/3n The best-fit-decreasing algorithm is run on all partitions of each topic. If an individual workunit\u2019s size exceeds maxGroupSize , it is put in a separate group. For each group, a new workunit is created which will be responsible for extracting all partitions in the group. The reason behind 3n is that if this number is too small (i.e., too close to n ), it is difficult for the second level to pack these groups into n balanced multiworkunits; if this number is too big, avgGroupSize will be small which doesn\u2019t help grouping partitions of the same topic together. 3n is a number that is empirically selected. The second step uses the same worst-fit-decreasing method as the first-level packer. This approach reduces the number of small files and the number of tasks, but it may have more mapper skew for two reasons: (1) in the worst-fit-decreasing approach, the less number of items to be packed, the more skew there will be; (2) when multiple partitions of a topic are assigned to the same mapper, if we underestimate the size of this topic, this mapper may take a much longer time than other mappers and the entire MR job has to wait for this mapper. This, however, can be mitigated by setting a time limit for each task, as explained above. Average Record Size-Based Workunit Size Estimator This size estimator uses the average record size of each partition to estimate the sizes of workunits. When using this size estimator, each job run will record the average record size of each partition it pulled. In the next run, for each partition the average record size pulled in the previous run is considered the average record size to be pulled in this run. If a partition was not pulled in a run, a default value of 1024 will be used in the next run. Average Record Time-Based Workunit Size Estimator This size estimator uses the average time to pull a record in each run to estimate the sizes of the workunits in the next run. When using this size estimator, each job run will record the average time per record of each partition. In the next run, the estimated average time per record for each topic is the geometric mean of the avg time per record of all partitions. For example if a topic has two partitions whose average time per record in the previous run are 2 and 8, the next run will use 4 as the estimated average time per record. If a topic is not pulled in a run, its estimated average time per record is the geometric mean of the estimated average time per record of all topics that are pulled in this run. If no topic was pulled in this run, a default value of 1.0 is used. The time-based estimator is more accurate than the size-based estimator when the time to pull a record is not proportional to the size of the record. However, the time-based estimator may lose accuracy when there are fluctuations in the Hadoop cluster which causes the average time for a partition to vary between different runs. Topic-Specific Configuration kafka.topic.specific.state is a configuration key that allows a user to specify config parameters on a topic specific level. The value of this config should be a JSON Array. Each entry should be a json string and should contain a primitive value that identifies the topic name. All configs in each topic entry will be added to the WorkUnit for that topic. An example value could be: [ { dataset : myTopic1 , writer.partition.columns : header.memberId }, { dataset : myTopic2 , writer.partition.columns : auditHeader.time } ] The dataset field also allows regular expressions. For example, one can specify key, value \"dataset\" : \"myTopic.\\*\" . In this case all topics whose name matches the pattern myTopic.* will have all the specified config properties added to their WorkUnit. If more than one topic matches multiple dataset s then the properties from all the JSON objects will be added to their WorkUnit. Kafka Deserializer Integration Gobblin integrates with Kafka's Deserializer API. Kafka's Deserializer Interface offers a generic interface for Kafka Clients to deserialize data from Kafka into Java Objects. Since Kafka Messages return byte array, the Deserializer class offers a convienient way of transforming those byte array's to Java Objects. Kafka's Client Library already has a few useful Deserializer s such as the the StringDeserializer and the ByteBufferDeserializer . Gobblin can integrate with any of these Deserializer s, that is any class that implements the Deserializer interface can be used to convert Kafka message to Java Objects. This is done in the KafkaDeserializerSource and the KafkaDeserializerExtractor classes. The type of Deserializer to be used in KafkaDeserializerExtractor can be specified by the property kafka.deserializer.type . This property can either be set to any of the pre-defined Deserializer s such as CONFLUENT_AVRO , CONFLUENT_JSON , GSON , BYTE_ARRAY , and STRING (see the section on Confluent Integration and KafkaGsonDeserializer for more details). The value of this property can point to the full-qualified path of a Deserializer implementation. If the value is set a class name, then a kafka.schema.registry.class must also be provided so that the Extractor knows how to retrieve the schema for the topic. Gobblin Deserializer Implementations KafkaGsonDeserializer The KafkaGsonDeserializer is an implementation of the Deserializer class that converts byte[] to JSONObject s. It uses GSON to do this. This class is useful for converting Kafka data to JSON Objects. Using this class simply requires setting kafka.deserializer.type to GSON . Comparison with KafkaSimpleSource Gobblin's KafkaSimpleSource and KafkaSimpleExtractor are very useful when data just needs to be read from Kafka and written to a text file. However, it does not provide good support for writing to more complex data file formats such as Avro or ORC . It also doesn't provide good support for record level manipulations such as Gobblin Converter s and it lacks good support for use with Gobblin's WriterPartitioner . The reason is that KafkaSimpleExtractor simply returns a byte[] , which is just a black-box of data. It is much easier to maniuplate the record if it is converted to a Java Object. This is where Gobblin's KafkaDeserializerExtractor becomes useful. Confluent Integration Confluent provides a standardized distribution of Apache Kafka , along with other useful tools for working with Kafka. One useful tool that Confluent provides is a generic Schema Registry . Gobblin has integration with Confluent's Schema Registry Library which provides a service to register and get Avro Schemas and provides a generic Avro Deserializer and JSON Deserializer . Confluent Schema Registry Gobblin integrates with Confluent's SchemaRegistryClient class in order to register and get Avro Schema's from the Confluent SchemaRegistry . This is implemented in the ConfluentKafkaSchemaRegistry class, which extends Gobblin's KafkaSchemaRegistry class. The ConfluentKafkaSchemaRegistry can be used by setting kafka.schema.registry.class to gobblin.source.extractor.extract.kafka.ConfluentKafkaSchemaRegistry . Confluent Deserializers Confluent's Schema Registry Library also provides a few useful Deserializer implementations: KafkaAvroDeserializer KafkaJsonDeserializer With regards to Gobblin, these classes are useful if Confluent's KafkaAvroSerializer or KafkaJsonSerializer is used to write data to Kafka. The Serializer class is a Kafka interface that is the converse of the Deserializer class. The Serializer provides a generic way of taking Java Objects and converting them to byte[] that are written to Kafka by a KafkaProducer . KafkaAvroDeserializer Documentation for the KafkaAvroDeserializer can be found here . If data is written to a Kafka cluster using Confluent's KafkaAvroSerializer , then the KafkaAvroDeserializer should be used in Gobblin. Setting this up simply requires a setting the config key kafka.deserializer.type to CONFLUENT_AVRO (see the section on Kafka Deserializer Integration for more information). KafkaJsonDeserializer The KafkaJsonDeserializer class uses Jackson's Object Mapper to convert byte[] to Java Objects. In order to KafkaJsonDeserializer to know which class the byte[] array should be converted to, the config property json.value.type needs to be set to the fully-qualified class name of the Java Object that the Deserializer should return. For more information about how the Jackson works, check out the docs here . Using the KafkaJsonDeserializer simply requires setting the config key kafka.deserializer.type to CONFLUENT_JSON (see the section on Kafka Deserializer Integration for more information).","title":"Kafka-HDFS Ingestion"},{"location":"case-studies/Kafka-HDFS-Ingestion/#table-of-contents","text":"Table of Contents Getting Started Standalone MapReduce Job Constructs Source and Extractor Writer and Publisher Job Config Properties Metrics and Events Task Level Metrics Task Level Events Job Level Metrics Job Level Events Grouping Workunits Single-Level Packing Bi-Level Packing Average Record Size-Based Workunit Size Estimator Average Record Time-Based Workunit Size Estimator Topic-Specific Configuration Kafka Deserializer Integration Gobblin Deserializer Implementations KafkaGsonDeserializer Comparison with KafkaSimpleSource Confluent Integration Confluent Schema Registry Confluent Deserializers KafkaAvroDeserializer KafkaJsonDeserializer","title":"Table of Contents"},{"location":"case-studies/Kafka-HDFS-Ingestion/#getting-started","text":"This section helps you set up a quick-start job for ingesting Kafka topics on a single machine. We provide quick start examples in both standalone and MapReduce mode.","title":"Getting Started"},{"location":"case-studies/Kafka-HDFS-Ingestion/#standalone","text":"Setup a single node Kafka broker by following the Kafka quick start guide . Suppose your broker URI is localhost:9092 , and you've created a topic \"test\" with two events \"This is a message\" and \"This is a another message\". The remaining steps are the same as the Wikipedia example , except using the following job config properties: job.name=GobblinKafkaQuickStart job.group=GobblinKafka job.description=Gobblin quick start job for Kafka job.lock.enabled=false kafka.brokers=localhost:9092 source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource extract.namespace=org.apache.gobblin.extract.kafka writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder writer.file.path.type=tablename writer.destination.type=HDFS writer.output.format=txt data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher mr.job.max.mappers=1 metrics.reporting.file.enabled=true metrics.log.dir=${gobblin.cluster.work.dir}/metrics metrics.reporting.file.suffix=txt bootstrap.with.offset=earliest After the job finishes, the following messages should be in the job log: INFO Pulling topic test INFO Pulling partition test:0 from offset 0 to 2, range=2 INFO Finished pulling partition test:0 INFO Finished pulling topic test INFO Extracted 2 data records INFO Actual high watermark for partition test:0=2, expected=2 INFO Task task_id completed in 31212ms with state SUCCESSFUL The output file will be in {gobblin.cluster.work.dir}/job-output/test , with the two messages you've just created in the Kafka broker. {gobblin.cluster.work.dir}/metrics will contain metrics collected from this run.","title":"Standalone"},{"location":"case-studies/Kafka-HDFS-Ingestion/#mapreduce","text":"Setup a single node Kafka broker same as in standalone mode. Setup a single node Hadoop cluster by following the steps in Hadoop: Setting up a Single Node Cluster . Suppose your HDFS URI is hdfs://localhost:9000 . Create a job config file with the following properties: job.name=GobblinKafkaQuickStart job.group=GobblinKafka job.description=Gobblin quick start job for Kafka job.lock.enabled=false kafka.brokers=localhost:9092 source.class=org.apache.gobblin.source.extractor.extract.kafka.KafkaSimpleSource extract.namespace=org.apache.gobblin.extract.kafka writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder writer.file.path.type=tablename writer.destination.type=HDFS writer.output.format=txt data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher mr.job.max.mappers=1 metrics.reporting.file.enabled=true metrics.log.dir=/gobblin-kafka/metrics metrics.reporting.file.suffix=txt bootstrap.with.offset=earliest fs.uri=hdfs://localhost:9000 writer.fs.uri=hdfs://localhost:9000 state.store.fs.uri=hdfs://localhost:9000 mr.job.root.dir=/gobblin-kafka/working state.store.dir=/gobblin-kafka/state-store task.data.root.dir=/jobs/kafkaetl/gobblin/gobblin-kafka/task-data data.publisher.final.dir=/gobblintest/job-output Run gobblin-mapreduce.sh : gobblin-mapreduce.sh --conf path-to-job-config-file After the job finishes, the job output file will be in /gobblintest/job-output/test in HDFS, and the metrics will be in /gobblin-kafka/metrics .","title":"MapReduce"},{"location":"case-studies/Kafka-HDFS-Ingestion/#job-constructs","text":"","title":"Job Constructs"},{"location":"case-studies/Kafka-HDFS-Ingestion/#source-and-extractor","text":"Gobblin provides two abstract classes, KafkaSource and KafkaExtractor . KafkaSource creates a workunit for each Kafka topic partition to be pulled, then merges and groups the workunits based on the desired number of workunits specified by property mr.job.max.mappers (this property is used in both standalone and MR mode). More details about how workunits are merged and grouped is available here . KafkaExtractor extracts the partitions assigned to a workunit, based on the specified low watermark and high watermark. To use them in a Kafka-HDFS ingestion job, one should subclass KafkaExtractor and implement method decodeRecord(MessageAndOffset) , which takes a MessageAndOffset object pulled from the Kafka broker and decodes it into a desired object. One should also subclass KafkaSource and implement getExtractor(WorkUnitState) which should return an instance of the Extractor class. As examples, take a look at KafkaSimpleSource , KafkaSimpleExtractor , and KafkaAvroExtractor . KafkaSimpleExtractor simply returns the payload of the MessageAndOffset object as a byte array. A job that uses KafkaSimpleExtractor may use a Converter to convert the byte array to whatever format desired. For example, if the desired output format is JSON, one may implement an ByteArrayToJsonConverter to convert the byte array to JSON. Alternatively one may implement a KafkaJsonExtractor , which extends KafkaExtractor and convert the MessageAndOffset object into a JSON object in the decodeRecord method. Both approaches should work equally well. KafkaAvroExtractor decodes the payload of the MessageAndOffset object into an Avro GenericRecord object.","title":"Source and Extractor"},{"location":"case-studies/Kafka-HDFS-Ingestion/#writer-and-publisher","text":"Any desired writer and publisher can be used, e.g., one may use the AvroHdfsDataWriter and the BaseDataPublisher , similar as the Wikipedia example job . If plain text output file is desired, one may use SimpleDataWriter .","title":"Writer and Publisher"},{"location":"case-studies/Kafka-HDFS-Ingestion/#job-config-properties","text":"These are some of the job config properties used by KafkaSource and KafkaExtractor . Property Name Semantics topic.whitelist (regex) Kafka topics to be pulled. Default value = .* topic.blacklist (regex) Kafka topics not to be pulled. Default value = empty kafka.brokers Comma separated Kafka brokers to ingest data from. mr.job.max.mappers Number of tasks to launch. In MR mode, this will be the number of mappers launched. If the number of topic partitions to be pulled is larger than the number of tasks, KafkaSource will assign partitions to tasks in a balanced manner. bootstrap.with.offset For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip. Default: latest reset.on.offset.out.of.range This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition). Default: nearest topics.move.to.latest.offset (no regex) Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property should rarely, if ever, be used. It is also possible to set a time limit for each task. For example, to set the time limit to 15 minutes, set the following properties: extract.limit.enabled=true extract.limit.type=time #(other possible values: rate, count, pool) extract.limit.timeLimit=15 extract.limit.timeLimitTimeunit=minutes","title":"Job Config Properties"},{"location":"case-studies/Kafka-HDFS-Ingestion/#metrics-and-events","text":"","title":"Metrics and Events"},{"location":"case-studies/Kafka-HDFS-Ingestion/#task-level-metrics","text":"Task level metrics can be created in Extractor , Converter and Writer by extending InstrumentedExtractor , InstrumentedConverter and InstrumentedDataWriter . For example, KafkaExtractor extends InstrumentedExtractor . So you can do the following in subclasses of KafkaExtractor : Counter decodingErrorCounter = this.getMetricContext().counter( num.of.decoding.errors ); decodingErrorCounter.inc(); Besides Counter, Meter and Histogram are also supported.","title":"Task Level Metrics"},{"location":"case-studies/Kafka-HDFS-Ingestion/#task-level-events","text":"Task level events can be submitted by creating an EventSubmitter instance and using EventSubmitter.submit() or EventSubmitter.getTimingEvent() .","title":"Task Level Events"},{"location":"case-studies/Kafka-HDFS-Ingestion/#job-level-metrics","text":"To create job level metrics, one may extend AbstractJobLauncher and create metrics there. For example: Optional JobMetrics jobMetrics = this.jobContext.getJobMetricsOptional(); if (!jobMetrics.isPresent()) { LOG.warn( job metrics is absent ); return; } Counter recordsWrittenCounter = jobMetrics.get().getCounter( job.records.written ); recordsWrittenCounter.inc(value); Job level metrics are often aggregations of task level metrics, such as the job.records.written counter above. Since AbstractJobLauncher doesn't have access to task-level metrics, one should set these counters in TaskState s, and override AbstractJobLauncher.postProcessTaskStates() to aggregate them. For example, in AvroHdfsTimePartitionedWriter.close() , property writer.records.written is set for the TaskState .","title":"Job Level Metrics"},{"location":"case-studies/Kafka-HDFS-Ingestion/#job-level-events","text":"Job level events can be created by extending AbstractJobLauncher and use this.eventSubmitter.submit() or this.eventSubmitter.getTimingEvent() . For more details about metrics, events and reporting them, please see Gobblin Metrics section.","title":"Job Level Events"},{"location":"case-studies/Kafka-HDFS-Ingestion/#grouping-workunits","text":"For each topic partition that should be ingested, KafkaSource first retrieves the last offset pulled by the previous run, which should be the first offset of the current run. It also retrieves the earliest and latest offsets currently available from the Kafka cluster and verifies that the first offset is between the earliest and the latest offsets. The latest offset is the last offset to be pulled by the current workunit. Since new records may be constantly published to Kafka and old records are deleted based on retention policies, the earliest and latest offsets of a partition may change constantly. For each partition, after the first and last offsets are determined, a workunit is created. If the number of Kafka partitions exceeds the desired number of workunits specified by property mr.job.max.mappers , KafkaSource will merge and group them into n MultiWorkUnit s where n=mr.job.max.mappers . This is done using KafkaWorkUnitPacker , which has two implementations: KafkaSingleLevelWorkUnitPacker and KafkaBiLevelWorkUnitPacker . The packer packs workunits based on the estimated size of each workunit, which is obtained from KafkaWorkUnitSizeEstimator , which also has two implementations, KafkaAvgRecordSizeBasedWorkUnitSizeEstimator and KafkaAvgRecordTimeBasedWorkUnitSizeEstimator .","title":"Grouping Workunits"},{"location":"case-studies/Kafka-HDFS-Ingestion/#single-level-packing","text":"The single-level packer uses a worst-fit-decreasing approach for assigning workunits to mappers: each workunit goes to the mapper that currently has the lightest load. This approach balances the mappers well. However, multiple partitions of the same topic are usually assigned to different mappers. This may cause two issues: (1) many small output files: if multiple partitions of a topic are assigned to different mappers, they cannot share output files. (2) task overhead: when multiple partitions of a topic are assigned to different mappers, a task is created for each partition, which may lead to a large number of tasks and large overhead.","title":"Single-Level Packing"},{"location":"case-studies/Kafka-HDFS-Ingestion/#bi-level-packing","text":"The bi-level packer packs workunits in two steps. In the first step, all workunits are grouped into approximately 3n groups, each of which contains partitions of the same topic. The max group size is set as maxGroupSize = totalWorkunitSize/3n The best-fit-decreasing algorithm is run on all partitions of each topic. If an individual workunit\u2019s size exceeds maxGroupSize , it is put in a separate group. For each group, a new workunit is created which will be responsible for extracting all partitions in the group. The reason behind 3n is that if this number is too small (i.e., too close to n ), it is difficult for the second level to pack these groups into n balanced multiworkunits; if this number is too big, avgGroupSize will be small which doesn\u2019t help grouping partitions of the same topic together. 3n is a number that is empirically selected. The second step uses the same worst-fit-decreasing method as the first-level packer. This approach reduces the number of small files and the number of tasks, but it may have more mapper skew for two reasons: (1) in the worst-fit-decreasing approach, the less number of items to be packed, the more skew there will be; (2) when multiple partitions of a topic are assigned to the same mapper, if we underestimate the size of this topic, this mapper may take a much longer time than other mappers and the entire MR job has to wait for this mapper. This, however, can be mitigated by setting a time limit for each task, as explained above.","title":"Bi-Level Packing"},{"location":"case-studies/Kafka-HDFS-Ingestion/#average-record-size-based-workunit-size-estimator","text":"This size estimator uses the average record size of each partition to estimate the sizes of workunits. When using this size estimator, each job run will record the average record size of each partition it pulled. In the next run, for each partition the average record size pulled in the previous run is considered the average record size to be pulled in this run. If a partition was not pulled in a run, a default value of 1024 will be used in the next run.","title":"Average Record Size-Based Workunit Size Estimator"},{"location":"case-studies/Kafka-HDFS-Ingestion/#average-record-time-based-workunit-size-estimator","text":"This size estimator uses the average time to pull a record in each run to estimate the sizes of the workunits in the next run. When using this size estimator, each job run will record the average time per record of each partition. In the next run, the estimated average time per record for each topic is the geometric mean of the avg time per record of all partitions. For example if a topic has two partitions whose average time per record in the previous run are 2 and 8, the next run will use 4 as the estimated average time per record. If a topic is not pulled in a run, its estimated average time per record is the geometric mean of the estimated average time per record of all topics that are pulled in this run. If no topic was pulled in this run, a default value of 1.0 is used. The time-based estimator is more accurate than the size-based estimator when the time to pull a record is not proportional to the size of the record. However, the time-based estimator may lose accuracy when there are fluctuations in the Hadoop cluster which causes the average time for a partition to vary between different runs.","title":"Average Record Time-Based Workunit Size Estimator"},{"location":"case-studies/Kafka-HDFS-Ingestion/#topic-specific-configuration","text":"kafka.topic.specific.state is a configuration key that allows a user to specify config parameters on a topic specific level. The value of this config should be a JSON Array. Each entry should be a json string and should contain a primitive value that identifies the topic name. All configs in each topic entry will be added to the WorkUnit for that topic. An example value could be: [ { dataset : myTopic1 , writer.partition.columns : header.memberId }, { dataset : myTopic2 , writer.partition.columns : auditHeader.time } ] The dataset field also allows regular expressions. For example, one can specify key, value \"dataset\" : \"myTopic.\\*\" . In this case all topics whose name matches the pattern myTopic.* will have all the specified config properties added to their WorkUnit. If more than one topic matches multiple dataset s then the properties from all the JSON objects will be added to their WorkUnit.","title":"Topic-Specific Configuration"},{"location":"case-studies/Kafka-HDFS-Ingestion/#kafka-deserializer-integration","text":"Gobblin integrates with Kafka's Deserializer API. Kafka's Deserializer Interface offers a generic interface for Kafka Clients to deserialize data from Kafka into Java Objects. Since Kafka Messages return byte array, the Deserializer class offers a convienient way of transforming those byte array's to Java Objects. Kafka's Client Library already has a few useful Deserializer s such as the the StringDeserializer and the ByteBufferDeserializer . Gobblin can integrate with any of these Deserializer s, that is any class that implements the Deserializer interface can be used to convert Kafka message to Java Objects. This is done in the KafkaDeserializerSource and the KafkaDeserializerExtractor classes. The type of Deserializer to be used in KafkaDeserializerExtractor can be specified by the property kafka.deserializer.type . This property can either be set to any of the pre-defined Deserializer s such as CONFLUENT_AVRO , CONFLUENT_JSON , GSON , BYTE_ARRAY , and STRING (see the section on Confluent Integration and KafkaGsonDeserializer for more details). The value of this property can point to the full-qualified path of a Deserializer implementation. If the value is set a class name, then a kafka.schema.registry.class must also be provided so that the Extractor knows how to retrieve the schema for the topic.","title":"Kafka Deserializer Integration"},{"location":"case-studies/Kafka-HDFS-Ingestion/#gobblin-deserializer-implementations","text":"","title":"Gobblin Deserializer Implementations"},{"location":"case-studies/Kafka-HDFS-Ingestion/#kafkagsondeserializer","text":"The KafkaGsonDeserializer is an implementation of the Deserializer class that converts byte[] to JSONObject s. It uses GSON to do this. This class is useful for converting Kafka data to JSON Objects. Using this class simply requires setting kafka.deserializer.type to GSON .","title":"KafkaGsonDeserializer"},{"location":"case-studies/Kafka-HDFS-Ingestion/#comparison-with-kafkasimplesource","text":"Gobblin's KafkaSimpleSource and KafkaSimpleExtractor are very useful when data just needs to be read from Kafka and written to a text file. However, it does not provide good support for writing to more complex data file formats such as Avro or ORC . It also doesn't provide good support for record level manipulations such as Gobblin Converter s and it lacks good support for use with Gobblin's WriterPartitioner . The reason is that KafkaSimpleExtractor simply returns a byte[] , which is just a black-box of data. It is much easier to maniuplate the record if it is converted to a Java Object. This is where Gobblin's KafkaDeserializerExtractor becomes useful.","title":"Comparison with KafkaSimpleSource"},{"location":"case-studies/Kafka-HDFS-Ingestion/#confluent-integration","text":"Confluent provides a standardized distribution of Apache Kafka , along with other useful tools for working with Kafka. One useful tool that Confluent provides is a generic Schema Registry . Gobblin has integration with Confluent's Schema Registry Library which provides a service to register and get Avro Schemas and provides a generic Avro Deserializer and JSON Deserializer .","title":"Confluent Integration"},{"location":"case-studies/Kafka-HDFS-Ingestion/#confluent-schema-registry","text":"Gobblin integrates with Confluent's SchemaRegistryClient class in order to register and get Avro Schema's from the Confluent SchemaRegistry . This is implemented in the ConfluentKafkaSchemaRegistry class, which extends Gobblin's KafkaSchemaRegistry class. The ConfluentKafkaSchemaRegistry can be used by setting kafka.schema.registry.class to gobblin.source.extractor.extract.kafka.ConfluentKafkaSchemaRegistry .","title":"Confluent Schema Registry"},{"location":"case-studies/Kafka-HDFS-Ingestion/#confluent-deserializers","text":"Confluent's Schema Registry Library also provides a few useful Deserializer implementations: KafkaAvroDeserializer KafkaJsonDeserializer With regards to Gobblin, these classes are useful if Confluent's KafkaAvroSerializer or KafkaJsonSerializer is used to write data to Kafka. The Serializer class is a Kafka interface that is the converse of the Deserializer class. The Serializer provides a generic way of taking Java Objects and converting them to byte[] that are written to Kafka by a KafkaProducer .","title":"Confluent Deserializers"},{"location":"case-studies/Kafka-HDFS-Ingestion/#kafkaavrodeserializer","text":"Documentation for the KafkaAvroDeserializer can be found here . If data is written to a Kafka cluster using Confluent's KafkaAvroSerializer , then the KafkaAvroDeserializer should be used in Gobblin. Setting this up simply requires a setting the config key kafka.deserializer.type to CONFLUENT_AVRO (see the section on Kafka Deserializer Integration for more information).","title":"KafkaAvroDeserializer"},{"location":"case-studies/Kafka-HDFS-Ingestion/#kafkajsondeserializer","text":"The KafkaJsonDeserializer class uses Jackson's Object Mapper to convert byte[] to Java Objects. In order to KafkaJsonDeserializer to know which class the byte[] array should be converted to, the config property json.value.type needs to be set to the fully-qualified class name of the Java Object that the Deserializer should return. For more information about how the Jackson works, check out the docs here . Using the KafkaJsonDeserializer simply requires setting the config key kafka.deserializer.type to CONFLUENT_JSON (see the section on Kafka Deserializer Integration for more information).","title":"KafkaJsonDeserializer"},{"location":"case-studies/Publishing-Data-to-S3/","text":"Table of Contents Table of Contents Introduction Hadoop and S3 The s3a File System The s3 File System Getting Gobblin to Publish to S3 Signing Up For AWS Setting Up EC2 Launching an EC2 Instance EC2 Package Installations Installing Java Setting Up S3 Setting Up Gobblin on EC2 Configuring Gobblin on EC2 Launching Gobblin on EC2 Writing to S3 Outside EC2 Configuration Properties for s3a FAQs How do I control the directory the s3a uses when writing to local disk Introduction While Gobblin is not tied to any specific cloud provider, Amazon Web Services is a popular choice. This document will outline how Gobblin can publish data to S3 . Specifically, it will provide a step by step guide to help setup Gobblin on Amazon EC2 , run Gobblin on EC2, and publish data from EC2 to S3. It is recommended to configure Gobblin to first write data to EBS , and then publish the data to S3. This is the recommended approach because there are a few caveats when working with with S3. See the Hadoop and S3 section for more details. This document will also provide a step by step guide for launching and configuring an EC2 instance and creating a S3 bucket. However, it is by no means a source of truth guide to working with AWS, it will only provide high level steps. The best place to learn about how to use AWS is through the Amazon documentation . Hadoop and S3 A majority of Gobblin's code base uses Hadoop's FileSystem object to read and write data. The FileSystem object is an abstract class, and typical implementations either write to the local file system, or write to HDFS. There has been significant work to create an implementation of the FileSystem object that reads and writes to S3. The best guide to read about the different S3 FileSystem implementations is here . There are a few different S3 FileSystem implementations, the two of note are the s3a and the s3 file systems. The s3a file system is relatively new and is only available in Hadoop 2.6.0 (see the original JIRA for more information). The s3 filesystem has been around for a while. The s3a File System The s3a file system uploads files to a specified bucket. The data uploaded to S3 via this file system is interoperable with other S3 tools. However, there are a few caveats when working with this file system: Since S3 does not support renaming of files in a bucket, the S3AFileSystem.rename(Path, Path) operation will actually copy data from the source Path to the destination Path , and then delete the source Path (see the source code for more information) When creating a file using S3AFileSystem.create(...) data will be first written to a staging file on the local file system, and when the file is closed, the staging file will be uploaded to S3 (see the source code for more information) Thus, when using the s3a file system with Gobblin it is recommended that one configures Gobblin to first write its staging data to the local filesystem, and then to publish the data to S3. The reason this is the recommended approach is that each Gobblin Task will write data to a staging file, and once the file has been completely written it publishes the file to a output directory (it does this by using a rename function). Finally, the DataPublisher moves the files from the staging directory to its final directory (again done using a rename function). This requires two renames operations and would be very inefficient if a Task wrote directly to S3. Furthermore, writing directly to S3 requires creating a staging file on the local file system, and then creating a PutObjectRequest to upload the data to S3. This is logically equivalent to just configuring Gobblin to write to a local file and then publishing it to S3. The s3 File System The s3 file system stores file as blocks, similar to how HDFS stores blocks. This makes renaming of files more efficient, but data written using this file system is not interoperable with other S3 tools. This limitation may make using this file system less desirable, so the majority of this document focuses on the s3a file system. Although the majority of the walkthrough should apply for the s3 file system also. Getting Gobblin to Publish to S3 This section will provide a step by step guide to setting up an EC2 instance, a S3 bucket, installing Gobblin on EC2, and configuring Gobblin to publish data to S3. This guide will use the free-tier provided by AWS to setup EC2 and S3. Signing Up For AWS In order to use EC2 and S3, one first needs to sign up for an AWS account. The easiest way to get started with AWS is to use their free tier . Setting Up EC2 Launching an EC2 Instance Once you have an AWS account, login to the AWS console . Select the EC2 link, which will bring you to the EC2 dashboard . Click on Launch Instance to create a new EC2 instance. Before the instance actually starts to run, there area a few more configuration steps necessary: Choose an Amazon Machine Image ( AMI ) For this walkthrough we will pick Red Hat Enterprise Linux ( RHEL ) AMI Choose an Instance Type Since this walkthrough uses the Amazon Free Tier, we will pick the General Purpose t2.micro instance This instance provides us with 1 vCPU and 1 GiB of RAM For more information on other instance types, check out the AWS docs Click Review and Launch We will use the defaults for all other setting options When reviewing your instance, you will most likely get a warning saying access to your EC2 instance is open to the world If you want to fix this you have to edit the Security Groups ; how to do that is out of the scope of this document Set Up SSH Keys After reviewing your instance, click Launch You should be prompted to setup SSH keys Use an existing key pair if you have one, otherwise create a new one and download it SSH to Launched Instance SSH using the following command: ssh -i my-private-key-file.pem ec2-user@instance-name The instance-name can be taken from the Public DNS field from the instance information SSH may complain that the private key file has insufficient permissions Execute chmod 600 my-private-key-file.pem to fix this Alternatively, one can modify the ~/.ssh/config file instead of specifying the -i option After following the above steps, you should be able to freely SSH into the launched EC2 instance, and monitor / control the instance from the EC2 dashboard . EC2 Package Installations Before setting up Gobblin, you need to install Java first. Depending on the AMI instance you are running Java may or may not already be installed (you can check if Java is already installed by executing java -version ). Installing Java Execute sudo yum install java-1.8.0-openjdk* to install Open JDK 8 Confirm the installation was successful by executing java -version Set the JAVA_HOME environment variable in the ~/.bashrc/ file The value for JAVA_HOME can be found by executing readlink `which java` Setting Up S3 Go to the S3 dashboard Click on Create Bucket Enter a name for the bucket (e.g. gobblin-demo-bucket ) Enter a Region for the bucket (e.g. US Standard ) Setting Up Gobblin on EC2 Download and Build Gobblin Locally On your local machine, clone the Gobblin repository : git clone git@github.com:apache/incubator-gobblin.git (this assumes you have Git installed locally) Build Gobblin using the following commands (it is important to use Hadoop version 2.6.0 as it includes the s3a file system implementation): cd gobblin ./gradlew clean build -PhadoopVersion=2.6.0 -x test Upload the Gobblin Tar to EC2 Execute the command: scp -i my-private-key-file.pem gobblin-dist-[project-version].tar.gz ec2-user@instance-name: Un-tar the Gobblin Distribution SSH to the EC2 Instance Un-tar the Gobblin distribution: tar -xvf gobblin-dist-[project-version].tar.gz Download AWS Libraries A few JARs need to be downloaded using some cURL commands: curl http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar gobblin-dist/lib/aws-java-sdk-1.7.4.jar curl http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar gobblin-dist/lib/hadoop-aws-2.6.0.jar Configuring Gobblin on EC2 Assuming we are running Gobblin in standalone mode , the following configuration options need to be modified in the file gobblin-dist/conf/gobblin-standalone.properties . Add the key data.publisher.fs.uri and set it to s3a://gobblin-demo-bucket/ This configures the job to publish data to the S3 bucket named gobblin-demo-bucket Add the AWS Access Key Id and Secret Access Key Set the keys fs.s3a.access.key and fs.s3a.secret.key to the appropriate values These keys correspond to AWS security credentials For information on how to get these credentials, check out the AWS documentation here The AWS documentation recommends using IAM roles ; how to set this up is out of the scope of this document; for this walkthrough we will use root access credentials Launching Gobblin on EC2 Assuming we want Gobblin to run in standalone mode, follow the usual steps for standalone deployment . For the sake of this walkthrough, we will launch the Gobblin wikipedia example . Directions on how to run this example can be found here . The command to launch Gobblin should look similar to: sh bin/gobblin standalone start --conf-dir /home/ec2-user/gobblin-dist/config If you are running on the Amazon free tier, you will probably get an error in the nohup.out file saying there is insufficient memory for the JVM. To fix this add --jvmflags \"-Xms256m -Xmx512m\" to the start command. Data should be written to S3 during the publishing phase of Gobblin. One can confirm data was successfully written to S3 by looking at the S3 dashboard . Writing to S3 Outside EC2 It is possible to write to an S3 bucket outside of an EC2 instance. The setup steps are similar to walkthrough outlined above. For more information on writing to S3 outside of AWS, check out this article . Configuration Properties for s3a The s3a FileSystem has a number of configuration properties that can be set to tune the behavior and performance of the s3a FileSystem. A complete list of the properties can be found here: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html FAQs How do I control the directory the s3a uses when writing to local disk The configuration property fs.s3a.buffer.dir controls the location where the s3a FileSystem will write data locally before uplodaing it to S3. By default, this property is set to ${hadoop.tmp.dir}/s3a .","title":"Publishing Data to S3"},{"location":"case-studies/Publishing-Data-to-S3/#table-of-contents","text":"Table of Contents Introduction Hadoop and S3 The s3a File System The s3 File System Getting Gobblin to Publish to S3 Signing Up For AWS Setting Up EC2 Launching an EC2 Instance EC2 Package Installations Installing Java Setting Up S3 Setting Up Gobblin on EC2 Configuring Gobblin on EC2 Launching Gobblin on EC2 Writing to S3 Outside EC2 Configuration Properties for s3a FAQs How do I control the directory the s3a uses when writing to local disk","title":"Table of Contents"},{"location":"case-studies/Publishing-Data-to-S3/#introduction","text":"While Gobblin is not tied to any specific cloud provider, Amazon Web Services is a popular choice. This document will outline how Gobblin can publish data to S3 . Specifically, it will provide a step by step guide to help setup Gobblin on Amazon EC2 , run Gobblin on EC2, and publish data from EC2 to S3. It is recommended to configure Gobblin to first write data to EBS , and then publish the data to S3. This is the recommended approach because there are a few caveats when working with with S3. See the Hadoop and S3 section for more details. This document will also provide a step by step guide for launching and configuring an EC2 instance and creating a S3 bucket. However, it is by no means a source of truth guide to working with AWS, it will only provide high level steps. The best place to learn about how to use AWS is through the Amazon documentation .","title":"Introduction"},{"location":"case-studies/Publishing-Data-to-S3/#hadoop-and-s3","text":"A majority of Gobblin's code base uses Hadoop's FileSystem object to read and write data. The FileSystem object is an abstract class, and typical implementations either write to the local file system, or write to HDFS. There has been significant work to create an implementation of the FileSystem object that reads and writes to S3. The best guide to read about the different S3 FileSystem implementations is here . There are a few different S3 FileSystem implementations, the two of note are the s3a and the s3 file systems. The s3a file system is relatively new and is only available in Hadoop 2.6.0 (see the original JIRA for more information). The s3 filesystem has been around for a while.","title":"Hadoop and S3"},{"location":"case-studies/Publishing-Data-to-S3/#the-s3a-file-system","text":"The s3a file system uploads files to a specified bucket. The data uploaded to S3 via this file system is interoperable with other S3 tools. However, there are a few caveats when working with this file system: Since S3 does not support renaming of files in a bucket, the S3AFileSystem.rename(Path, Path) operation will actually copy data from the source Path to the destination Path , and then delete the source Path (see the source code for more information) When creating a file using S3AFileSystem.create(...) data will be first written to a staging file on the local file system, and when the file is closed, the staging file will be uploaded to S3 (see the source code for more information) Thus, when using the s3a file system with Gobblin it is recommended that one configures Gobblin to first write its staging data to the local filesystem, and then to publish the data to S3. The reason this is the recommended approach is that each Gobblin Task will write data to a staging file, and once the file has been completely written it publishes the file to a output directory (it does this by using a rename function). Finally, the DataPublisher moves the files from the staging directory to its final directory (again done using a rename function). This requires two renames operations and would be very inefficient if a Task wrote directly to S3. Furthermore, writing directly to S3 requires creating a staging file on the local file system, and then creating a PutObjectRequest to upload the data to S3. This is logically equivalent to just configuring Gobblin to write to a local file and then publishing it to S3.","title":"The s3a File System"},{"location":"case-studies/Publishing-Data-to-S3/#the-s3-file-system","text":"The s3 file system stores file as blocks, similar to how HDFS stores blocks. This makes renaming of files more efficient, but data written using this file system is not interoperable with other S3 tools. This limitation may make using this file system less desirable, so the majority of this document focuses on the s3a file system. Although the majority of the walkthrough should apply for the s3 file system also.","title":"The s3 File System"},{"location":"case-studies/Publishing-Data-to-S3/#getting-gobblin-to-publish-to-s3","text":"This section will provide a step by step guide to setting up an EC2 instance, a S3 bucket, installing Gobblin on EC2, and configuring Gobblin to publish data to S3. This guide will use the free-tier provided by AWS to setup EC2 and S3.","title":"Getting Gobblin to Publish to S3"},{"location":"case-studies/Publishing-Data-to-S3/#signing-up-for-aws","text":"In order to use EC2 and S3, one first needs to sign up for an AWS account. The easiest way to get started with AWS is to use their free tier .","title":"Signing Up For AWS"},{"location":"case-studies/Publishing-Data-to-S3/#setting-up-ec2","text":"","title":"Setting Up EC2"},{"location":"case-studies/Publishing-Data-to-S3/#launching-an-ec2-instance","text":"Once you have an AWS account, login to the AWS console . Select the EC2 link, which will bring you to the EC2 dashboard . Click on Launch Instance to create a new EC2 instance. Before the instance actually starts to run, there area a few more configuration steps necessary: Choose an Amazon Machine Image ( AMI ) For this walkthrough we will pick Red Hat Enterprise Linux ( RHEL ) AMI Choose an Instance Type Since this walkthrough uses the Amazon Free Tier, we will pick the General Purpose t2.micro instance This instance provides us with 1 vCPU and 1 GiB of RAM For more information on other instance types, check out the AWS docs Click Review and Launch We will use the defaults for all other setting options When reviewing your instance, you will most likely get a warning saying access to your EC2 instance is open to the world If you want to fix this you have to edit the Security Groups ; how to do that is out of the scope of this document Set Up SSH Keys After reviewing your instance, click Launch You should be prompted to setup SSH keys Use an existing key pair if you have one, otherwise create a new one and download it SSH to Launched Instance SSH using the following command: ssh -i my-private-key-file.pem ec2-user@instance-name The instance-name can be taken from the Public DNS field from the instance information SSH may complain that the private key file has insufficient permissions Execute chmod 600 my-private-key-file.pem to fix this Alternatively, one can modify the ~/.ssh/config file instead of specifying the -i option After following the above steps, you should be able to freely SSH into the launched EC2 instance, and monitor / control the instance from the EC2 dashboard .","title":"Launching an EC2 Instance"},{"location":"case-studies/Publishing-Data-to-S3/#ec2-package-installations","text":"Before setting up Gobblin, you need to install Java first. Depending on the AMI instance you are running Java may or may not already be installed (you can check if Java is already installed by executing java -version ).","title":"EC2 Package Installations"},{"location":"case-studies/Publishing-Data-to-S3/#installing-java","text":"Execute sudo yum install java-1.8.0-openjdk* to install Open JDK 8 Confirm the installation was successful by executing java -version Set the JAVA_HOME environment variable in the ~/.bashrc/ file The value for JAVA_HOME can be found by executing readlink `which java`","title":"Installing Java"},{"location":"case-studies/Publishing-Data-to-S3/#setting-up-s3","text":"Go to the S3 dashboard Click on Create Bucket Enter a name for the bucket (e.g. gobblin-demo-bucket ) Enter a Region for the bucket (e.g. US Standard )","title":"Setting Up S3"},{"location":"case-studies/Publishing-Data-to-S3/#setting-up-gobblin-on-ec2","text":"Download and Build Gobblin Locally On your local machine, clone the Gobblin repository : git clone git@github.com:apache/incubator-gobblin.git (this assumes you have Git installed locally) Build Gobblin using the following commands (it is important to use Hadoop version 2.6.0 as it includes the s3a file system implementation): cd gobblin ./gradlew clean build -PhadoopVersion=2.6.0 -x test Upload the Gobblin Tar to EC2 Execute the command: scp -i my-private-key-file.pem gobblin-dist-[project-version].tar.gz ec2-user@instance-name: Un-tar the Gobblin Distribution SSH to the EC2 Instance Un-tar the Gobblin distribution: tar -xvf gobblin-dist-[project-version].tar.gz Download AWS Libraries A few JARs need to be downloaded using some cURL commands: curl http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar gobblin-dist/lib/aws-java-sdk-1.7.4.jar curl http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar gobblin-dist/lib/hadoop-aws-2.6.0.jar","title":"Setting Up Gobblin on EC2"},{"location":"case-studies/Publishing-Data-to-S3/#configuring-gobblin-on-ec2","text":"Assuming we are running Gobblin in standalone mode , the following configuration options need to be modified in the file gobblin-dist/conf/gobblin-standalone.properties . Add the key data.publisher.fs.uri and set it to s3a://gobblin-demo-bucket/ This configures the job to publish data to the S3 bucket named gobblin-demo-bucket Add the AWS Access Key Id and Secret Access Key Set the keys fs.s3a.access.key and fs.s3a.secret.key to the appropriate values These keys correspond to AWS security credentials For information on how to get these credentials, check out the AWS documentation here The AWS documentation recommends using IAM roles ; how to set this up is out of the scope of this document; for this walkthrough we will use root access credentials","title":"Configuring Gobblin on EC2"},{"location":"case-studies/Publishing-Data-to-S3/#launching-gobblin-on-ec2","text":"Assuming we want Gobblin to run in standalone mode, follow the usual steps for standalone deployment . For the sake of this walkthrough, we will launch the Gobblin wikipedia example . Directions on how to run this example can be found here . The command to launch Gobblin should look similar to: sh bin/gobblin standalone start --conf-dir /home/ec2-user/gobblin-dist/config If you are running on the Amazon free tier, you will probably get an error in the nohup.out file saying there is insufficient memory for the JVM. To fix this add --jvmflags \"-Xms256m -Xmx512m\" to the start command. Data should be written to S3 during the publishing phase of Gobblin. One can confirm data was successfully written to S3 by looking at the S3 dashboard .","title":"Launching Gobblin on EC2"},{"location":"case-studies/Publishing-Data-to-S3/#writing-to-s3-outside-ec2","text":"It is possible to write to an S3 bucket outside of an EC2 instance. The setup steps are similar to walkthrough outlined above. For more information on writing to S3 outside of AWS, check out this article .","title":"Writing to S3 Outside EC2"},{"location":"case-studies/Publishing-Data-to-S3/#configuration-properties-for-s3a","text":"The s3a FileSystem has a number of configuration properties that can be set to tune the behavior and performance of the s3a FileSystem. A complete list of the properties can be found here: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html","title":"Configuration Properties for s3a"},{"location":"case-studies/Publishing-Data-to-S3/#faqs","text":"","title":"FAQs"},{"location":"case-studies/Publishing-Data-to-S3/#how-do-i-control-the-directory-the-s3a-uses-when-writing-to-local-disk","text":"The configuration property fs.s3a.buffer.dir controls the location where the s3a FileSystem will write data locally before uplodaing it to S3. By default, this property is set to ${hadoop.tmp.dir}/s3a .","title":"How do I control the directory the s3a uses when writing to local disk"},{"location":"case-studies/Writing-ORC-Data/","text":"Table of Contents Table of Contents Introduction Hive SerDe Integration Writing to an ORC File Data Flow Extending Gobblin's SerDe Integration Introduction Gobblin is capable of writing data to ORC files by leveraging Hive's SerDe library. Gobblin has native integration with Hive SerDe's library via the HiveSerDeWrapper class. This document will briefly explain how Gobblin integrates with Hive's SerDe library, and show an example of writing ORC files. Hive SerDe Integration Hive's SerDe library defines the interface Hive uses for serialization and deserialization of data. The Hive SerDe library has out of the box SerDe support for Avro, ORC, Parquet, CSV, and JSON SerDes. However, users are free to define custom SerDes. Gobblin integrates with the Hive SerDe's in a few different places. Here is a list of integration points that are relevant for this document: HiveSerDeWrapper wrapper around Hive's SerDe library that provides some nice utilities and structure that the rest of Gobblin can interfact with HiveSerDeConverter takes a Writable object in a specific format, and converts it to the Writable of another format (e.g. from AvroGenericRecordWritable to OrcSerdeRow ) HiveWritableHdfsDataWriter writes a Writable object to a specific file, typically this writes the output of a HiveSerDeConverter Writing to an ORC File An end-to-end example of writing to an ORC file is provided in the configuration found here . This .pull file is almost identical to the Wikipedia example discussed in the Getting Started Guide . The only difference is that the output is written in ORC instead of Avro. The configuration file mentioned above can be directly used as a template for writing data to ORC files, below is a detailed explanation of the configuration options that need to be changed, and why they need to be changed. converter.classes requires two additional converters: gobblin.converter.avro.AvroRecordToAvroWritableConverter and gobblin.converter.serde.HiveSerDeConverter The output of the first converter (the WikipediaConverter ) returns Avro GenericRecord s These records must be converted to Writable object in order for the Hive SerDe to process them, which is where the AvroRecordToAvroWritableConverter comes in The HiveSerDeConverter does the actual heavy lifting of converting the Avro Records to ORC Records In order to configure the HiveSerDeConverter the following properites need to be added: serde.deserializer.type=AVRO says that the records being fed into the converter are Avro records avro.schema.literal or avro.schema.url must be set when using this deserializer so that the Hive SerDe knows what Avro Schema to use when converting the record serde.serializer.type=ORC says that the records that should be returned by the converter are ORC records writer.builder.class should be set to gobblin.writer.HiveWritableHdfsDataWriterBuilder This writer class will take the output of the HiveSerDeConverter and write the actual ORC records to an ORC file writer.output.format should be set to ORC ; this ensures the files produced end with the .orc file extension fork.record.queue.capacity should be set to 1 This ensures no caching of records is done before they get passed to the writer; this is necessary because the OrcSerde caches the object it uses to serialize records, and it does not allow copying of Orc Records The example job can be run the same way the regular Wikipedia job is run, except the output will be in the ORC format. Data Flow For the Wikipedia to ORC example, data flows in the following manner: It is extracted from Wikipedia via the WikipediaExtractor , which also converts each Wikipedia entry into a JsonElement The WikipediaConverter then converts the Wikipedia JSON entry into an Avro GenericRecord The AvroRecordToAvroWritableConverter converts the Avro GenericRecord to a AvroGenericRecordWritable The HiveSerDeConverter converts the AvroGenericRecordWritable to a OrcSerdeRow The HiveWritableHdfsDataWriter uses the OrcOutputFormat to write the OrcSerdeRow to an OrcFile Extending Gobblin's SerDe Integration While this tutorial only discusses Avro to ORC conversion, it should be relatively straightfoward to use the approach mentioned in this document to convert CSV, JSON, etc. data into ORC.","title":"Writing ORC Data"},{"location":"case-studies/Writing-ORC-Data/#table-of-contents","text":"Table of Contents Introduction Hive SerDe Integration Writing to an ORC File Data Flow Extending Gobblin's SerDe Integration","title":"Table of Contents"},{"location":"case-studies/Writing-ORC-Data/#introduction","text":"Gobblin is capable of writing data to ORC files by leveraging Hive's SerDe library. Gobblin has native integration with Hive SerDe's library via the HiveSerDeWrapper class. This document will briefly explain how Gobblin integrates with Hive's SerDe library, and show an example of writing ORC files.","title":"Introduction"},{"location":"case-studies/Writing-ORC-Data/#hive-serde-integration","text":"Hive's SerDe library defines the interface Hive uses for serialization and deserialization of data. The Hive SerDe library has out of the box SerDe support for Avro, ORC, Parquet, CSV, and JSON SerDes. However, users are free to define custom SerDes. Gobblin integrates with the Hive SerDe's in a few different places. Here is a list of integration points that are relevant for this document: HiveSerDeWrapper wrapper around Hive's SerDe library that provides some nice utilities and structure that the rest of Gobblin can interfact with HiveSerDeConverter takes a Writable object in a specific format, and converts it to the Writable of another format (e.g. from AvroGenericRecordWritable to OrcSerdeRow ) HiveWritableHdfsDataWriter writes a Writable object to a specific file, typically this writes the output of a HiveSerDeConverter","title":"Hive SerDe Integration"},{"location":"case-studies/Writing-ORC-Data/#writing-to-an-orc-file","text":"An end-to-end example of writing to an ORC file is provided in the configuration found here . This .pull file is almost identical to the Wikipedia example discussed in the Getting Started Guide . The only difference is that the output is written in ORC instead of Avro. The configuration file mentioned above can be directly used as a template for writing data to ORC files, below is a detailed explanation of the configuration options that need to be changed, and why they need to be changed. converter.classes requires two additional converters: gobblin.converter.avro.AvroRecordToAvroWritableConverter and gobblin.converter.serde.HiveSerDeConverter The output of the first converter (the WikipediaConverter ) returns Avro GenericRecord s These records must be converted to Writable object in order for the Hive SerDe to process them, which is where the AvroRecordToAvroWritableConverter comes in The HiveSerDeConverter does the actual heavy lifting of converting the Avro Records to ORC Records In order to configure the HiveSerDeConverter the following properites need to be added: serde.deserializer.type=AVRO says that the records being fed into the converter are Avro records avro.schema.literal or avro.schema.url must be set when using this deserializer so that the Hive SerDe knows what Avro Schema to use when converting the record serde.serializer.type=ORC says that the records that should be returned by the converter are ORC records writer.builder.class should be set to gobblin.writer.HiveWritableHdfsDataWriterBuilder This writer class will take the output of the HiveSerDeConverter and write the actual ORC records to an ORC file writer.output.format should be set to ORC ; this ensures the files produced end with the .orc file extension fork.record.queue.capacity should be set to 1 This ensures no caching of records is done before they get passed to the writer; this is necessary because the OrcSerde caches the object it uses to serialize records, and it does not allow copying of Orc Records The example job can be run the same way the regular Wikipedia job is run, except the output will be in the ORC format.","title":"Writing to an ORC File"},{"location":"case-studies/Writing-ORC-Data/#data-flow","text":"For the Wikipedia to ORC example, data flows in the following manner: It is extracted from Wikipedia via the WikipediaExtractor , which also converts each Wikipedia entry into a JsonElement The WikipediaConverter then converts the Wikipedia JSON entry into an Avro GenericRecord The AvroRecordToAvroWritableConverter converts the Avro GenericRecord to a AvroGenericRecordWritable The HiveSerDeConverter converts the AvroGenericRecordWritable to a OrcSerdeRow The HiveWritableHdfsDataWriter uses the OrcOutputFormat to write the OrcSerdeRow to an OrcFile","title":"Data Flow"},{"location":"case-studies/Writing-ORC-Data/#extending-gobblins-serde-integration","text":"While this tutorial only discusses Avro to ORC conversion, it should be relatively straightfoward to use the approach mentioned in this document to convert CSV, JSON, etc. data into ORC.","title":"Extending Gobblin's SerDe Integration"},{"location":"data-management/DistcpNgEvents/","text":"Table of contents Table of contents Copy publisher monitoring events Copy publisher monitoring events The following metadata attributes are shared across all events: Standard execution metadata (TODO add link) namespace=org.apache.gobblin.copy.CopyDataPublisher metadata[\"class\"]=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher Events by name : DatasetPublished - a datasets gets successfully copied and published datasetUrn - the URN of the dataset (dataset-specific) that was copied partition - the partition (dataset-specific) that was copied originTimestamp - the timestamp of the dataset partition as generated in the origin (e.g. the database) upstreamTimestamp - the timestamp of the dataset partition as made available in the upstream system; this will be equal to originTimestamp if the data is read directly from the origin. DatasetPublishFailed - a dataset failed to publish datasetUrn - the URN of the dataset (dataset-specific) whose publish failed FilePublished - sent when an individual file gets published datasetUrn - the URN of the dataset (dataset-specific) of which the part is part partition - the partition (dataset-specific) of which this file is part SourcePath - the full source path for the file TargetPath - the full destination path for the file originTimestamp - similar to originTimestamp for DatasetPublished events upstreamTimestamp - similar to upstreamTimestamp for DatasetPublished events SizeInBytes - the size in bytes of the file","title":"Distcp-NG events"},{"location":"data-management/DistcpNgEvents/#table-of-contents","text":"Table of contents Copy publisher monitoring events","title":"Table of contents"},{"location":"data-management/DistcpNgEvents/#copy-publisher-monitoring-events","text":"The following metadata attributes are shared across all events: Standard execution metadata (TODO add link) namespace=org.apache.gobblin.copy.CopyDataPublisher metadata[\"class\"]=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher Events by name : DatasetPublished - a datasets gets successfully copied and published datasetUrn - the URN of the dataset (dataset-specific) that was copied partition - the partition (dataset-specific) that was copied originTimestamp - the timestamp of the dataset partition as generated in the origin (e.g. the database) upstreamTimestamp - the timestamp of the dataset partition as made available in the upstream system; this will be equal to originTimestamp if the data is read directly from the origin. DatasetPublishFailed - a dataset failed to publish datasetUrn - the URN of the dataset (dataset-specific) whose publish failed FilePublished - sent when an individual file gets published datasetUrn - the URN of the dataset (dataset-specific) of which the part is part partition - the partition (dataset-specific) of which this file is part SourcePath - the full source path for the file TargetPath - the full destination path for the file originTimestamp - similar to originTimestamp for DatasetPublished events upstreamTimestamp - similar to upstreamTimestamp for DatasetPublished events SizeInBytes - the size in bytes of the file","title":"Copy publisher monitoring events"},{"location":"data-management/Gobblin-Retention/","text":"Table Of Contents Table Of Contents Introduction Design Overview of Gobblin Config Management Library Retention Constructs DatasetCleaner DatasetFinder ManagedCleanableDatasetFinder ConfigurableCleanableDataset VersionFinder VersionSelectionPolicy RetentionAction Retention Configuration Tags Dataset overrides Examples Supported Retention Configurations 1. Time based retention 2. Newest K retention 3. Combining multiple policies 4. Datasets with multiple kinds of versions 5. Time based Hive Retention 6. Setting permissions/owner/group for versions of a dataset Introduction Gobblin retention management is a framework to manage the retention of Hadoop datasets. The system allows users to configure retention policies for individual datasets using the Gobblin config store. This framework gives the flexibility to associate retention configurations both at a dataset level and a cluster level. For HDFS datasets, the framework comes with several standard policies like timebased policy, policy to retain top k files in a dataset and many more. It also has in-built support for standard data layouts like daily/hourly paritioned data and snapshot data. Gobblin retention management supports several retention actions. The most basic action is deleting files that satisfy a policy. Gobblin also supports actions like access control which set permissions on files that satisfy a policy. Design The design has two parts. First part describes the contructs like dataset finders, version finders and policies. Second part describes the configuration aspects of gobblin retention management. Overview of Gobblin Config Management Library To support all the retention configuration requirements, we use Gobblin Dataset Config Management library.This is a short overview. In the gobblin code base it can be found in the module gobblin-config-management The Gobblin Dataset Config Management Library is a library for storing, managing and accessing configuration. The library is an extension to TypeSafe Config with additional features like dataset awareness and tags. The library provides a mapping from a config key to a config object. Each config key is represented through a URI. The config object is a map from property name to a property value. A config key K can import one or more config keys I1, I2, ... . The config key K will inherit any properties from I1, I2, \u2026 that are not defined in K. The inheritance is resolved in the order of the keys I1, I2, \u2026 etc., i.e. the property will be resolved to the value in the last Im that defines the property. Applications can create tags T1, T2 etc and import them explicitly in K. We also use the path in the config key URI for implicit tagging. For example, /trackingData/someEvent implicitly imports which /trackingData which implicitly imports /. ConfigClient - The client APIs that an application uses to interact with the library ConfigLibrary - Core implementation that stores the topology of configs in the store. Business logic such as substitution resolution and interpolation of configs happen here. ConfigStore - The physical store for all the configs and tags. Currently a HDFS based ConfigStore is implemented but other physical stores can be implemented Retention Constructs DatasetCleaner The DatasetCleaner is the retention runner. The class takes in job properites as key value pairs. A single DatasetCleaner can manage retention for different kinds of datasets. Each kind of dataset gets its own DatasetFinder . DatasetCleaner is responsible for instantiating all the DatasetFinder s. For each DatasetFinder it finds all the CleanableDataset s and calls the CleanableDataset.clean() method to delete data. To instantiate all the dataset finders, it uses the gobblin.retention.tag job property. This is a comma seperated list of tag URIs in the ConfigStore . A DatasetFinder will be created for every dataset that imports any of these tags. For instance let's say we have a all the event based datasets at /datasets/trackingData in the ConfigStore and it is tagged with a tag /tags/retention/TimeBased . When gobblin.retention.tag is set to /tags/retention/TimeBased . All datasets that are tagged with /tags/retention/TimeBased in the ConfigStore will be processed by this retention job. So in this case a DatasetFinder will be created for /datasets/trackingData . More details about the ConfigStore in Retention Configuration section. DatasetFinder A DatasetFinder is an interface to find all CleanableDataset s ManagedCleanableDatasetFinder This is the most basic implementation of a DatasetFinder that extends a ConfigurableGlobDatasetFinder to find HDFS datasets based on a glob pattern. It uses the ConfigClient to connect to the ConfigStore and get the dataset specific configs for each dataset found. ConfigurableCleanableDataset The ManagedCleanableDatasetFinder instantiates a ConfigurableClenableDataset for every match in the glob pattern. This class reads the dataset config to instatiate a list of VersionFinder and VersionSelectionPolicy pairs. The Retention Configuration section provides details on config keys used to specify the VersionFinder and VersionSelectionPolicy classes. VersionFinder A version is defined as a deletable entity (or a path) in a dataset. A version can either be retained or deleted. The VersionFinder finds all the versions of a dataset. VersionSelectionPolicy A predicate to select subset of versions from the list of all version discovered by the VersionFinder . By default all the versions selected by the VersionSelectionPolicy will be deleted . Apart from delete, gobblin also provides other RetentionAction s on the selected versions. RetentionAction An abstraction for the kind of action to be performed on all the versions discoverd by the VersionFinder or a subset of versions filtered by the VersionSelectionPolicy . Delete is the default action on selected versions. Gobblin also supports AccessControlAction which sets permissions on selected versions. Retention Configuration Gobblin Retention is configured through Gobblin config management. All dataset configs are stored in a config store that can be accessed through a ConfigClient . The gobblin config management uses TypeSafe Config . The language used is HOCON , a more readable JSON superset. The gobblin config management library allows any implementation of config store but for the scope of this document we assume a HDFS based ConfigStore that stores dataset configs in files on HDFS. Let us take an example ConfigStore instance on HDFS as below. \u251c\u2500\u2500 _CONFIG_STORE \u2514\u2500\u2500 2.0 \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 events \u2502 \u2514\u2500\u2500 main.conf \u2502 \u2514\u2500\u2500 includes.conf \u2502 \u251c\u2500\u2500 loginEvent \u2502 \u2514\u2500\u2500 main.conf \u2502 \u2514\u2500\u2500 includes.conf \u251c\u2500\u2500 tags \u2514\u2500\u2500 retention \u2514\u2500\u2500 main.conf \u251c\u2500\u2500 timebased \u2514\u2500\u2500 main.conf Every config store has a store root directory named _CONFIG_STORE . Each new deployment of a store creates a new version (2.0 shown above). Each directory in the store may have a main.conf file and an includes.conf file. The main.conf file holds the config key/value pairs. And includes.conf are used to import other directory paths in the same store. For instance, _CONFIG_STORE/2.0/data/events can import /tags/retention in its includes.conf file. All the key value pairs in /tags/retention/main.conf are automatically imported into /data/events . Note that the directory structure under the configStore correspond to the direcctory structure of data on HDFS. In this case hdfs://data/events/loginEvent 's retention configs are at hdfs://_CONFIG_STORE/2.0/data/events/loginEvent/main.conf in the config store. Tags For maintainability and reusablity we define all the configs as tags and import them into the dataset. Below is a sample timebased retention tag, /tags/retention/timebased/main.conf gobblin.retention : { ##Alias TimeBasedSelectionPolicy=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy DateTimeDatasetVersionFinder=org.apache.gobblin.data.management.version.finder.DateTimeDatasetVersionFinder dataset : { finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder partitions=[${gobblin.retention.daily} } daily : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime=1000d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"daily/*/*/*\" datetime.pattern = \"yyyy/MM/dd\" } } } To apply this retention config to hdfs://data/events the tag /tags/retention/timeBased can be imported by _CONFIG_STORE/2.0/data/events/includes.conf shown below. ###### Include files for /data/events ###### tags/retention/timeBased _CONFIG_STORE/2.0/data/events/includes.conf Will have the configs specific to data/events shown below. ##### Common configs for all of /data/events ###### # Glob pattern to use to find datasets gobblin.dataset.pattern = \"/data/events/*\" Similarly the same tag /tags/retention/timebased can be imported by other datasets as well. Dataset overrides By default all the event datasets under hdfs://data/events get the configs from _CONFIG_STORE/2.0/data/events but sometimes it becomes necessary to override the retention for a specific dataset under hdfs://data/events . This can be done by creating a directory under _CONFIG_STORE/2.0/data/events with the name of dataset and overriding config keys. For instance if we want retention of 1d for loginEvent we can create _CONFIG_STORE/2.0/data/events/loginEvent/main.conf as below. All other event datasets will have the default retention of 1000d. gobblin.retention : { daily : { selection { timeBased.lookbackTime=1d } } } Examples Browse the gobblin-data-management/config-example directory to see example configuration. Supported Retention Configurations Below is a list of ready to use supported retention configurations. But users can always implement their own DatasetFinder , VersionFinder and VersionSelectionPolicy and plug it in. 1. Time based retention To delete data older than some time gobblin.retention : { dataset : { pattern=\"/user/gobblin/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy timeBased.lookbackTime=7d } version : { finder.class=org.apache.gobblin.data.management.version.finder.GlobModTimeDatasetVersionFinder } } 2. Newest K retention To always keep k new versions and delete the rest gobblin.retention : { dataset : { pattern=\"/user/gobblin/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.NewestKSelectionPolicy newestK.versionsNotSelected=2 } version : { finder.class=org.apache.gobblin.data.management.version.finder.GlobModTimeDatasetVersionFinder } } 3. Combining multiple policies The below config deletes versions older than 3 days but making sure we always have at least 2 version. So if we have only 1 version and it is 4 days old it is not deleted. gobblin.retention : { dataset : { pattern=\"/user/gobblin/snapshots/*/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.CombineSelectionPolicy combine.operation=INTERSECT combine.policy.classes=[ org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy, org.apache.gobblin.data.management.policy.NewestKSelectionPolicy ] timeBased.lookbackTime=3d newestK.versionsNotSelected=2 } version : { finder.class=org.apache.gobblin.data.management.version.finder.GlobModTimeDatasetVersionFinder } } 4. Datasets with multiple kinds of versions This is mostly useful for retention management of datasets that have different kinds of versions with each having their own policies. For example an event dataset may have daily and hourly partitions. For daily we may want a higher retention of 5 days but hourly the retention may be set to 2 days. gobblin.retention : { TimeBasedSelectionPolicy=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy DateTimeDatasetVersionFinder=org.apache.gobblin.data.management.version.finder.DateTimeDatasetVersionFinder dataset : { pattern=\"/user/gobblin/data/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder partitions=[${gobblin.retention.hourly}, ${gobblin.retention.daily}] } daily : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 5d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"daily/*/*/*\" datetime.pattern = \"yyyy/MM/dd\" } } hourly : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 2d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"hourly/*/*/*/*\" datetime.pattern = \"yyyy/MM/dd/hh\" } } } 5. Time based Hive Retention Gobblin supports retention for a hive partitioned table. Partitions older than n days can be dropped using this policy. A job can optionally choose to delete data associated with the partition. By default the job does NOT delete data. It only drops the hive partition. gobblin.retention : { is.blacklisted=false dataset : { finder.class=org.apache.gobblin.data.management.retention.dataset.finder.CleanableHiveDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy ## Partitions older than 3 days will be deleted timeBased.lookbackTime=3d } version.finder.class=org.apache.gobblin.data.management.version.finder.DatePartitionHiveVersionFinder hive { partition { key.name=datepartition value.datetime.pattern=yyyy-MM-dd-HH } } } Job level configuration to enable data deletion gobblin.retention.hive.shouldDeleteData=true 6. Setting permissions/owner/group for versions of a dataset Gobblin retention can set permissions, change owner/group for certain versions of a dataset. The below configuration is an extention to example #4, where along with deleting daily versions older than 5 days, it also restricts the access for daily versions older than 4 days to owner only. All the access control policies to apply are discovered through the key accessControl.policies . The below example shows one such policy called ownerOnly . Users can define any arbitrary policy and add them to accessControl.policies . gobblin.retention : { TimeBasedSelectionPolicy=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy DateTimeDatasetVersionFinder=org.apache.gobblin.data.management.version.finder.DateTimeDatasetVersionFinder dataset : { pattern=\"/user/gobblin/data/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder partitions=[${gobblin.retention.hourly}, ${gobblin.retention.daily}] } daily : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 5d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"daily/*/*/*\" datetime.pattern = \"yyyy/MM/dd\" } accessControl { ## Provide a list of comma separated policies to apply. Each entry in this list should have a corresponding config section. policies = [ownerOnly] ownerOnly { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime=4d } mode : 700 user : myUser group : noAccess } } } hourly : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 2d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"hourly/*/*/*/*\" datetime.pattern = \"yyyy/MM/dd/hh\" } } }","title":"Retention"},{"location":"data-management/Gobblin-Retention/#table-of-contents","text":"Table Of Contents Introduction Design Overview of Gobblin Config Management Library Retention Constructs DatasetCleaner DatasetFinder ManagedCleanableDatasetFinder ConfigurableCleanableDataset VersionFinder VersionSelectionPolicy RetentionAction Retention Configuration Tags Dataset overrides Examples Supported Retention Configurations 1. Time based retention 2. Newest K retention 3. Combining multiple policies 4. Datasets with multiple kinds of versions 5. Time based Hive Retention 6. Setting permissions/owner/group for versions of a dataset","title":"Table Of Contents"},{"location":"data-management/Gobblin-Retention/#introduction","text":"Gobblin retention management is a framework to manage the retention of Hadoop datasets. The system allows users to configure retention policies for individual datasets using the Gobblin config store. This framework gives the flexibility to associate retention configurations both at a dataset level and a cluster level. For HDFS datasets, the framework comes with several standard policies like timebased policy, policy to retain top k files in a dataset and many more. It also has in-built support for standard data layouts like daily/hourly paritioned data and snapshot data. Gobblin retention management supports several retention actions. The most basic action is deleting files that satisfy a policy. Gobblin also supports actions like access control which set permissions on files that satisfy a policy.","title":"Introduction"},{"location":"data-management/Gobblin-Retention/#design","text":"The design has two parts. First part describes the contructs like dataset finders, version finders and policies. Second part describes the configuration aspects of gobblin retention management.","title":"Design"},{"location":"data-management/Gobblin-Retention/#overview-of-gobblin-config-management-library","text":"To support all the retention configuration requirements, we use Gobblin Dataset Config Management library.This is a short overview. In the gobblin code base it can be found in the module gobblin-config-management The Gobblin Dataset Config Management Library is a library for storing, managing and accessing configuration. The library is an extension to TypeSafe Config with additional features like dataset awareness and tags. The library provides a mapping from a config key to a config object. Each config key is represented through a URI. The config object is a map from property name to a property value. A config key K can import one or more config keys I1, I2, ... . The config key K will inherit any properties from I1, I2, \u2026 that are not defined in K. The inheritance is resolved in the order of the keys I1, I2, \u2026 etc., i.e. the property will be resolved to the value in the last Im that defines the property. Applications can create tags T1, T2 etc and import them explicitly in K. We also use the path in the config key URI for implicit tagging. For example, /trackingData/someEvent implicitly imports which /trackingData which implicitly imports /. ConfigClient - The client APIs that an application uses to interact with the library ConfigLibrary - Core implementation that stores the topology of configs in the store. Business logic such as substitution resolution and interpolation of configs happen here. ConfigStore - The physical store for all the configs and tags. Currently a HDFS based ConfigStore is implemented but other physical stores can be implemented","title":"Overview of Gobblin Config Management Library"},{"location":"data-management/Gobblin-Retention/#retention-constructs","text":"","title":"Retention Constructs"},{"location":"data-management/Gobblin-Retention/#datasetcleaner","text":"The DatasetCleaner is the retention runner. The class takes in job properites as key value pairs. A single DatasetCleaner can manage retention for different kinds of datasets. Each kind of dataset gets its own DatasetFinder . DatasetCleaner is responsible for instantiating all the DatasetFinder s. For each DatasetFinder it finds all the CleanableDataset s and calls the CleanableDataset.clean() method to delete data. To instantiate all the dataset finders, it uses the gobblin.retention.tag job property. This is a comma seperated list of tag URIs in the ConfigStore . A DatasetFinder will be created for every dataset that imports any of these tags. For instance let's say we have a all the event based datasets at /datasets/trackingData in the ConfigStore and it is tagged with a tag /tags/retention/TimeBased . When gobblin.retention.tag is set to /tags/retention/TimeBased . All datasets that are tagged with /tags/retention/TimeBased in the ConfigStore will be processed by this retention job. So in this case a DatasetFinder will be created for /datasets/trackingData . More details about the ConfigStore in Retention Configuration section.","title":"DatasetCleaner"},{"location":"data-management/Gobblin-Retention/#datasetfinder","text":"A DatasetFinder is an interface to find all CleanableDataset s","title":"DatasetFinder"},{"location":"data-management/Gobblin-Retention/#managedcleanabledatasetfinder","text":"This is the most basic implementation of a DatasetFinder that extends a ConfigurableGlobDatasetFinder to find HDFS datasets based on a glob pattern. It uses the ConfigClient to connect to the ConfigStore and get the dataset specific configs for each dataset found.","title":"ManagedCleanableDatasetFinder"},{"location":"data-management/Gobblin-Retention/#configurablecleanabledataset","text":"The ManagedCleanableDatasetFinder instantiates a ConfigurableClenableDataset for every match in the glob pattern. This class reads the dataset config to instatiate a list of VersionFinder and VersionSelectionPolicy pairs. The Retention Configuration section provides details on config keys used to specify the VersionFinder and VersionSelectionPolicy classes.","title":"ConfigurableCleanableDataset"},{"location":"data-management/Gobblin-Retention/#versionfinder","text":"A version is defined as a deletable entity (or a path) in a dataset. A version can either be retained or deleted. The VersionFinder finds all the versions of a dataset.","title":"VersionFinder"},{"location":"data-management/Gobblin-Retention/#versionselectionpolicy","text":"A predicate to select subset of versions from the list of all version discovered by the VersionFinder . By default all the versions selected by the VersionSelectionPolicy will be deleted . Apart from delete, gobblin also provides other RetentionAction s on the selected versions.","title":"VersionSelectionPolicy"},{"location":"data-management/Gobblin-Retention/#retentionaction","text":"An abstraction for the kind of action to be performed on all the versions discoverd by the VersionFinder or a subset of versions filtered by the VersionSelectionPolicy . Delete is the default action on selected versions. Gobblin also supports AccessControlAction which sets permissions on selected versions.","title":"RetentionAction"},{"location":"data-management/Gobblin-Retention/#retention-configuration","text":"Gobblin Retention is configured through Gobblin config management. All dataset configs are stored in a config store that can be accessed through a ConfigClient . The gobblin config management uses TypeSafe Config . The language used is HOCON , a more readable JSON superset. The gobblin config management library allows any implementation of config store but for the scope of this document we assume a HDFS based ConfigStore that stores dataset configs in files on HDFS. Let us take an example ConfigStore instance on HDFS as below. \u251c\u2500\u2500 _CONFIG_STORE \u2514\u2500\u2500 2.0 \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 events \u2502 \u2514\u2500\u2500 main.conf \u2502 \u2514\u2500\u2500 includes.conf \u2502 \u251c\u2500\u2500 loginEvent \u2502 \u2514\u2500\u2500 main.conf \u2502 \u2514\u2500\u2500 includes.conf \u251c\u2500\u2500 tags \u2514\u2500\u2500 retention \u2514\u2500\u2500 main.conf \u251c\u2500\u2500 timebased \u2514\u2500\u2500 main.conf Every config store has a store root directory named _CONFIG_STORE . Each new deployment of a store creates a new version (2.0 shown above). Each directory in the store may have a main.conf file and an includes.conf file. The main.conf file holds the config key/value pairs. And includes.conf are used to import other directory paths in the same store. For instance, _CONFIG_STORE/2.0/data/events can import /tags/retention in its includes.conf file. All the key value pairs in /tags/retention/main.conf are automatically imported into /data/events . Note that the directory structure under the configStore correspond to the direcctory structure of data on HDFS. In this case hdfs://data/events/loginEvent 's retention configs are at hdfs://_CONFIG_STORE/2.0/data/events/loginEvent/main.conf in the config store.","title":"Retention Configuration"},{"location":"data-management/Gobblin-Retention/#tags","text":"For maintainability and reusablity we define all the configs as tags and import them into the dataset. Below is a sample timebased retention tag, /tags/retention/timebased/main.conf gobblin.retention : { ##Alias TimeBasedSelectionPolicy=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy DateTimeDatasetVersionFinder=org.apache.gobblin.data.management.version.finder.DateTimeDatasetVersionFinder dataset : { finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder partitions=[${gobblin.retention.daily} } daily : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime=1000d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"daily/*/*/*\" datetime.pattern = \"yyyy/MM/dd\" } } } To apply this retention config to hdfs://data/events the tag /tags/retention/timeBased can be imported by _CONFIG_STORE/2.0/data/events/includes.conf shown below. ###### Include files for /data/events ###### tags/retention/timeBased _CONFIG_STORE/2.0/data/events/includes.conf Will have the configs specific to data/events shown below. ##### Common configs for all of /data/events ###### # Glob pattern to use to find datasets gobblin.dataset.pattern = \"/data/events/*\" Similarly the same tag /tags/retention/timebased can be imported by other datasets as well.","title":"Tags"},{"location":"data-management/Gobblin-Retention/#dataset-overrides","text":"By default all the event datasets under hdfs://data/events get the configs from _CONFIG_STORE/2.0/data/events but sometimes it becomes necessary to override the retention for a specific dataset under hdfs://data/events . This can be done by creating a directory under _CONFIG_STORE/2.0/data/events with the name of dataset and overriding config keys. For instance if we want retention of 1d for loginEvent we can create _CONFIG_STORE/2.0/data/events/loginEvent/main.conf as below. All other event datasets will have the default retention of 1000d. gobblin.retention : { daily : { selection { timeBased.lookbackTime=1d } } }","title":"Dataset overrides"},{"location":"data-management/Gobblin-Retention/#examples","text":"Browse the gobblin-data-management/config-example directory to see example configuration.","title":"Examples"},{"location":"data-management/Gobblin-Retention/#supported-retention-configurations","text":"Below is a list of ready to use supported retention configurations. But users can always implement their own DatasetFinder , VersionFinder and VersionSelectionPolicy and plug it in.","title":"Supported Retention Configurations"},{"location":"data-management/Gobblin-Retention/#1-time-based-retention","text":"To delete data older than some time gobblin.retention : { dataset : { pattern=\"/user/gobblin/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy timeBased.lookbackTime=7d } version : { finder.class=org.apache.gobblin.data.management.version.finder.GlobModTimeDatasetVersionFinder } }","title":"1. Time based retention"},{"location":"data-management/Gobblin-Retention/#2-newest-k-retention","text":"To always keep k new versions and delete the rest gobblin.retention : { dataset : { pattern=\"/user/gobblin/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.NewestKSelectionPolicy newestK.versionsNotSelected=2 } version : { finder.class=org.apache.gobblin.data.management.version.finder.GlobModTimeDatasetVersionFinder } }","title":"2. Newest K retention"},{"location":"data-management/Gobblin-Retention/#3-combining-multiple-policies","text":"The below config deletes versions older than 3 days but making sure we always have at least 2 version. So if we have only 1 version and it is 4 days old it is not deleted. gobblin.retention : { dataset : { pattern=\"/user/gobblin/snapshots/*/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.CombineSelectionPolicy combine.operation=INTERSECT combine.policy.classes=[ org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy, org.apache.gobblin.data.management.policy.NewestKSelectionPolicy ] timeBased.lookbackTime=3d newestK.versionsNotSelected=2 } version : { finder.class=org.apache.gobblin.data.management.version.finder.GlobModTimeDatasetVersionFinder } }","title":"3. Combining multiple policies"},{"location":"data-management/Gobblin-Retention/#4-datasets-with-multiple-kinds-of-versions","text":"This is mostly useful for retention management of datasets that have different kinds of versions with each having their own policies. For example an event dataset may have daily and hourly partitions. For daily we may want a higher retention of 5 days but hourly the retention may be set to 2 days. gobblin.retention : { TimeBasedSelectionPolicy=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy DateTimeDatasetVersionFinder=org.apache.gobblin.data.management.version.finder.DateTimeDatasetVersionFinder dataset : { pattern=\"/user/gobblin/data/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder partitions=[${gobblin.retention.hourly}, ${gobblin.retention.daily}] } daily : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 5d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"daily/*/*/*\" datetime.pattern = \"yyyy/MM/dd\" } } hourly : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 2d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"hourly/*/*/*/*\" datetime.pattern = \"yyyy/MM/dd/hh\" } } }","title":"4. Datasets with multiple kinds of versions"},{"location":"data-management/Gobblin-Retention/#5-time-based-hive-retention","text":"Gobblin supports retention for a hive partitioned table. Partitions older than n days can be dropped using this policy. A job can optionally choose to delete data associated with the partition. By default the job does NOT delete data. It only drops the hive partition. gobblin.retention : { is.blacklisted=false dataset : { finder.class=org.apache.gobblin.data.management.retention.dataset.finder.CleanableHiveDatasetFinder } selection : { policy.class=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy ## Partitions older than 3 days will be deleted timeBased.lookbackTime=3d } version.finder.class=org.apache.gobblin.data.management.version.finder.DatePartitionHiveVersionFinder hive { partition { key.name=datepartition value.datetime.pattern=yyyy-MM-dd-HH } } } Job level configuration to enable data deletion gobblin.retention.hive.shouldDeleteData=true","title":"5. Time based Hive Retention"},{"location":"data-management/Gobblin-Retention/#6-setting-permissionsownergroup-for-versions-of-a-dataset","text":"Gobblin retention can set permissions, change owner/group for certain versions of a dataset. The below configuration is an extention to example #4, where along with deleting daily versions older than 5 days, it also restricts the access for daily versions older than 4 days to owner only. All the access control policies to apply are discovered through the key accessControl.policies . The below example shows one such policy called ownerOnly . Users can define any arbitrary policy and add them to accessControl.policies . gobblin.retention : { TimeBasedSelectionPolicy=org.apache.gobblin.data.management.policy.SelectBeforeTimeBasedPolicy DateTimeDatasetVersionFinder=org.apache.gobblin.data.management.version.finder.DateTimeDatasetVersionFinder dataset : { pattern=\"/user/gobblin/data/*\" finder.class=org.apache.gobblin.data.management.retention.profile.ManagedCleanableDatasetFinder partitions=[${gobblin.retention.hourly}, ${gobblin.retention.daily}] } daily : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 5d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"daily/*/*/*\" datetime.pattern = \"yyyy/MM/dd\" } accessControl { ## Provide a list of comma separated policies to apply. Each entry in this list should have a corresponding config section. policies = [ownerOnly] ownerOnly { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime=4d } mode : 700 user : myUser group : noAccess } } } hourly : { selection { policy.class = ${gobblin.retention.TimeBasedSelectionPolicy} timeBased.lookbackTime = 2d } version : { finder.class=${gobblin.retention.DateTimeDatasetVersionFinder} globPattern = \"hourly/*/*/*/*\" datetime.pattern = \"yyyy/MM/dd/hh\" } } }","title":"6. Setting permissions/owner/group for versions of a dataset"},{"location":"developer-guide/CodingStyle/","text":"Overview The code formatting standard in this project is based on the Oracle/Sun Code Convention and Google Java Style . Guideline The coding style is consistent with most of the open source projects with the following callout: Naming Conventions Variables are camel case beginning with a lowercase letter, e.g. fooBar Constant variables are declared as static final and should be all uppercase ASCII letters delimited by underscore (\"_\"), e.g. FOO_BAR Import statement Do not use 'star' imports, e.g. import java.io.* ; Import order: java , org , com , gobblin . Indentation Two spaces should be used as the unit of indentation; Tabs must expand to spaces and the tab width should be set to two; Line length: lines should not exceed 120 characters; White space Blank lines should be provided to improve readability: Between the local variables in a method and its first statement Between methods Blank spaces should be used in the following circumstances: A keyword followed by a parenthesis should be separated by a space (e.g. while (true) { ) A binary operators except . should be separated from their operands by spaces (e.g. a + b ); Comments: Implementation comments: Block comments ( /* ... */ ), end-of-line comments ( //... ) can be used to illustrate a particular implementation; Documentation comments ( /** ... */ ) should be used to describe Java classes, interfaces, methods; Compound statements are lists of statements enclosed in curly braces and should be formatted according to the following conventions: The enclosed statements should be indented one more level than the enclosing statement The opening brace should be on the same line as the enclosing statement (e.g. the 'if' clause) The closing brace should be on a line by itself indented to match the enclosing statement Braces are used around all statements, even single statements, when they are part of a control structure, such as if-else or for statements. This makes it easier to add statements without accidentally introducing bugs due to forgetting to add braces. Code Style Template File Eclipse Download the codetyle-eclipse.xml , Import the file through Preferences Java Code Style Formatter Download the prefs-eclipse.epf , Import the file File Import General Preferences IntelliJ Download the codestyle-intellij-gobblin.xml , Copy the file to the appropriate codestyles directory for your installation. This is typically ~/.INTELLIJ_VERSION/config/codestyles on Linux (or $HOME/Library/Preferences/INTELLIJ_VERSION/codestyles on Mac). The specific INTELLIJ_VERSION identifier will depend on your version; examples are IntelliJIdeal3, IdeaC15 etc. Restart the IDE Go to File Settings Code Style General Scheme to select the new style (LinkedIn Gobblin Style)","title":"Code Style Guide"},{"location":"developer-guide/CodingStyle/#overview","text":"The code formatting standard in this project is based on the Oracle/Sun Code Convention and Google Java Style .","title":"Overview"},{"location":"developer-guide/CodingStyle/#guideline","text":"The coding style is consistent with most of the open source projects with the following callout: Naming Conventions Variables are camel case beginning with a lowercase letter, e.g. fooBar Constant variables are declared as static final and should be all uppercase ASCII letters delimited by underscore (\"_\"), e.g. FOO_BAR Import statement Do not use 'star' imports, e.g. import java.io.* ; Import order: java , org , com , gobblin . Indentation Two spaces should be used as the unit of indentation; Tabs must expand to spaces and the tab width should be set to two; Line length: lines should not exceed 120 characters; White space Blank lines should be provided to improve readability: Between the local variables in a method and its first statement Between methods Blank spaces should be used in the following circumstances: A keyword followed by a parenthesis should be separated by a space (e.g. while (true) { ) A binary operators except . should be separated from their operands by spaces (e.g. a + b ); Comments: Implementation comments: Block comments ( /* ... */ ), end-of-line comments ( //... ) can be used to illustrate a particular implementation; Documentation comments ( /** ... */ ) should be used to describe Java classes, interfaces, methods; Compound statements are lists of statements enclosed in curly braces and should be formatted according to the following conventions: The enclosed statements should be indented one more level than the enclosing statement The opening brace should be on the same line as the enclosing statement (e.g. the 'if' clause) The closing brace should be on a line by itself indented to match the enclosing statement Braces are used around all statements, even single statements, when they are part of a control structure, such as if-else or for statements. This makes it easier to add statements without accidentally introducing bugs due to forgetting to add braces.","title":"Guideline"},{"location":"developer-guide/CodingStyle/#code-style-template-file","text":"Eclipse Download the codetyle-eclipse.xml , Import the file through Preferences Java Code Style Formatter Download the prefs-eclipse.epf , Import the file File Import General Preferences IntelliJ Download the codestyle-intellij-gobblin.xml , Copy the file to the appropriate codestyles directory for your installation. This is typically ~/.INTELLIJ_VERSION/config/codestyles on Linux (or $HOME/Library/Preferences/INTELLIJ_VERSION/codestyles on Mac). The specific INTELLIJ_VERSION identifier will depend on your version; examples are IntelliJIdeal3, IdeaC15 etc. Restart the IDE Go to File Settings Code Style General Scheme to select the new style (LinkedIn Gobblin Style)","title":"Code Style Template File"},{"location":"developer-guide/Contributing/","text":"Table of Contents Table of Contents Contributing to Gobblin Code Contributions Documentation Contributions Contributing to Gobblin You can contribute to Gobblin in multiple ways. For resources and guides, please refer here . Code Contributions We observe standard Apache practices for code contributions. For code changes, we recommend forking the repository and making your local changes on a feature branch, then updating the Jira, and opening a pull request (PR). A committer will review the changes and merge it in once it is approved. For first time contributors to Gobblin, we do request that you fill out a one-time survey , so that we can identify and credit you properly in the future. Documentation Contributions To make changes to the documentation modify the files under gobblin-docs as you would any other version controlled file. All documentation is checked into GitHub, so the process for making documentation changes is similar to how code changes are made (creating Pull Requests). If one wants to see what the rendered documentation looks like they simply need to take the following steps: Install MkDocs locally, this page has directions on how to do so: http://www.mkdocs.org/#installation Make sure you are in the top level directory for the Gobblin repo and execute mkdocs serve These steps will start a local server to server the documentation, simply go to the URL show by the output of mkdocs serve and you should be able to see the documentation. One the changes have been made and tested, create a PR and a committer will review and merge the documentation changes. Updates to the documentation page happen automatically everytime a commit is merged into the master branch; however, there may be a 10 to 15 minute delay before the changes actually show up.","title":"Contributing"},{"location":"developer-guide/Contributing/#table-of-contents","text":"Table of Contents Contributing to Gobblin Code Contributions Documentation Contributions","title":"Table of Contents"},{"location":"developer-guide/Contributing/#contributing-to-gobblin","text":"You can contribute to Gobblin in multiple ways. For resources and guides, please refer here .","title":"Contributing to Gobblin"},{"location":"developer-guide/Contributing/#code-contributions","text":"We observe standard Apache practices for code contributions. For code changes, we recommend forking the repository and making your local changes on a feature branch, then updating the Jira, and opening a pull request (PR). A committer will review the changes and merge it in once it is approved. For first time contributors to Gobblin, we do request that you fill out a one-time survey , so that we can identify and credit you properly in the future.","title":"Code Contributions"},{"location":"developer-guide/Contributing/#documentation-contributions","text":"To make changes to the documentation modify the files under gobblin-docs as you would any other version controlled file. All documentation is checked into GitHub, so the process for making documentation changes is similar to how code changes are made (creating Pull Requests). If one wants to see what the rendered documentation looks like they simply need to take the following steps: Install MkDocs locally, this page has directions on how to do so: http://www.mkdocs.org/#installation Make sure you are in the top level directory for the Gobblin repo and execute mkdocs serve These steps will start a local server to server the documentation, simply go to the URL show by the output of mkdocs serve and you should be able to see the documentation. One the changes have been made and tested, create a PR and a committer will review and merge the documentation changes. Updates to the documentation page happen automatically everytime a commit is merged into the master branch; however, there may be a 10 to 15 minute delay before the changes actually show up.","title":"Documentation Contributions"},{"location":"developer-guide/Customization-for-Converter-and-Operator/","text":"To be updated.","title":"Customization for Converter and Operator"},{"location":"developer-guide/Customization-for-New-Source/","text":"To be updated.","title":"Customization for New Source"},{"location":"developer-guide/Documentation-Architecture/","text":"Table of Contents Table of Contents Documentation Overview GitHub Wiki Limitations MkDocs ReadTheDocs Additional Information Documentation Overview The documentation for Gobblin is based on ReadTheDocs and MkDocs . MkDocs is used to convert MarkDown files to HTML, and ReadTheDocs is used to host the documentation. GitHub Wiki Limitations Historically, documentation was hosted using the GitHub wiki. The problem is that only Gobblin committers can modify the wiki; any external contributors who want to update or add documentation cannot do so. Editing the Gobblin Wiki is also not PR based, so any committer can make changes without going through a review process. MkDocs MkDocs is an open source tool that converts Python Flavored Markdown to HTML files. MkDocs has a number of pre-defined themes that can be used to display the MarkDown files. New themes can be added, or custom CSS and JavaScript can be added to modify existing themes. MkDocs is configured using mkdocs.yml file. This file also specifies a master Table of Contents for the entire website. ReadTheDocs ReadTheDocs is an open source, free tool that can build documentation in a GitHub repository and host it for public use. ReadTheDocs links to a specified GitHub project, and on every push to the repository the documentation is updated. ReadTheDocs essentially clones the repo and builds the documentation using either Sphinx or MkDocs (for Gobblin we only use MkDocs). It then hosts the documentation on internal servers so any end user can view the documentation. ReadTheDocs has other very nice features such as versioning of documentation, exporting documentation to PDFs, and search. ReadTheDocs is configured via its UI, the home page for Gobblin on ReadTheDocs is: https://readthedocs.org/projects/gobblin/. The full documentation for ReadTheDocs can be found here: http://docs.readthedocs.org/ Additional Information For more information on how this architecture was decided and the different tradeoffs between other documentation services, check out the original PR: https://github.com/apache/incubator-gobblin/pull/788","title":"Documentation Architecture"},{"location":"developer-guide/Documentation-Architecture/#table-of-contents","text":"Table of Contents Documentation Overview GitHub Wiki Limitations MkDocs ReadTheDocs Additional Information","title":"Table of Contents"},{"location":"developer-guide/Documentation-Architecture/#documentation-overview","text":"The documentation for Gobblin is based on ReadTheDocs and MkDocs . MkDocs is used to convert MarkDown files to HTML, and ReadTheDocs is used to host the documentation.","title":"Documentation Overview"},{"location":"developer-guide/Documentation-Architecture/#github-wiki-limitations","text":"Historically, documentation was hosted using the GitHub wiki. The problem is that only Gobblin committers can modify the wiki; any external contributors who want to update or add documentation cannot do so. Editing the Gobblin Wiki is also not PR based, so any committer can make changes without going through a review process.","title":"GitHub Wiki Limitations"},{"location":"developer-guide/Documentation-Architecture/#mkdocs","text":"MkDocs is an open source tool that converts Python Flavored Markdown to HTML files. MkDocs has a number of pre-defined themes that can be used to display the MarkDown files. New themes can be added, or custom CSS and JavaScript can be added to modify existing themes. MkDocs is configured using mkdocs.yml file. This file also specifies a master Table of Contents for the entire website.","title":"MkDocs"},{"location":"developer-guide/Documentation-Architecture/#readthedocs","text":"ReadTheDocs is an open source, free tool that can build documentation in a GitHub repository and host it for public use. ReadTheDocs links to a specified GitHub project, and on every push to the repository the documentation is updated. ReadTheDocs essentially clones the repo and builds the documentation using either Sphinx or MkDocs (for Gobblin we only use MkDocs). It then hosts the documentation on internal servers so any end user can view the documentation. ReadTheDocs has other very nice features such as versioning of documentation, exporting documentation to PDFs, and search. ReadTheDocs is configured via its UI, the home page for Gobblin on ReadTheDocs is: https://readthedocs.org/projects/gobblin/. The full documentation for ReadTheDocs can be found here: http://docs.readthedocs.org/","title":"ReadTheDocs"},{"location":"developer-guide/Documentation-Architecture/#additional-information","text":"For more information on how this architecture was decided and the different tradeoffs between other documentation services, check out the original PR: https://github.com/apache/incubator-gobblin/pull/788","title":"Additional Information"},{"location":"developer-guide/Gobblin-Compliance-Design/","text":"Introduction Design Onboarding Purger Gobblin constructs Hive operations Retention Restore Introduction The Gobblin Compliance module allows for data purging to meet regulatory compliance requirements. The module includes purging, retention and restore functionality for datasets. The purging is performed using Hive meaning that purging of datasets is supported in any format that Hive can read from and write to, including for example ORC and Parquet. Further the purger is built on top of the Gobblin framework which means that the fault-tolerance, scalability and flexibility that Gobblin provides is taken full advantage of. The User Guide describes how to onboard a dataset for purging. Design The elements of the Compliance design are: The onboarding process The purge process The retention process The restore process Onboarding A dataset is onboarded to the Purger with these steps: The whitelist includes either of the database or table that will be considered for purging Every table that is to be purged includes the necessary information for purging (dataset descriptor) as a JSON string in its TBLPROPERTIES The purger iterates over all the tables that are whitelisted, and of those tables further looks for the presence of the dataset descriptor to specify the information required by the purger to proceed with the purge process. With this information, the purger iterates over the partitions of the table that needs to be purged and proceeds to purge each partition of the table individually. Purger The purger code is mostly in the gobblin.compliance.purger package. The elements of the purger are: The Gobblin constructs The Hive operations Gobblin constructs The Gobblin constructs that make up the Purger are: HivePurgerSource generates a WorkUnit per partition that needs to be purged HivePurgerExtractor instantiates a PurgeableHivePartitionDataset object that encapsulates all the information required to purge the partition For each partition, HivePurgerConverter populates the purge queries into the PurgeableHivePartitionDataset object The purge queries are executed by HivePurgerWriter The HivePurgerPublisher moves successful Workunits to the COMMITTED state Hive operations The purging process operates as follows: The partition information including location and partitioning scheme is determined from the metadata of the partition A new external staging table is created using the Hive LIKE construct of the current table that is being purged The location of this staging table on HDFS is a new folder within the table location with the current timestamp The purge query executes a LEFT OUTER JOIN of the original table against the table containing the ids whose data is to be purged and INSERT OVERWRITE s this data into the staging table, and thereby location. Once this query returns, the location will contain the purged data Since when we ALTER the original partition location next to the new staging table location, we preserve the location of the current/original location of the partition by creating a backup table pointing to this location. We do not move this immediately to avoid breaking any in-flight queries. The next step is to ALTER the partition location to the location containing the purged data The final step is to DROP the staging table, this only drops the metadata and not the data Taking as an example, a tracking.event table, and the datepartition=2017-02-16-00/is_guest=0 partition, the purge process would be the following: Let's assume the tracking.event table is located at the location /user/tracking/event/ The full partition name would be tracking@event@datepartition=2017-02-16-00/is_guest=0 per Hive, and let's assume the data is located at /user/tracking/event/original/datepartition=2017-02-16-00/is_guest=0/ A staging table tracking.event_staging_1234567890123 ( 1234567890123 is the example timestamp we will use for clarity, a real timestamp looks more like '1487154972824') is created LIKE tracking.event with the location /user/tracking/event/1234567890123/datepartition=2017-02-16-00/is_guest=0/ . This would be within the original table location The purge query would be similar to (assuming u_purger.guestids has the ids whose data is to be purged): INSERT OVERWRITE TABLE tracking@event_staging_1234567890123 PARTITION (datepartition='2017-02-16-00',is_guest='0') SELECT /*+MAPJOIN(b) */ a.metadata.guestid, a.col_a, a.col_b FROM tracking.event a LEFT JOIN u_purger.guestids b ON a.metadata.guestid=b.guestid WHERE b.guestid IS NULL AND a.datepartition='2017-02-16-00' AND a.is_guest='0' A backup table tracking.event_backup_1234567890123 is created with PARTITION datepartition=2017-02-16-00,is_guest=0 pointing to the original location /user/tracking/event/original/datepartition=2017-02-16-00/is_guest=0 The partition location of tracking@event@2017-02-16-00 is updated to be /user/tracking/event/1234567890123/datepartition=2017-02-16-00/is_guest=0 The tracking.event_staging_1234567890123 table is dropped Retention The retention code is mostly in the gobblin.compliance.retention package. The retention process builds on top of Gobblin Retention and performs the following operations: Cleanup of backup data beyond a specified policy Cleanup of any staging tables not cleaned up in case of failures Reaping of backup locations from the original location Cleanup of trash data from the restore process beyond a specified policy Restore The restore code is mostly in the gobblin.compliance.restore package. The restore process allows for restoration to a backup dataset if required.","title":"Gobblin Compliance Design"},{"location":"developer-guide/Gobblin-Compliance-Design/#introduction","text":"The Gobblin Compliance module allows for data purging to meet regulatory compliance requirements. The module includes purging, retention and restore functionality for datasets. The purging is performed using Hive meaning that purging of datasets is supported in any format that Hive can read from and write to, including for example ORC and Parquet. Further the purger is built on top of the Gobblin framework which means that the fault-tolerance, scalability and flexibility that Gobblin provides is taken full advantage of. The User Guide describes how to onboard a dataset for purging.","title":"Introduction"},{"location":"developer-guide/Gobblin-Compliance-Design/#design","text":"The elements of the Compliance design are: The onboarding process The purge process The retention process The restore process","title":"Design"},{"location":"developer-guide/Gobblin-Compliance-Design/#onboarding","text":"A dataset is onboarded to the Purger with these steps: The whitelist includes either of the database or table that will be considered for purging Every table that is to be purged includes the necessary information for purging (dataset descriptor) as a JSON string in its TBLPROPERTIES The purger iterates over all the tables that are whitelisted, and of those tables further looks for the presence of the dataset descriptor to specify the information required by the purger to proceed with the purge process. With this information, the purger iterates over the partitions of the table that needs to be purged and proceeds to purge each partition of the table individually.","title":"Onboarding"},{"location":"developer-guide/Gobblin-Compliance-Design/#purger","text":"The purger code is mostly in the gobblin.compliance.purger package. The elements of the purger are: The Gobblin constructs The Hive operations","title":"Purger"},{"location":"developer-guide/Gobblin-Compliance-Design/#gobblin-constructs","text":"The Gobblin constructs that make up the Purger are: HivePurgerSource generates a WorkUnit per partition that needs to be purged HivePurgerExtractor instantiates a PurgeableHivePartitionDataset object that encapsulates all the information required to purge the partition For each partition, HivePurgerConverter populates the purge queries into the PurgeableHivePartitionDataset object The purge queries are executed by HivePurgerWriter The HivePurgerPublisher moves successful Workunits to the COMMITTED state","title":"Gobblin constructs"},{"location":"developer-guide/Gobblin-Compliance-Design/#hive-operations","text":"The purging process operates as follows: The partition information including location and partitioning scheme is determined from the metadata of the partition A new external staging table is created using the Hive LIKE construct of the current table that is being purged The location of this staging table on HDFS is a new folder within the table location with the current timestamp The purge query executes a LEFT OUTER JOIN of the original table against the table containing the ids whose data is to be purged and INSERT OVERWRITE s this data into the staging table, and thereby location. Once this query returns, the location will contain the purged data Since when we ALTER the original partition location next to the new staging table location, we preserve the location of the current/original location of the partition by creating a backup table pointing to this location. We do not move this immediately to avoid breaking any in-flight queries. The next step is to ALTER the partition location to the location containing the purged data The final step is to DROP the staging table, this only drops the metadata and not the data Taking as an example, a tracking.event table, and the datepartition=2017-02-16-00/is_guest=0 partition, the purge process would be the following: Let's assume the tracking.event table is located at the location /user/tracking/event/ The full partition name would be tracking@event@datepartition=2017-02-16-00/is_guest=0 per Hive, and let's assume the data is located at /user/tracking/event/original/datepartition=2017-02-16-00/is_guest=0/ A staging table tracking.event_staging_1234567890123 ( 1234567890123 is the example timestamp we will use for clarity, a real timestamp looks more like '1487154972824') is created LIKE tracking.event with the location /user/tracking/event/1234567890123/datepartition=2017-02-16-00/is_guest=0/ . This would be within the original table location The purge query would be similar to (assuming u_purger.guestids has the ids whose data is to be purged): INSERT OVERWRITE TABLE tracking@event_staging_1234567890123 PARTITION (datepartition='2017-02-16-00',is_guest='0') SELECT /*+MAPJOIN(b) */ a.metadata.guestid, a.col_a, a.col_b FROM tracking.event a LEFT JOIN u_purger.guestids b ON a.metadata.guestid=b.guestid WHERE b.guestid IS NULL AND a.datepartition='2017-02-16-00' AND a.is_guest='0' A backup table tracking.event_backup_1234567890123 is created with PARTITION datepartition=2017-02-16-00,is_guest=0 pointing to the original location /user/tracking/event/original/datepartition=2017-02-16-00/is_guest=0 The partition location of tracking@event@2017-02-16-00 is updated to be /user/tracking/event/1234567890123/datepartition=2017-02-16-00/is_guest=0 The tracking.event_staging_1234567890123 table is dropped","title":"Hive operations"},{"location":"developer-guide/Gobblin-Compliance-Design/#retention","text":"The retention code is mostly in the gobblin.compliance.retention package. The retention process builds on top of Gobblin Retention and performs the following operations: Cleanup of backup data beyond a specified policy Cleanup of any staging tables not cleaned up in case of failures Reaping of backup locations from the original location Cleanup of trash data from the restore process beyond a specified policy","title":"Retention"},{"location":"developer-guide/Gobblin-Compliance-Design/#restore","text":"The restore code is mostly in the gobblin.compliance.restore package. The restore process allows for restoration to a backup dataset if required.","title":"Restore"},{"location":"developer-guide/GobblinModules/","text":"Table of Contents Table of Contents Introduction How it works gobblin-modules/ Gobblin flavor Current flavors and modules What's next Introduction Gobblin-modules is a way to support customization of the gobblin-distribution build. One of the core features of Gobblin is ability to integrate for a number of systems for data management (sources, targets, monitoring, etc.) Often this leads to inclusion of libraries specific to those systems. Sometimes, such systems also introduce incompatible changes in their APIs (e.g. Kafka 0.8 vs Kafka 0.9). As the adoption of Gobblin grows and we see an increased number of such dependencies, it is no longer easy (or possible) to maintain a single monolithic gobblin-distribution build. This is where gobblin-modules. How it works gobblin-modules/ We are moving non-core functionality which may bring conflicting or large external dependencies to a new location: gobblin-modules/ . This contains the collection of libraries (modules) which bring external depenencies. For example, currently we have: gobblin-kafka-08 - source, writer, metrics reporter using Kafka 0.8 API gobblin-metrics-graphite - metrics reporter to Graphite Other libraries can refer to those modules using standard Gradle dependencies. Gobblin flavor We have added a build property gobblinFlavor which controls what modules to be build and included in the gobblin-distribution tarball. The property can be used as follows ./gradlew -PgobblinFlavor=minimal build Gobblin libraries that support customization can add build files like gobblin-flavor- FLAVOR .gradle which declare the dependencies. For example, let's look at the current gobblin-core/gobblin-flavor-standard.gradle : dependencies { compile project(':gobblin-modules:gobblin-kafka-08') } That specifies that the \"standard\" flavor of Gobblin will include the Kafka 0.8 source, writer and metric reporter. When one specifies the -PgobblinFlavor= FLAVOR during build time, the build script will automatically include the dependencies specified in the corresponding gobblin-flavor- FLAVOR .gradle files in any library that contains such file. Currently, Gobblin defines 4 flavors out of the box: minimal - no modules standard - standard modules for frequently used components. This is the flavor used if none is explicitly specified cluster - modules for running Gobblin clusters (YARN, AWS, stand-alone) full - all non-conflicting modules custom - by default, like minimal but lets users/developers modify and customize the dependencies to be included. Users/developers can define their own flavor files. Current flavors and modules Module Flavors Description gobblin-azkaban standard, full Classes to run gobblin jobs in Azkaban gobblin-aws cluster, full Classes to run gobblin clusters on AWS gobblin-cluster cluster, full Generic classes for running Gobblin clusters gobblin-compliance full Source,converters, writer for cleaning existing datasets for compliance purposes gobblin-helix full State store implementation using Helix/ZK gobblin-kafka-08 standard, full Source, writer and metrics reporter using Kafka 0.8 APIs gobblin-kafka-09 Source, writer and metrics reporter using Kafka 0.9 APIs gobblin-metrics-graphite standard, full metrics reporter to Graphite gobblin-metrics-influxdb standard, full metrics reporter to InfluxDB gobblin-metrics-hadoop standard, full metrics reporter to Hadoop counters gobblin-yarn cluster, full Classes to run gobblin clusters on YARN as a native app google-ingestion standard, full Source/extractors for GoogleWebMaster, GoogleAnalytics, GoogleDrive gobblin-azure-datalake full FileSystem for Azure Data lake Note: Some grandfathered modules may not be in the gobblin-modules/ directory yet. Typically, those are in the root directory. What's next We are in the process of moving existing external dependencies out of gobblin-core into separate modules. To preserve backwards compatibility, we will preserve package and class names and make the \"standard\" flavor of gobblin-core depend on these modules. In the future, new external source, writer and other dependencies are expected to be added directly to gobblin-modules/. Further, we may decide to switch modules between flavors to conrol the number of external dependencies. This will always be done with advanced notice.","title":"Gobblin Modules"},{"location":"developer-guide/GobblinModules/#table-of-contents","text":"Table of Contents Introduction How it works gobblin-modules/ Gobblin flavor Current flavors and modules What's next","title":"Table of Contents"},{"location":"developer-guide/GobblinModules/#introduction","text":"Gobblin-modules is a way to support customization of the gobblin-distribution build. One of the core features of Gobblin is ability to integrate for a number of systems for data management (sources, targets, monitoring, etc.) Often this leads to inclusion of libraries specific to those systems. Sometimes, such systems also introduce incompatible changes in their APIs (e.g. Kafka 0.8 vs Kafka 0.9). As the adoption of Gobblin grows and we see an increased number of such dependencies, it is no longer easy (or possible) to maintain a single monolithic gobblin-distribution build. This is where gobblin-modules.","title":"Introduction"},{"location":"developer-guide/GobblinModules/#how-it-works","text":"","title":"How it works"},{"location":"developer-guide/GobblinModules/#gobblin-modules","text":"We are moving non-core functionality which may bring conflicting or large external dependencies to a new location: gobblin-modules/ . This contains the collection of libraries (modules) which bring external depenencies. For example, currently we have: gobblin-kafka-08 - source, writer, metrics reporter using Kafka 0.8 API gobblin-metrics-graphite - metrics reporter to Graphite Other libraries can refer to those modules using standard Gradle dependencies.","title":"gobblin-modules/"},{"location":"developer-guide/GobblinModules/#gobblin-flavor","text":"We have added a build property gobblinFlavor which controls what modules to be build and included in the gobblin-distribution tarball. The property can be used as follows ./gradlew -PgobblinFlavor=minimal build Gobblin libraries that support customization can add build files like gobblin-flavor- FLAVOR .gradle which declare the dependencies. For example, let's look at the current gobblin-core/gobblin-flavor-standard.gradle : dependencies { compile project(':gobblin-modules:gobblin-kafka-08') } That specifies that the \"standard\" flavor of Gobblin will include the Kafka 0.8 source, writer and metric reporter. When one specifies the -PgobblinFlavor= FLAVOR during build time, the build script will automatically include the dependencies specified in the corresponding gobblin-flavor- FLAVOR .gradle files in any library that contains such file. Currently, Gobblin defines 4 flavors out of the box: minimal - no modules standard - standard modules for frequently used components. This is the flavor used if none is explicitly specified cluster - modules for running Gobblin clusters (YARN, AWS, stand-alone) full - all non-conflicting modules custom - by default, like minimal but lets users/developers modify and customize the dependencies to be included. Users/developers can define their own flavor files.","title":"Gobblin flavor"},{"location":"developer-guide/GobblinModules/#current-flavors-and-modules","text":"Module Flavors Description gobblin-azkaban standard, full Classes to run gobblin jobs in Azkaban gobblin-aws cluster, full Classes to run gobblin clusters on AWS gobblin-cluster cluster, full Generic classes for running Gobblin clusters gobblin-compliance full Source,converters, writer for cleaning existing datasets for compliance purposes gobblin-helix full State store implementation using Helix/ZK gobblin-kafka-08 standard, full Source, writer and metrics reporter using Kafka 0.8 APIs gobblin-kafka-09 Source, writer and metrics reporter using Kafka 0.9 APIs gobblin-metrics-graphite standard, full metrics reporter to Graphite gobblin-metrics-influxdb standard, full metrics reporter to InfluxDB gobblin-metrics-hadoop standard, full metrics reporter to Hadoop counters gobblin-yarn cluster, full Classes to run gobblin clusters on YARN as a native app google-ingestion standard, full Source/extractors for GoogleWebMaster, GoogleAnalytics, GoogleDrive gobblin-azure-datalake full FileSystem for Azure Data lake Note: Some grandfathered modules may not be in the gobblin-modules/ directory yet. Typically, those are in the root directory.","title":"Current flavors and modules"},{"location":"developer-guide/GobblinModules/#whats-next","text":"We are in the process of moving existing external dependencies out of gobblin-core into separate modules. To preserve backwards compatibility, we will preserve package and class names and make the \"standard\" flavor of gobblin-core depend on these modules. In the future, new external source, writer and other dependencies are expected to be added directly to gobblin-modules/. Further, we may decide to switch modules between flavors to conrol the number of external dependencies. This will always be done with advanced notice.","title":"What's next"},{"location":"developer-guide/HighLevelConsumer/","text":"Problem Statement Current Gobblin Kafka High Level Consumer uses Kafka Consumer (0.8) APIs and gobblin support for them will be deprecated. The Re-design's primary goal is to replace old kafka consumer APIs like ConsumerConnector and MessageAndMetadata with a consumer abstraction GobblinKafkaConsumerClient . Additionally, the old design uses kafka auto commit feature which can cause potential loss of messages when offsets are committed and the system fails before messages are processed. Detailed design and implementation details can be found here New Design Details GobblinKafkaConsumerClient The new design uses GobblinKafkaConsumerClient which is a simplified, generic wrapper client to communicate with Kafka. This class does not depend on classes defined in kafka-clients library. This allows the high level consumer to work with different versions of kafka. Concrete classes implementing this interface use a specific version of kafka-client library. See Kafka09ConsumerClient Manual Offset Commit GobblinKafkaConsumerClient API has been enhanced to allow manual committing of offsets. /** * Commit offsets manually to Kafka asynchronously */ default void commitOffsetsAsync(Map KafkaPartition, Long partitionOffsets) { return; } /** * Commit offsets manually to Kafka synchronously */ default void commitOffsetsSync(Map KafkaPartition, Long partitionOffsets) { return; } /** * returns the last committed offset for a KafkaPartition * @param partition * @return last committed offset or -1 for invalid KafkaPartition */ default long committed(KafkaPartition partition) { return -1L; } High level consumer records topic partitions and their offsets AFTER the messages are processed and commits them periodically to kafka. This ensures at-least once delivery in case of a failure. Additionally, APIs are provided to subscribe to a topic along with a GobblinKafkaRebalanceListener that provides hooks to when a consumer joins/leaves a consumer group. In this case, we commit remaining offsets and clear offset caches. /** * Subscribe to a topic * @param topic */ default void subscribe(String topic) { return; } /** * Subscribe to a topic along with a GobblinKafkaRebalanceListener * @param topic */ default void subscribe(String topic, GobblinConsumerRebalanceListener listener) { return; }","title":"High Level Consumer"},{"location":"developer-guide/HighLevelConsumer/#problem-statement","text":"Current Gobblin Kafka High Level Consumer uses Kafka Consumer (0.8) APIs and gobblin support for them will be deprecated. The Re-design's primary goal is to replace old kafka consumer APIs like ConsumerConnector and MessageAndMetadata with a consumer abstraction GobblinKafkaConsumerClient . Additionally, the old design uses kafka auto commit feature which can cause potential loss of messages when offsets are committed and the system fails before messages are processed. Detailed design and implementation details can be found here","title":"Problem Statement"},{"location":"developer-guide/HighLevelConsumer/#new-design-details","text":"GobblinKafkaConsumerClient The new design uses GobblinKafkaConsumerClient which is a simplified, generic wrapper client to communicate with Kafka. This class does not depend on classes defined in kafka-clients library. This allows the high level consumer to work with different versions of kafka. Concrete classes implementing this interface use a specific version of kafka-client library. See Kafka09ConsumerClient Manual Offset Commit GobblinKafkaConsumerClient API has been enhanced to allow manual committing of offsets. /** * Commit offsets manually to Kafka asynchronously */ default void commitOffsetsAsync(Map KafkaPartition, Long partitionOffsets) { return; } /** * Commit offsets manually to Kafka synchronously */ default void commitOffsetsSync(Map KafkaPartition, Long partitionOffsets) { return; } /** * returns the last committed offset for a KafkaPartition * @param partition * @return last committed offset or -1 for invalid KafkaPartition */ default long committed(KafkaPartition partition) { return -1L; } High level consumer records topic partitions and their offsets AFTER the messages are processed and commits them periodically to kafka. This ensures at-least once delivery in case of a failure. Additionally, APIs are provided to subscribe to a topic along with a GobblinKafkaRebalanceListener that provides hooks to when a consumer joins/leaves a consumer group. In this case, we commit remaining offsets and clear offset caches. /** * Subscribe to a topic * @param topic */ default void subscribe(String topic) { return; } /** * Subscribe to a topic along with a GobblinKafkaRebalanceListener * @param topic */ default void subscribe(String topic, GobblinConsumerRebalanceListener listener) { return; }","title":"New Design &amp; Details"},{"location":"developer-guide/IDE-setup/","text":"Table of Contents Table of Contents Introduction IntelliJ Integration Eclipse Integration Lombok Introduction This document is for users who want to import the Gobblin code base into an IDE and directly modify that Gobblin code base. This is not for users who want to just setup Gobblin as a Maven dependency. IntelliJ Integration Gobblin uses standard build tools to import code into an IntelliJ project. Execute the following command to build the necessary *.iml files: ./gradlew clean idea Once the command finishes, use standard practices (File Open; navigate to root of gobblin codebase on filesystem) to import the project into IntelliJ. If you plan to write code, remember to read the coding style guide and import the coding style template file. Eclipse Integration Gobblin uses standard build tools to import code into an Eclipse project. Execute the following command to build the necessary *.classpath and *.project files: ./gradlew clean eclipse Once the command finishes, use standard practices to import the project into Eclipse. If you plan to write code, remember to read the coding style guide and import the coding style template file. Lombok Gobblin uses Lombok for reducing boilerplate code. Lombok auto generates boilerplate code at runtime if you are building gobblin from command line.If you are using an IDE, you will see compile errors in some of the classes that use Lombok. Please follow the IDE setup instructions for your IDE to setup lombok.","title":"IDE setup"},{"location":"developer-guide/IDE-setup/#table-of-contents","text":"Table of Contents Introduction IntelliJ Integration Eclipse Integration Lombok","title":"Table of Contents"},{"location":"developer-guide/IDE-setup/#introduction","text":"This document is for users who want to import the Gobblin code base into an IDE and directly modify that Gobblin code base. This is not for users who want to just setup Gobblin as a Maven dependency.","title":"Introduction"},{"location":"developer-guide/IDE-setup/#intellij-integration","text":"Gobblin uses standard build tools to import code into an IntelliJ project. Execute the following command to build the necessary *.iml files: ./gradlew clean idea Once the command finishes, use standard practices (File Open; navigate to root of gobblin codebase on filesystem) to import the project into IntelliJ. If you plan to write code, remember to read the coding style guide and import the coding style template file.","title":"IntelliJ Integration"},{"location":"developer-guide/IDE-setup/#eclipse-integration","text":"Gobblin uses standard build tools to import code into an Eclipse project. Execute the following command to build the necessary *.classpath and *.project files: ./gradlew clean eclipse Once the command finishes, use standard practices to import the project into Eclipse. If you plan to write code, remember to read the coding style guide and import the coding style template file.","title":"Eclipse Integration"},{"location":"developer-guide/IDE-setup/#lombok","text":"Gobblin uses Lombok for reducing boilerplate code. Lombok auto generates boilerplate code at runtime if you are building gobblin from command line.If you are using an IDE, you will see compile errors in some of the classes that use Lombok. Please follow the IDE setup instructions for your IDE to setup lombok.","title":"Lombok"},{"location":"developer-guide/Monitoring-Design/","text":"Metrics Collection Basics Please refer to Gobblin Metrics Architecture section.","title":"Monitoring Design"},{"location":"developer-guide/Monitoring-Design/#metrics-collection-basics","text":"Please refer to Gobblin Metrics Architecture section.","title":"Metrics Collection Basics"},{"location":"metrics/Existing-Reporters/","text":"Table of Contents Table of Contents Metric Reporters Event Reporters Metric Reporters Output Stream Reporter : allows printing metrics to any OutputStream, including STDOUT and files. Kafka Reporter : emits metrics to Kafka topic as Json messages. Kafka Avro Reporter : emits metrics to Kafka topic as Avro messages with schema MetricReport . Graphite Reporter : emits metrics to Graphite. This reporter has a different, deprecated construction API included in its javadoc. Influx DB Reporter : emits metrics to Influx DB. This reporter has a different, deprecated construction API included in its javadoc. Hadoop Counter Reporter : emits metrics as Hadoop counters at the end of the execution. Available for old and new Hadoop API. This reporter has a different, deprecated construction API included in its javadoc. Due to limits on the number of Hadoop counters that can be created, this reporter is not recommended except for applications with very few metrics. Event Reporters Output Stream Event Reporter : Emits events to any output stream, including STDOUT and files. Kafka Event Reporter : Emits events to Kafka topic as Json messages. Kafka Avro Event Reporter : Emits events to Kafka topic as Avro messages using the schema GobblinTrackingEvent .","title":"Existing Reporters"},{"location":"metrics/Existing-Reporters/#table-of-contents","text":"Table of Contents Metric Reporters Event Reporters","title":"Table of Contents"},{"location":"metrics/Existing-Reporters/#metric-reporters","text":"Output Stream Reporter : allows printing metrics to any OutputStream, including STDOUT and files. Kafka Reporter : emits metrics to Kafka topic as Json messages. Kafka Avro Reporter : emits metrics to Kafka topic as Avro messages with schema MetricReport . Graphite Reporter : emits metrics to Graphite. This reporter has a different, deprecated construction API included in its javadoc. Influx DB Reporter : emits metrics to Influx DB. This reporter has a different, deprecated construction API included in its javadoc. Hadoop Counter Reporter : emits metrics as Hadoop counters at the end of the execution. Available for old and new Hadoop API. This reporter has a different, deprecated construction API included in its javadoc. Due to limits on the number of Hadoop counters that can be created, this reporter is not recommended except for applications with very few metrics.","title":"Metric Reporters"},{"location":"metrics/Existing-Reporters/#event-reporters","text":"Output Stream Event Reporter : Emits events to any output stream, including STDOUT and files. Kafka Event Reporter : Emits events to Kafka topic as Json messages. Kafka Avro Event Reporter : Emits events to Kafka topic as Avro messages using the schema GobblinTrackingEvent .","title":"Event Reporters"},{"location":"metrics/Gobblin-Metrics-Architecture/","text":"Table of Contents Table of Contents Metric Context Metrics Events Event Submitter Reporters RecursiveScheduleMetricReporter EventReporter Metric Context Metric contexts are organized hierarchically in a tree. Each metric context has a set of Tags, each of which is just key-value pair. The keys of all tags are strings, while the values are allowed to be of any type. However, most reporters will serialize the tag values using their toString() method. Children contexts automatically inherit the tags of their parent context, and can add more tags, or override tags present in the parent. Tags can only be defined during construction of each metric context, and are immutable afterwards. This simplifies the inheritance and overriding of metrics. Metric Contexts are created using MetricContext.Builder , which allows adding tags and specifying the parent. This is the only time tags can be added to the context. When building, the tags of the parent and the new tags are merged to obtain the final tags for this context. When building a child context for Metric Context context , calling context.childBuilder(String) generates a Builder with the correct parent. Each metric context contains the following instance variables: A String name . The name is not used by the core metrics engine, but can be accessed by users to identify the context. A reference to the parent metric context, or null if it has no parent. A list of children metric context references, stored as soft references. An object of type Tagged containing the tags for this metric context. A Set of notification targets. Notification targets are objects of type Function Notification , Void which are all called every time there is a new notification. Notifications can be submitted to the Metric Context using the method sendNotification(Notification) . Notification targets can be added using addNotificationTarget(Function Notification, Void ) . A lazily instantiated ExecutorService used for asynchronously executing the notification targets. The executor service will only be started the first time there is a notification and the number of notification targets is positive. A ConcurrentMap from metric names to Metric for all metrics registered in this Metric Context. Metrics can be added to this map using the register(Metric) , register(String, Metric) , or registerAll(MetricSet) , although it is recommended to instead use the methods to create and register the metrics. Metric Context implements getter methods for all metrics, as well as for each type of metric individually ( getMetrics , getGauges , getCounters , getHistograms , getMeters , getTimers ). Metrics All metrics extend the interface ContextAwareMetric . Each metric type in Dropwizard Metrics is extended to a Context Aware type: ContextAwareCounter , ContextAwareGauge , ContextAwareHistogram , ContextAwareMeter , ContextAwareTimer . Context Aware metrics all always created from the Metric Context where they will be registered. For example, to get a counter under Metric Context context , the user would call context.counter(\"counter.name\") . This method first checks all registered metrics in the Metric Context to find a counter with that name, if it succeeds, it simply returns that counter. If a counter with that name has not been registered in context , then a new ContextAwareCounter is created and registered in context . On creation, each Context Aware metric (except Gauges) checks if its parent Metric Context has parents itself. If so, then it automatically creates a metric of the same type, with the same name, in that parent. This will be repeated recursively until, at the end, all ancestor Metric Contexts will all contain a context aware metric of the same type and with the same name. Every time the context aware metric is updated, the metric will automatically call the same update method, with the same update value, for its parent metric. Again, this will continue recursively until the corresponding metrics in all ancestor metric contexts are updated by the same value. If multiple children of a metric context context all have metrics with the same name, when either of them is updated, the corresponding metric in context will also get updated. In this way, the corresponding metric in context will aggregate all updated to the metrics in the children context. Users can also register objects of type com.codahale.metrics.Metric with any Metric Context, but they will not be auto-aggregated. Events Events are objects of type GobblinTrackingEvent , which is a type generated from an Avro schema. Events have: A namespace . A name . A timestamp . A Map String,String of metadata . Events are submitted using the MetricContext#submitEvent(GobblinTrackingEvent) method. When called, this method packages the event into an EventNotification and submits it to the metric context using the method MetricContext#sendNotification(Notification) . This notification is passed to all metrics context ancestors. Each notification target of each ancestor metric context will receive the EventNotification. Events are not stored by any Metric Context, so the notification targets need to handle these events appropriately. Events can be created manually using Avro constructors, and using the method context.submitEvent(GobblinTrackinEvent) , but this is unfriendly when trying to build events incrementally, especially when using metadata. To address this, users can instead use EventSubmitter which is an abstraction around the Avro constructor for GobblinTrackingEvent. Event Submitter An event submitter is created using an EventSubmitter.Builder . It is associated with a Metric Context where it will submit all events, and it contains a namespace and default metadata that will be applied to all events generated through the event submitter. The user can then call EventSubmitter#submit which will package the event with the provided metadata and submit it to the Metric Context. Reporters Reporters export the metrics and/or events of a metric context to a sink. Reporters extend the interface com.codahale.metrics.Reporter . Most reporters will attach themselves to a Metric Context. The reporter can then navigate the Metric Context tree where the Metric Context belongs, get tags and metrics, get notified of events, and export them to the sink. The two best entry points for developing reporters are RecursiveScheduledMetricReporter and EventReporter . These classes do most of the heavy lifting for reporting metrics and events respectively. They are both scheduled reporters, meaning the will export their metrics / events following a configurable schedule. RecursiveScheduleMetricReporter This abstract reporter base is used for emitting metrics on a schedule. The reporter, on creation, is attached to a particular Metric Report. Every time the reporter is required to emit events, the reporter selects the attached Metric Context and all descendant Metric Contexts. For each of these metric contexts, it queries the Metric Context for all metrics, filtered by an optional user supplied filter, and then calls RecursiveScheduledMetricReporter#report , providing the method with all appropriate metrics and tags. Developers need only implement the report method. EventReporter This abstract reporter base is used for emitting events. The EventReporter, on creation, takes a Metric Context it should listen to. It registers a callback function as a notification target for that Metric Context. Every time the callback is called, if the notification is of type EventNotification , the EventReporter unpacks the event and adds it to a LinkedBlockingQueue of events. On a configurable schedule, the event reporter calls the abstract method EventReporter#reportEventQueue(Queue GobblinTrackingEvent ) , which should be implemented by the concrete subclass. To keep memory limited, the event queue has a maximum size. Whenever the queue reaches a size 2/3 of the maximum size, EventReporter#reportEventQueue is called immediately.","title":"Gobblin Metrics Architecture"},{"location":"metrics/Gobblin-Metrics-Architecture/#table-of-contents","text":"Table of Contents Metric Context Metrics Events Event Submitter Reporters RecursiveScheduleMetricReporter EventReporter","title":"Table of Contents"},{"location":"metrics/Gobblin-Metrics-Architecture/#metric-context","text":"Metric contexts are organized hierarchically in a tree. Each metric context has a set of Tags, each of which is just key-value pair. The keys of all tags are strings, while the values are allowed to be of any type. However, most reporters will serialize the tag values using their toString() method. Children contexts automatically inherit the tags of their parent context, and can add more tags, or override tags present in the parent. Tags can only be defined during construction of each metric context, and are immutable afterwards. This simplifies the inheritance and overriding of metrics. Metric Contexts are created using MetricContext.Builder , which allows adding tags and specifying the parent. This is the only time tags can be added to the context. When building, the tags of the parent and the new tags are merged to obtain the final tags for this context. When building a child context for Metric Context context , calling context.childBuilder(String) generates a Builder with the correct parent. Each metric context contains the following instance variables: A String name . The name is not used by the core metrics engine, but can be accessed by users to identify the context. A reference to the parent metric context, or null if it has no parent. A list of children metric context references, stored as soft references. An object of type Tagged containing the tags for this metric context. A Set of notification targets. Notification targets are objects of type Function Notification , Void which are all called every time there is a new notification. Notifications can be submitted to the Metric Context using the method sendNotification(Notification) . Notification targets can be added using addNotificationTarget(Function Notification, Void ) . A lazily instantiated ExecutorService used for asynchronously executing the notification targets. The executor service will only be started the first time there is a notification and the number of notification targets is positive. A ConcurrentMap from metric names to Metric for all metrics registered in this Metric Context. Metrics can be added to this map using the register(Metric) , register(String, Metric) , or registerAll(MetricSet) , although it is recommended to instead use the methods to create and register the metrics. Metric Context implements getter methods for all metrics, as well as for each type of metric individually ( getMetrics , getGauges , getCounters , getHistograms , getMeters , getTimers ).","title":"Metric Context"},{"location":"metrics/Gobblin-Metrics-Architecture/#metrics","text":"All metrics extend the interface ContextAwareMetric . Each metric type in Dropwizard Metrics is extended to a Context Aware type: ContextAwareCounter , ContextAwareGauge , ContextAwareHistogram , ContextAwareMeter , ContextAwareTimer . Context Aware metrics all always created from the Metric Context where they will be registered. For example, to get a counter under Metric Context context , the user would call context.counter(\"counter.name\") . This method first checks all registered metrics in the Metric Context to find a counter with that name, if it succeeds, it simply returns that counter. If a counter with that name has not been registered in context , then a new ContextAwareCounter is created and registered in context . On creation, each Context Aware metric (except Gauges) checks if its parent Metric Context has parents itself. If so, then it automatically creates a metric of the same type, with the same name, in that parent. This will be repeated recursively until, at the end, all ancestor Metric Contexts will all contain a context aware metric of the same type and with the same name. Every time the context aware metric is updated, the metric will automatically call the same update method, with the same update value, for its parent metric. Again, this will continue recursively until the corresponding metrics in all ancestor metric contexts are updated by the same value. If multiple children of a metric context context all have metrics with the same name, when either of them is updated, the corresponding metric in context will also get updated. In this way, the corresponding metric in context will aggregate all updated to the metrics in the children context. Users can also register objects of type com.codahale.metrics.Metric with any Metric Context, but they will not be auto-aggregated.","title":"Metrics"},{"location":"metrics/Gobblin-Metrics-Architecture/#events","text":"Events are objects of type GobblinTrackingEvent , which is a type generated from an Avro schema. Events have: A namespace . A name . A timestamp . A Map String,String of metadata . Events are submitted using the MetricContext#submitEvent(GobblinTrackingEvent) method. When called, this method packages the event into an EventNotification and submits it to the metric context using the method MetricContext#sendNotification(Notification) . This notification is passed to all metrics context ancestors. Each notification target of each ancestor metric context will receive the EventNotification. Events are not stored by any Metric Context, so the notification targets need to handle these events appropriately. Events can be created manually using Avro constructors, and using the method context.submitEvent(GobblinTrackinEvent) , but this is unfriendly when trying to build events incrementally, especially when using metadata. To address this, users can instead use EventSubmitter which is an abstraction around the Avro constructor for GobblinTrackingEvent.","title":"Events"},{"location":"metrics/Gobblin-Metrics-Architecture/#event-submitter","text":"An event submitter is created using an EventSubmitter.Builder . It is associated with a Metric Context where it will submit all events, and it contains a namespace and default metadata that will be applied to all events generated through the event submitter. The user can then call EventSubmitter#submit which will package the event with the provided metadata and submit it to the Metric Context.","title":"Event Submitter"},{"location":"metrics/Gobblin-Metrics-Architecture/#reporters","text":"Reporters export the metrics and/or events of a metric context to a sink. Reporters extend the interface com.codahale.metrics.Reporter . Most reporters will attach themselves to a Metric Context. The reporter can then navigate the Metric Context tree where the Metric Context belongs, get tags and metrics, get notified of events, and export them to the sink. The two best entry points for developing reporters are RecursiveScheduledMetricReporter and EventReporter . These classes do most of the heavy lifting for reporting metrics and events respectively. They are both scheduled reporters, meaning the will export their metrics / events following a configurable schedule.","title":"Reporters"},{"location":"metrics/Gobblin-Metrics-Architecture/#recursiveschedulemetricreporter","text":"This abstract reporter base is used for emitting metrics on a schedule. The reporter, on creation, is attached to a particular Metric Report. Every time the reporter is required to emit events, the reporter selects the attached Metric Context and all descendant Metric Contexts. For each of these metric contexts, it queries the Metric Context for all metrics, filtered by an optional user supplied filter, and then calls RecursiveScheduledMetricReporter#report , providing the method with all appropriate metrics and tags. Developers need only implement the report method.","title":"RecursiveScheduleMetricReporter"},{"location":"metrics/Gobblin-Metrics-Architecture/#eventreporter","text":"This abstract reporter base is used for emitting events. The EventReporter, on creation, takes a Metric Context it should listen to. It registers a callback function as a notification target for that Metric Context. Every time the callback is called, if the notification is of type EventNotification , the EventReporter unpacks the event and adds it to a LinkedBlockingQueue of events. On a configurable schedule, the event reporter calls the abstract method EventReporter#reportEventQueue(Queue GobblinTrackingEvent ) , which should be implemented by the concrete subclass. To keep memory limited, the event queue has a maximum size. Whenever the queue reaches a size 2/3 of the maximum size, EventReporter#reportEventQueue is called immediately.","title":"EventReporter"},{"location":"metrics/Gobblin-Metrics-Performance/","text":"Table of Contents Table of Contents Generalities How to interpret these numbers What if I need larger QPS? Update Metrics Performance Multiple metric updates per iteration Multi-threading Running Performance Tests Generalities These are the main resources used by Gobblin Metrics: CPU time for updating metrics: scales with number of metrics and frequency of metric update CPU time for metric emission and lifecycle management: scales with number of metrics and frequency of emission Memory for storing metrics: scales with number of metrics and metric contexts I/O for reporting metrics: scales with number of metrics and frequency of emission External resources for metrics emission (e.g. HDFS space, Kafka queue space, etc.): scales with number of metrics and frequency of emission This page focuses on the CPU time for updating metrics, as these updates are usually in the critical performance path of an application. Each metric requires bounded memory, and having a few metrics should have no major effect on memory usage. Metrics and Metric Contexts are cleaned when no longer needed to further reduce this impact. Resources related to metric emission can always be reduced by reporting fewer metrics or decreasing the reporting frequency when necessary. How to interpret these numbers This document provides maximum QPS achievable by Gobblin Metrics. If the application attempts to update metrics at a higher rate than this, the metrics will effectively throttle the application. If, on the other hand, the application only updates metrics at 10% or less of the maximum QPS, the performance impact of Gobblin Metrics should be minimal. What if I need larger QPS? If your application needs larger QPS, the recommendation is to batch metrics updates. Counters and Meters offer the option to increase their values by multiple units at a time. Histograms and Timers don't offer this option, but for very high throughput applications, randomly registering for example only 10% of the values will not affect statistics significantly (although you will have to adjust timer and histogram counts manually). Update Metrics Performance Metric updates are the most common interaction with Gobblin Metrics in an application. Every time a counter is increased, a meter is marked, or entries are added to histograms and timers, an update happens. As such, metric updates are the most likely to impact application performance. We measured the max number of metric updates that can be executed per second. The performance of different metric types is different. Also, the performance of metrics depends on the depth in the Metric Context tree at which they are created. Metrics in the Root Metric Context are the fastest, while metrics deep in the tree are slower because they have to update all ancestors as well. The following table shows reference max QPS in updates per second as well as the equivalent single update delay in nanoseconds for each metric type in a i7 processor: Metric Root level Depth: 1 Depth: 2 Depth: 3 Counter 76M (13ns) 39M (25ns) 29M (34ns) 24M (41ns) Meter 11M (90ns) 7M (142ns) 4.5M (222ns) 3.5M (285ns) Histogram 2.4M (416ns) 2.4M (416ns) 1.8M (555ns) 1.3M (769ns) Timer 1.4M (714ns) 1.4M (714ns) 1M (1us) 1M (1us) Multiple metric updates per iteration If a single thread updates multiple metrics, the average delay for metric updates will be the sum of the delays of each metric independently. For example, if each iteration the application is updating two counters, one timer, and one histogram at the root metric context level, the total delay will be 13ns + 13ns + 416ns + 714ns = 1156ns for a max QPS of 865k . Multi-threading Updating metrics with different names can be parallelized efficiently, e.g. different threads updating metrics with different names will not interfere with each other. However, multiple threads updating metrics with the same names will interfere with each other, as the updates of common ancestor metrics are synchronized (to provide with auto-aggregation). In experiments we observed that updating metrics with the same name from multiple threads increases the maximum QPS sub-linearly, saturating at about 3x the single threaded QPS, i.e. the total QPS of metrics updates across any number of threads will not go about 3x the numbers shown in the table above. On the other hand, if each thread is updating multiple metrics, the updates might interleave with each other, potentially increasing the max total QPS. In the example with two counters, one timer, and one histogram, one thread could be updating the timer while another could be updating the histogram, reducing interference, but never exceeding the max QPS of the single most expensive metric. Note that there is no optimization in code to produce this interleaving, it is merely an effect of synchronization, so the effect might vary. Running Performance Tests To run the performance tests cd gobblin-metrics ../gradlew performance After finishing, it should create a TestNG report at build/gobblin-metrics/reports/tests/packages/gobblin.metrics.performance.html . Nicely printed performance results are available on the Output tab.","title":"Gobblin Metrics Performance"},{"location":"metrics/Gobblin-Metrics-Performance/#table-of-contents","text":"Table of Contents Generalities How to interpret these numbers What if I need larger QPS? Update Metrics Performance Multiple metric updates per iteration Multi-threading Running Performance Tests","title":"Table of Contents"},{"location":"metrics/Gobblin-Metrics-Performance/#generalities","text":"These are the main resources used by Gobblin Metrics: CPU time for updating metrics: scales with number of metrics and frequency of metric update CPU time for metric emission and lifecycle management: scales with number of metrics and frequency of emission Memory for storing metrics: scales with number of metrics and metric contexts I/O for reporting metrics: scales with number of metrics and frequency of emission External resources for metrics emission (e.g. HDFS space, Kafka queue space, etc.): scales with number of metrics and frequency of emission This page focuses on the CPU time for updating metrics, as these updates are usually in the critical performance path of an application. Each metric requires bounded memory, and having a few metrics should have no major effect on memory usage. Metrics and Metric Contexts are cleaned when no longer needed to further reduce this impact. Resources related to metric emission can always be reduced by reporting fewer metrics or decreasing the reporting frequency when necessary.","title":"Generalities"},{"location":"metrics/Gobblin-Metrics-Performance/#how-to-interpret-these-numbers","text":"This document provides maximum QPS achievable by Gobblin Metrics. If the application attempts to update metrics at a higher rate than this, the metrics will effectively throttle the application. If, on the other hand, the application only updates metrics at 10% or less of the maximum QPS, the performance impact of Gobblin Metrics should be minimal.","title":"How to interpret these numbers"},{"location":"metrics/Gobblin-Metrics-Performance/#what-if-i-need-larger-qps","text":"If your application needs larger QPS, the recommendation is to batch metrics updates. Counters and Meters offer the option to increase their values by multiple units at a time. Histograms and Timers don't offer this option, but for very high throughput applications, randomly registering for example only 10% of the values will not affect statistics significantly (although you will have to adjust timer and histogram counts manually).","title":"What if I need larger QPS?"},{"location":"metrics/Gobblin-Metrics-Performance/#update-metrics-performance","text":"Metric updates are the most common interaction with Gobblin Metrics in an application. Every time a counter is increased, a meter is marked, or entries are added to histograms and timers, an update happens. As such, metric updates are the most likely to impact application performance. We measured the max number of metric updates that can be executed per second. The performance of different metric types is different. Also, the performance of metrics depends on the depth in the Metric Context tree at which they are created. Metrics in the Root Metric Context are the fastest, while metrics deep in the tree are slower because they have to update all ancestors as well. The following table shows reference max QPS in updates per second as well as the equivalent single update delay in nanoseconds for each metric type in a i7 processor: Metric Root level Depth: 1 Depth: 2 Depth: 3 Counter 76M (13ns) 39M (25ns) 29M (34ns) 24M (41ns) Meter 11M (90ns) 7M (142ns) 4.5M (222ns) 3.5M (285ns) Histogram 2.4M (416ns) 2.4M (416ns) 1.8M (555ns) 1.3M (769ns) Timer 1.4M (714ns) 1.4M (714ns) 1M (1us) 1M (1us)","title":"Update Metrics Performance"},{"location":"metrics/Gobblin-Metrics-Performance/#multiple-metric-updates-per-iteration","text":"If a single thread updates multiple metrics, the average delay for metric updates will be the sum of the delays of each metric independently. For example, if each iteration the application is updating two counters, one timer, and one histogram at the root metric context level, the total delay will be 13ns + 13ns + 416ns + 714ns = 1156ns for a max QPS of 865k .","title":"Multiple metric updates per iteration"},{"location":"metrics/Gobblin-Metrics-Performance/#multi-threading","text":"Updating metrics with different names can be parallelized efficiently, e.g. different threads updating metrics with different names will not interfere with each other. However, multiple threads updating metrics with the same names will interfere with each other, as the updates of common ancestor metrics are synchronized (to provide with auto-aggregation). In experiments we observed that updating metrics with the same name from multiple threads increases the maximum QPS sub-linearly, saturating at about 3x the single threaded QPS, i.e. the total QPS of metrics updates across any number of threads will not go about 3x the numbers shown in the table above. On the other hand, if each thread is updating multiple metrics, the updates might interleave with each other, potentially increasing the max total QPS. In the example with two counters, one timer, and one histogram, one thread could be updating the timer while another could be updating the histogram, reducing interference, but never exceeding the max QPS of the single most expensive metric. Note that there is no optimization in code to produce this interleaving, it is merely an effect of synchronization, so the effect might vary.","title":"Multi-threading"},{"location":"metrics/Gobblin-Metrics-Performance/#running-performance-tests","text":"To run the performance tests cd gobblin-metrics ../gradlew performance After finishing, it should create a TestNG report at build/gobblin-metrics/reports/tests/packages/gobblin.metrics.performance.html . Nicely printed performance results are available on the Output tab.","title":"Running Performance Tests"},{"location":"metrics/Gobblin-Metrics-next-generation-instrumentation-for-applications/","text":"Table of Contents Table of Contents Long running, complex applications are prone to operational issues. Good instrumentation, monitoring, and accessible historical information on its execution helps diagnose them, and many times even prevent them. For Gobblin ingestion, we wanted to add this instrumentation to all parts of the application. Some of the requirements we had were: Report progress of the ingestion processing for each job, task, and module. Many reports would be almost identical, just covering different instances of the same module. Report major milestones in the processing: when a Gobblin job starts, when the ingestion of a dataset finishes, when files of a dataset get committed, etc. Provide various levels of granularity: totals aggregations give a quick view of the performance of the application, but detailed, instance level reports are essential for debugging. Easily switch between sinks where reports and events are emitted. Generate queriable reports. Among existing solutions, we found Dropwizard Metrics to be the closest to what we needed, but it was not enough, so we developed Gobblin Metrics. Gobblin Metrics is a metrics library, which is based on Dropwizard Metrics but extends it considerably to provide all the amazing features that make monitoring and execution auditing easy. The library is designed for modular applications: the application is a set of module instances, organized hierarchically. Following this pattern, the metrics library uses Metric Contexts organized hierarchically to instrument instances of classes and modules (see figure below for an example of this hierarchy for Gobblin ingestion). Each metric context in the tree contains a set of tags describing the context where particular metrics are being collected. Children in this tree automatically inherit the tags of their parents, giving a rich description of each instrumented object in the application. Of course, Gobblin Metrics is not limited to this kind of applications; we have taken advantage of all the other features of the library in much flatter programs. Each metric context manages a set of metrics (like counters, timers, meters, and histograms), providing information for instance on the throughput for each reader and writer, serialization/deserialization times, etc. Metrics are automatically aggregated in the metric context tree: for example, while each writer is computing is throughput independently, we are also computing in real-time the throughput across each task (containing many writers) and each job (containing many tasks). Gobblin Metrics also introduces the concept of events. Events are fire-and-forget reports of milestones of the execution, enriched by metadata relevant to that milestone, plus all of the context information derived from tags. For example, every time we finish processing a file, we emit an event cotaining detailed information like the number of records read, number of records written, and the location where the file was published. The events can be used to get historical information on previous executions, as well as detect and report failures. Finally, the library would not be complete without options to actually export metrics and events to external sinks. Following Dropwizard Metric's model, we use Reporters to write out metrics and events. A few sinks are implemented by default, which we already use heavily: Kafka, OutputStream, Graphite, and InfluxDB. However, any developer can easily implement their own sinks. There is already logic to publish metrics and events as Avro records. Combining this with Hive / Pig, or any other data query engine, allows users to easily generate reports about the execution of their application. In the future we plan to follow the model of Log4j, using configuration files rather than hard coded reporters for metric reporting, which will permit users to quickly change their sinks as well as precisely which metrics and events get reported without touching code. To learn more about Gobblin Metrics, check out the Wiki and the Gobblin project in Github.","title":"Gobblin Metrics next generation instrumentation for applications"},{"location":"metrics/Gobblin-Metrics-next-generation-instrumentation-for-applications/#table-of-contents","text":"Table of Contents Long running, complex applications are prone to operational issues. Good instrumentation, monitoring, and accessible historical information on its execution helps diagnose them, and many times even prevent them. For Gobblin ingestion, we wanted to add this instrumentation to all parts of the application. Some of the requirements we had were: Report progress of the ingestion processing for each job, task, and module. Many reports would be almost identical, just covering different instances of the same module. Report major milestones in the processing: when a Gobblin job starts, when the ingestion of a dataset finishes, when files of a dataset get committed, etc. Provide various levels of granularity: totals aggregations give a quick view of the performance of the application, but detailed, instance level reports are essential for debugging. Easily switch between sinks where reports and events are emitted. Generate queriable reports. Among existing solutions, we found Dropwizard Metrics to be the closest to what we needed, but it was not enough, so we developed Gobblin Metrics. Gobblin Metrics is a metrics library, which is based on Dropwizard Metrics but extends it considerably to provide all the amazing features that make monitoring and execution auditing easy. The library is designed for modular applications: the application is a set of module instances, organized hierarchically. Following this pattern, the metrics library uses Metric Contexts organized hierarchically to instrument instances of classes and modules (see figure below for an example of this hierarchy for Gobblin ingestion). Each metric context in the tree contains a set of tags describing the context where particular metrics are being collected. Children in this tree automatically inherit the tags of their parents, giving a rich description of each instrumented object in the application. Of course, Gobblin Metrics is not limited to this kind of applications; we have taken advantage of all the other features of the library in much flatter programs. Each metric context manages a set of metrics (like counters, timers, meters, and histograms), providing information for instance on the throughput for each reader and writer, serialization/deserialization times, etc. Metrics are automatically aggregated in the metric context tree: for example, while each writer is computing is throughput independently, we are also computing in real-time the throughput across each task (containing many writers) and each job (containing many tasks). Gobblin Metrics also introduces the concept of events. Events are fire-and-forget reports of milestones of the execution, enriched by metadata relevant to that milestone, plus all of the context information derived from tags. For example, every time we finish processing a file, we emit an event cotaining detailed information like the number of records read, number of records written, and the location where the file was published. The events can be used to get historical information on previous executions, as well as detect and report failures. Finally, the library would not be complete without options to actually export metrics and events to external sinks. Following Dropwizard Metric's model, we use Reporters to write out metrics and events. A few sinks are implemented by default, which we already use heavily: Kafka, OutputStream, Graphite, and InfluxDB. However, any developer can easily implement their own sinks. There is already logic to publish metrics and events as Avro records. Combining this with Hive / Pig, or any other data query engine, allows users to easily generate reports about the execution of their application. In the future we plan to follow the model of Log4j, using configuration files rather than hard coded reporters for metric reporting, which will permit users to quickly change their sinks as well as precisely which metrics and events get reported without touching code. To learn more about Gobblin Metrics, check out the Wiki and the Gobblin project in Github.","title":"Table of Contents"},{"location":"metrics/Gobblin-Metrics/","text":"Table of Contents Table of Contents Quick Start Metric Contexts Metrics Events Reporters Gobblin Metrics is a metrics library for emitting metrics and events instrumenting java applications. Metrics and events are easy to use and enriched with tags. Metrics allow full granularity, auto-aggregation, and configurable reporting schedules. Gobblin Metrics is based on Dropwizard Metrics , enhanced to better support modular applications (by providing hierarchical, auto-aggregated metrics) and their monitoring / auditing. Quick Start The following code excerpt shows the functionality of Gobblin Metrics. // ======================================== // METRIC CONTEXTS // ======================================== // Create a Metric context with a Tag MetricContext context = MetricContext.builder( MyMetricContext ).addTag(new Tag Integer ( key , value)).build(); // Create a child metric context. It will automatically inherit tags from parent. // All metrics in the child context will be auto-aggregated in the parent context. MetricContext childContext = context.childBuilder( childContext ).build(); // ======================================== // METRICS // ======================================== // Create a reporter for metrics. This reporter will write metrics to STDOUT. OutputStreamReporter.Factory.newBuilder().build(new Properties()); // Start all metric reporters. RootMetricContext.get().startReporting(); // Create a counter. Counter counter = childContext.counter( my.counter.name ); // Increase the counter. The next time metrics are reported, my.counter.name will be reported as 1. counter.inc(); // ======================================== // EVENTS // ======================================== // Create an reporter for events. This reporter will write events to STDOUT. ScheduledReporter eventReporter = OutputStreamEventReporter.forContext(context).build(); eventReporter.start(); // Create an event submitter, can include default metadata. EventSubmitter eventSubmitter = new EventSubmitter.Builder(context, events.namespace ).addMetadata( metadataKey , value ).build(); // Submit an event. Its metadata will contain all tags in context, all metadata in eventSubmitter, // and any additional metadata specified in the call. // This event will be displayed the next time the event reporter flushes. eventSubmitter.submit( EventName , additionalMetadataKey , value ); Metric Contexts A metric context is a context from which users can emit metrics and events. These contexts contain a set of tags, each tag being a key-value pair. Contexts are hierarchical in nature: each context has one parent and children. They automatically inherit the tags of their parent, and can define or override more tags. Generally, a metric context is associated with a specific instance of an object that should be instrumented. Different instances of the same object will have separate instrumentations. However, each context also aggregates all metrics defined by its descendants, providing with a full range of granularities for reporting. With this functionality if, for example, an application has 10 different data writers, users can monitor each writer individually, or all at the same time. Metrics Metrics are used to monitor the progress of an application. Metrics are emitted regularly following a schedule and represent the current state of the application. The metrics supported by Gobblin Metrics are the same ones as those supported by Dropwizard Metrics Core , adapted for tagging and auto-aggregation. The types supported are: Counter: simple long counter. Meter: counter with added computation of the rate at which the counter is changing. Histogram: stores a histogram of a value, divides all of the values observed into buckets, and reports the count for each bucket. Timer: a histogram for timing information. Gauge: simply stores a value. Gauges are not auto-aggregated because the aggregation operation is context-dependent. Events Events are fire-and-forget messages indicating a milestone in the execution of an application, along with metadata that can provide further information about that event (all tags of the metric context used to generate the event are also added as metadata). Reporters Reporters periodically output the metrics and events to particular sinks following a configurable schedule. Events and Metrics reporters are kept separate to allow users more control in case they want to emit metrics and events to separate sinks (for example, different files). Reporters for a few sinks are implemented by default, but additional sinks can be implemented by extending the RecursiveScheduledMetricReporter and the EventReporter . Each of the included reporters has a simple builder. The metric reporter implementations included with Gobblin Metrics are: OutputStreamReporter: Supports any output stream, including STDOUT and files. KafkaReporter: Emits metrics to a Kafka topic as Json messages. KafkaAvroReporter: Emits metrics to a Kafka topic as Avro messages. InfluxDBReporter: Emits metrics to Influx DB. GraphiteReporter: Emits metrics to Graphite. HadoopCounterReporter: Emits metrics as Hadoop counters. The event reporter implementations included with Gobblin metrics are: OutputStreamEventReporter: Supports any output stream, including STDOUT and files. KafkaEventReporter: Emits events to Kafka as Json messages. KafkaEventAvroReporter: Emits events to Kafka as Avro messages.","title":"Quick Start"},{"location":"metrics/Gobblin-Metrics/#table-of-contents","text":"Table of Contents Quick Start Metric Contexts Metrics Events Reporters Gobblin Metrics is a metrics library for emitting metrics and events instrumenting java applications. Metrics and events are easy to use and enriched with tags. Metrics allow full granularity, auto-aggregation, and configurable reporting schedules. Gobblin Metrics is based on Dropwizard Metrics , enhanced to better support modular applications (by providing hierarchical, auto-aggregated metrics) and their monitoring / auditing.","title":"Table of Contents"},{"location":"metrics/Gobblin-Metrics/#quick-start","text":"The following code excerpt shows the functionality of Gobblin Metrics. // ======================================== // METRIC CONTEXTS // ======================================== // Create a Metric context with a Tag MetricContext context = MetricContext.builder( MyMetricContext ).addTag(new Tag Integer ( key , value)).build(); // Create a child metric context. It will automatically inherit tags from parent. // All metrics in the child context will be auto-aggregated in the parent context. MetricContext childContext = context.childBuilder( childContext ).build(); // ======================================== // METRICS // ======================================== // Create a reporter for metrics. This reporter will write metrics to STDOUT. OutputStreamReporter.Factory.newBuilder().build(new Properties()); // Start all metric reporters. RootMetricContext.get().startReporting(); // Create a counter. Counter counter = childContext.counter( my.counter.name ); // Increase the counter. The next time metrics are reported, my.counter.name will be reported as 1. counter.inc(); // ======================================== // EVENTS // ======================================== // Create an reporter for events. This reporter will write events to STDOUT. ScheduledReporter eventReporter = OutputStreamEventReporter.forContext(context).build(); eventReporter.start(); // Create an event submitter, can include default metadata. EventSubmitter eventSubmitter = new EventSubmitter.Builder(context, events.namespace ).addMetadata( metadataKey , value ).build(); // Submit an event. Its metadata will contain all tags in context, all metadata in eventSubmitter, // and any additional metadata specified in the call. // This event will be displayed the next time the event reporter flushes. eventSubmitter.submit( EventName , additionalMetadataKey , value );","title":"Quick Start"},{"location":"metrics/Gobblin-Metrics/#metric-contexts","text":"A metric context is a context from which users can emit metrics and events. These contexts contain a set of tags, each tag being a key-value pair. Contexts are hierarchical in nature: each context has one parent and children. They automatically inherit the tags of their parent, and can define or override more tags. Generally, a metric context is associated with a specific instance of an object that should be instrumented. Different instances of the same object will have separate instrumentations. However, each context also aggregates all metrics defined by its descendants, providing with a full range of granularities for reporting. With this functionality if, for example, an application has 10 different data writers, users can monitor each writer individually, or all at the same time.","title":"Metric Contexts"},{"location":"metrics/Gobblin-Metrics/#metrics","text":"Metrics are used to monitor the progress of an application. Metrics are emitted regularly following a schedule and represent the current state of the application. The metrics supported by Gobblin Metrics are the same ones as those supported by Dropwizard Metrics Core , adapted for tagging and auto-aggregation. The types supported are: Counter: simple long counter. Meter: counter with added computation of the rate at which the counter is changing. Histogram: stores a histogram of a value, divides all of the values observed into buckets, and reports the count for each bucket. Timer: a histogram for timing information. Gauge: simply stores a value. Gauges are not auto-aggregated because the aggregation operation is context-dependent.","title":"Metrics"},{"location":"metrics/Gobblin-Metrics/#events","text":"Events are fire-and-forget messages indicating a milestone in the execution of an application, along with metadata that can provide further information about that event (all tags of the metric context used to generate the event are also added as metadata).","title":"Events"},{"location":"metrics/Gobblin-Metrics/#reporters","text":"Reporters periodically output the metrics and events to particular sinks following a configurable schedule. Events and Metrics reporters are kept separate to allow users more control in case they want to emit metrics and events to separate sinks (for example, different files). Reporters for a few sinks are implemented by default, but additional sinks can be implemented by extending the RecursiveScheduledMetricReporter and the EventReporter . Each of the included reporters has a simple builder. The metric reporter implementations included with Gobblin Metrics are: OutputStreamReporter: Supports any output stream, including STDOUT and files. KafkaReporter: Emits metrics to a Kafka topic as Json messages. KafkaAvroReporter: Emits metrics to a Kafka topic as Avro messages. InfluxDBReporter: Emits metrics to Influx DB. GraphiteReporter: Emits metrics to Graphite. HadoopCounterReporter: Emits metrics as Hadoop counters. The event reporter implementations included with Gobblin metrics are: OutputStreamEventReporter: Supports any output stream, including STDOUT and files. KafkaEventReporter: Emits events to Kafka as Json messages. KafkaEventAvroReporter: Emits events to Kafka as Avro messages.","title":"Reporters"},{"location":"metrics/Implementing-New-Reporters/","text":"Table of Contents Table of Contents Extending Builders Metric Reporting Event Reporting Other Reporters The two best entry points for implementing custom reporters are RecursiveScheduledMetricReporter and EventReporter . Each of these classes automatically schedules reporting, extracts the correct metrics, and calls a single method that must be implemented by the developer. These methods also implement builder patterns that can be extended by the developer. In the interest of giving more control to the users, metric and event reporters are kept separate, allowing users to more easily specify separate sinks for events and metrics. However, it is possible to implement a single report that handles both events and metrics. It is recommended that each reporter has a constructor with signature init (Properties) . In the near future we are planning to implement auto-starting, file-configurable reporting similar to Log4j architecture, and compliant reporters will be required to have such a constructor. Extending Builders The builder patterns implemented in the base reporters are designed to be extendable. The architecture is a bit complicated, but a subclass of the base reporters wanting to use builder patterns should follow this pattern (replacing with RecursiveScheduledMetricReporter in the case of a metrics reporter): class MyReporter extends EventReporter { private MyReporter(Builder ? builder) throws IOException { super(builder); // Other initialization logic. } // Concrete implementation of extendable Builder. public static class BuilderImpl extends Builder BuilderImpl { private BuilderImpl(MetricContext context) { super(context); } @Override protected BuilderImpl self() { return this; } } public static class Factory { /** * Returns a new {@link MyReporter.Builder} for {@link MyReporter}. * Will automatically add all Context tags to the reporter. * * @param context the {@link org.apache.gobblin.metrics.MetricContext} to report * @return MyReporter builder */ public static BuilderImpl forContext(MetricContext context) { return new BuilderImpl(context); } } /** * Builder for {@link MyReporter}. */ public static abstract class Builder T extends EventReporter.Builder T extends EventReporter.Builder T { // Additional instance variables needed to construct MyReporter. private int myBuilderVariable; protected Builder(MetricContext context) { super(context); this.myBuilderVariable = 0; } /** * Set myBuilderVariable. */ public T withMyBuilderVariable(int value) { this.myBuilderVariable = value; return self(); } // Other setters for Builder variables. /** * Builds and returns {@link MyReporter}. */ public MyReporter build() throws IOException { return new MyReporter(this); } } } This pattern allows users to simply call MyReporter reporter = MyReporter.Factory.forContext(context).build(); to generate an instance of the reporter. Additionally, if you want to further extend MyReporter, following the exact same pattern except extending MyReporter instead of EventReporter will work correctly (which would not be true with standard Builder pattern). Metric Reporting Developers should extend RecursiveScheduledMetricReporter and implement the method RecursiveScheduledMetricReporter#report . The base class will call report when appropriate with the list of metrics, separated by type, and tags that should be reported. Event Reporting Developers should extend EventReporter and implement the method EventReporter#reportEventQueue(Queue GobblinTrackingEvent ) . The base class will call this method with a queue of all events to report as needed. Other Reporters It is also possible to implement a reporter without using the suggested classes. Reporters are recommended, but not required, to extend the interface Reporter . Reporters can use the public methods of MetricContext to navigate the Metric Context tree, query metrics, and register for notifications.","title":"Implementing New Reporters"},{"location":"metrics/Implementing-New-Reporters/#table-of-contents","text":"Table of Contents Extending Builders Metric Reporting Event Reporting Other Reporters The two best entry points for implementing custom reporters are RecursiveScheduledMetricReporter and EventReporter . Each of these classes automatically schedules reporting, extracts the correct metrics, and calls a single method that must be implemented by the developer. These methods also implement builder patterns that can be extended by the developer. In the interest of giving more control to the users, metric and event reporters are kept separate, allowing users to more easily specify separate sinks for events and metrics. However, it is possible to implement a single report that handles both events and metrics. It is recommended that each reporter has a constructor with signature init (Properties) . In the near future we are planning to implement auto-starting, file-configurable reporting similar to Log4j architecture, and compliant reporters will be required to have such a constructor.","title":"Table of Contents"},{"location":"metrics/Implementing-New-Reporters/#extending-builders","text":"The builder patterns implemented in the base reporters are designed to be extendable. The architecture is a bit complicated, but a subclass of the base reporters wanting to use builder patterns should follow this pattern (replacing with RecursiveScheduledMetricReporter in the case of a metrics reporter): class MyReporter extends EventReporter { private MyReporter(Builder ? builder) throws IOException { super(builder); // Other initialization logic. } // Concrete implementation of extendable Builder. public static class BuilderImpl extends Builder BuilderImpl { private BuilderImpl(MetricContext context) { super(context); } @Override protected BuilderImpl self() { return this; } } public static class Factory { /** * Returns a new {@link MyReporter.Builder} for {@link MyReporter}. * Will automatically add all Context tags to the reporter. * * @param context the {@link org.apache.gobblin.metrics.MetricContext} to report * @return MyReporter builder */ public static BuilderImpl forContext(MetricContext context) { return new BuilderImpl(context); } } /** * Builder for {@link MyReporter}. */ public static abstract class Builder T extends EventReporter.Builder T extends EventReporter.Builder T { // Additional instance variables needed to construct MyReporter. private int myBuilderVariable; protected Builder(MetricContext context) { super(context); this.myBuilderVariable = 0; } /** * Set myBuilderVariable. */ public T withMyBuilderVariable(int value) { this.myBuilderVariable = value; return self(); } // Other setters for Builder variables. /** * Builds and returns {@link MyReporter}. */ public MyReporter build() throws IOException { return new MyReporter(this); } } } This pattern allows users to simply call MyReporter reporter = MyReporter.Factory.forContext(context).build(); to generate an instance of the reporter. Additionally, if you want to further extend MyReporter, following the exact same pattern except extending MyReporter instead of EventReporter will work correctly (which would not be true with standard Builder pattern).","title":"Extending Builders"},{"location":"metrics/Implementing-New-Reporters/#metric-reporting","text":"Developers should extend RecursiveScheduledMetricReporter and implement the method RecursiveScheduledMetricReporter#report . The base class will call report when appropriate with the list of metrics, separated by type, and tags that should be reported.","title":"Metric Reporting"},{"location":"metrics/Implementing-New-Reporters/#event-reporting","text":"Developers should extend EventReporter and implement the method EventReporter#reportEventQueue(Queue GobblinTrackingEvent ) . The base class will call this method with a queue of all events to report as needed.","title":"Event Reporting"},{"location":"metrics/Implementing-New-Reporters/#other-reporters","text":"It is also possible to implement a reporter without using the suggested classes. Reporters are recommended, but not required, to extend the interface Reporter . Reporters can use the public methods of MetricContext to navigate the Metric Context tree, query metrics, and register for notifications.","title":"Other Reporters"},{"location":"metrics/Metrics-for-Gobblin-ETL/","text":"Table of Contents Table of Contents Configuring Metrics and Event emission Operational Metrics Extractor Metrics Converter Metrics Fork Operator Metrics Row Level Policy Metrics Data Writer Metrics Runtime Events Job Progression Events Job Timing Events Customizing Instrumentation Custom constructs Instrumentable Interface Callback Methods Custom Reporters Gobblin ETL comes equipped with instrumentation using Gobblin Metrics , as well as end points to easily extend this instrumentation. Configuring Metrics and Event emission The following configurations are used for metrics and event emission: Configuration Key Definition Default metrics.enabled Whether metrics are enabled. If false, will not report metrics. true metrics.report.interval Metrics report interval in milliseconds. 30000 metrics.reporting.file.enabled Whether metrics will be reported to a file. false metrics.log.dir If file enabled, the directory where metrics will be written. If missing, will not report to file. N/A metrics.reporting.kafka.enabled Whether metrics will be reported to Kafka. false metrics.reporting.kafka.brokers Kafka brokers for Kafka metrics emission. N/A metrics.reporting.kafka.topic.metrics Kafka topic where metrics (but not events) will be reported. N/A metrics.reporting.kafka.topic.events Kafka topic where events (but not metrics) will be reported. N/A metrics.reporting.kafka.format Format of metrics / events emitted to Kafka. (Options: json, avro) json metrics.reporting.kafka.avro.use.schema.registry Whether to use a schema registry for Kafka emitting. false kafka.schema.registry.url If using schema registry, the url of the schema registry. N/A metrics.reporting.jmx.enabled Whether to report metrics to JMX. false metrics.reporting.custom.builders Comma-separated list of classes for custom metrics reporters. (See Custom Reporters ) Operational Metrics Each construct in a Gobblin ETL run computes metrics regarding it's performance / progress. Each metric is tagged by default with the following tags: jobName: Gobblin generated name for the job. jobId: Gobblin generated id for the job. clusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host. taskId: Gobblin generated id for the task that generated the metric. construct: construct type that generated the metric (e.g. extractor, converter, etc.) class: specific class of the construct that generated the metric. finalMetricReport: metrics are emitted regularly. Sometimes it is useful to select only the last report from each context. To aid with this, some reporters will add this tag with value \"true\" only to the final report from a metric context. This is the list of operational metrics implemented by default, grouped by construct. Extractor Metrics gobblin.extractor.records.read: meter for records read. gobblin.extractor.records.failed: meter for records failed to read. gobblin.extractor.extract.time: timer for reading of records. Converter Metrics gobblin.converter.records.in: meter for records going into the converter. gobblin.converter.records.out: meter for records outputted by the converter. gobblin.converter.records.failed: meter for records that failed to be converted. gobblin.converter.convert.time: timer for conversion time of each record. Fork Operator Metrics gobblin.fork.operator.records.in: meter for records going into the fork operator. gobblin.fork.operator.forks.out: meter for records going out of the fork operator (each record is counted once for each fork it is emitted to). gobblin.fork.operator.fork.time: timer for forking of each record. Row Level Policy Metrics gobblin.qualitychecker.records.in: meter for records going into the row level policy. gobblin.qualitychecker.records.passed: meter for records passing the row level policy check. gobblin.qualitychecker.records.failed: meter for records failing the row level policy check. gobblin.qualitychecker.check.time: timer for row level policy checking of each record. Data Writer Metrics gobblin.writer.records.in: meter for records requested to be written. gobblin.writer.records.written: meter for records actually written. gobblin.writer.records.failed: meter for records failed to be written. gobblin.writer.write.time: timer for writing each record. Runtime Events The Gobblin ETL runtime emits events marking its progress. All events have the following metadata: jobName: Gobblin generated name for the job. jobId: Gobblin generated id for the job. clusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host. taskId: Gobblin generated id for the task that generated the metric (if applicable). This is the list of events that are emitted by the Gobblin runtime: Job Progression Events LockInUse: emitted if a job fails because it fails to get a lock. WorkUnitsMissing: emitted if a job exits because source failed to get work units. WorkUnitsEmpty: emitted if a job exits because there were no work units to process. WorkUnitsCreated: emitted when workunits are created for a task. Metadata: workUnitsCreated(Number of bin-packed workunits created). TasksSubmitted: emitted when tasks are submitted for execution. Metadata: tasksCount(number of tasks submitted). TaskFailed: emitted when a task fails. Metadata: taskId(id of the failed task). Job_Successful: emitted at the end of a successful job. Job_Failed: emitted at the end of a failed job. Job Timing Events These events give information on timing on certain parts of the execution. Each timing event contains the following metadata: startTime: timestamp when the timed processing started. endTime: timestamp when the timed processing finished. durationMillis: duration in milliseconds of the timed processing. eventType: always \"timingEvent\" for timing events. The following timing events are emitted: FullJobExecutionTimer: times the entire job execution. WorkUnitsCreationTimer: times the creation of work units. WorkUnitsPreparationTime: times the preparation of work units. JobRunTimer: times the actual running of job (i.e. processing of all work units). JobCommitTimer: times the committing of work units. JobCleanupTimer: times the job cleanup. JobLocalSetupTimer: times the setup of a local job. JobMrStagingDataCleanTimer: times the deletion of staging directories from previous work units (MR mode). JobMrDistributedCacheSetupTimer: times the setting up of distributed cache (MR mode). JobMrSetupTimer: times the setup of the MR job (MR mode). JobMrRunTimer: times the execution of the MR job (MR mode). Customizing Instrumentation Custom constructs When using a custom construct (for example a custom extractor for your data source), you will get the above mentioned instrumentation for free. However, you may want to implement additional metrics. To aid with this, instead of extending the usual class Extractor, you can extend the class gobblin.instrumented.extractor.InstrumentedExtractor . Similarly, for each construct there is an instrumented version that allows extension of the default metrics ( InstrumentedExtractor , InstrumentedConverter , InstrumentedForkOperator , InstrumentedRowLevelPolicy , and InstrumentedDataWriter ). All of the instrumented constructs have Javadoc providing with additional information. In general, when extending an instrumented construct, you will have to implement a different method. For example, when extending an InstrumentedExtractor, instead of implementing readRecord , you will implement readRecordImpl . To make this clearer for the user, implementing readRecord will throw a compilation error, and the javadoc of each method specifies the method that should be implemented. Instrumentable Interface Instrumented constructs extend the interface Instrumentable . It contains the following methods: getMetricContext() : get the default metric context generated for that instance of the construct, with all the appropriate tags. Use this metric context to create any additional metrics. isInstrumentationEnabled() : returns true if instrumentation is enabled. switchMetricsContext(List Tag ? ) : switches the default metric context returned by getMetricContext() to a metric context containing the supplied tags. All default metrics will be reported to the new metric context. This method is useful when the state of a construct changes during the execution, and the user desires to reflect that in the emitted tags (for example, Kafka extractor can handle multiple topics in the same extractor, and we want to reflect this in the metrics). switchMetricContext(MetricContext) : similar to the above method, but uses the supplied metric context instead of generating a new metric context. It is the responsibility of the caller to ensure the new metric context has the correct tags and parent. The following method can be re-implemented by the user: * generateTags(State) : this method should return a list of tags to use for metric contexts created for this construct. If overriding this method, it is always a good idea to call super() and only append tags to this list. Callback Methods Instrumented constructs have a set of callback methods that are called at different points in the processing of each record, and which can be used to update metrics. For example, the InstrumentedExtractor has the callbacks beforeRead() , afterRead(D, long) , and onException(Exception) . The javadoc for the instrumented constructs has further descriptions for each callback. Users should always call super() when overriding this callbacks, as default metrics depend on that. Custom Reporters Besides the reporters implemented by default (file, Kafka, and JMX), users can add custom reporters to the classpath and instruct Gobblin to use these reporters. To do this, users should extend the interface CustomReporterFactory , and specify a comma-separated list of CustomReporterFactory classes in the configuration key metrics.reporting.custom.builders . Gobblin will automatically search for these CustomReporterFactory implementations, instantiate each one with a parameter-less constructor, and then call the method newScheduledReporter(MetricContext, Properties) , where the properties contain all of the input configurations supplied to Gobblin. Gobblin will then manage this ScheduledReporter .","title":"Metrics for Gobblin ETL"},{"location":"metrics/Metrics-for-Gobblin-ETL/#table-of-contents","text":"Table of Contents Configuring Metrics and Event emission Operational Metrics Extractor Metrics Converter Metrics Fork Operator Metrics Row Level Policy Metrics Data Writer Metrics Runtime Events Job Progression Events Job Timing Events Customizing Instrumentation Custom constructs Instrumentable Interface Callback Methods Custom Reporters Gobblin ETL comes equipped with instrumentation using Gobblin Metrics , as well as end points to easily extend this instrumentation.","title":"Table of Contents"},{"location":"metrics/Metrics-for-Gobblin-ETL/#configuring-metrics-and-event-emission","text":"The following configurations are used for metrics and event emission: Configuration Key Definition Default metrics.enabled Whether metrics are enabled. If false, will not report metrics. true metrics.report.interval Metrics report interval in milliseconds. 30000 metrics.reporting.file.enabled Whether metrics will be reported to a file. false metrics.log.dir If file enabled, the directory where metrics will be written. If missing, will not report to file. N/A metrics.reporting.kafka.enabled Whether metrics will be reported to Kafka. false metrics.reporting.kafka.brokers Kafka brokers for Kafka metrics emission. N/A metrics.reporting.kafka.topic.metrics Kafka topic where metrics (but not events) will be reported. N/A metrics.reporting.kafka.topic.events Kafka topic where events (but not metrics) will be reported. N/A metrics.reporting.kafka.format Format of metrics / events emitted to Kafka. (Options: json, avro) json metrics.reporting.kafka.avro.use.schema.registry Whether to use a schema registry for Kafka emitting. false kafka.schema.registry.url If using schema registry, the url of the schema registry. N/A metrics.reporting.jmx.enabled Whether to report metrics to JMX. false metrics.reporting.custom.builders Comma-separated list of classes for custom metrics reporters. (See Custom Reporters )","title":"Configuring Metrics and Event emission"},{"location":"metrics/Metrics-for-Gobblin-ETL/#operational-metrics","text":"Each construct in a Gobblin ETL run computes metrics regarding it's performance / progress. Each metric is tagged by default with the following tags: jobName: Gobblin generated name for the job. jobId: Gobblin generated id for the job. clusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host. taskId: Gobblin generated id for the task that generated the metric. construct: construct type that generated the metric (e.g. extractor, converter, etc.) class: specific class of the construct that generated the metric. finalMetricReport: metrics are emitted regularly. Sometimes it is useful to select only the last report from each context. To aid with this, some reporters will add this tag with value \"true\" only to the final report from a metric context. This is the list of operational metrics implemented by default, grouped by construct.","title":"Operational Metrics"},{"location":"metrics/Metrics-for-Gobblin-ETL/#extractor-metrics","text":"gobblin.extractor.records.read: meter for records read. gobblin.extractor.records.failed: meter for records failed to read. gobblin.extractor.extract.time: timer for reading of records.","title":"Extractor Metrics"},{"location":"metrics/Metrics-for-Gobblin-ETL/#converter-metrics","text":"gobblin.converter.records.in: meter for records going into the converter. gobblin.converter.records.out: meter for records outputted by the converter. gobblin.converter.records.failed: meter for records that failed to be converted. gobblin.converter.convert.time: timer for conversion time of each record.","title":"Converter Metrics"},{"location":"metrics/Metrics-for-Gobblin-ETL/#fork-operator-metrics","text":"gobblin.fork.operator.records.in: meter for records going into the fork operator. gobblin.fork.operator.forks.out: meter for records going out of the fork operator (each record is counted once for each fork it is emitted to). gobblin.fork.operator.fork.time: timer for forking of each record.","title":"Fork Operator Metrics"},{"location":"metrics/Metrics-for-Gobblin-ETL/#row-level-policy-metrics","text":"gobblin.qualitychecker.records.in: meter for records going into the row level policy. gobblin.qualitychecker.records.passed: meter for records passing the row level policy check. gobblin.qualitychecker.records.failed: meter for records failing the row level policy check. gobblin.qualitychecker.check.time: timer for row level policy checking of each record.","title":"Row Level Policy Metrics"},{"location":"metrics/Metrics-for-Gobblin-ETL/#data-writer-metrics","text":"gobblin.writer.records.in: meter for records requested to be written. gobblin.writer.records.written: meter for records actually written. gobblin.writer.records.failed: meter for records failed to be written. gobblin.writer.write.time: timer for writing each record.","title":"Data Writer Metrics"},{"location":"metrics/Metrics-for-Gobblin-ETL/#runtime-events","text":"The Gobblin ETL runtime emits events marking its progress. All events have the following metadata: jobName: Gobblin generated name for the job. jobId: Gobblin generated id for the job. clusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host. taskId: Gobblin generated id for the task that generated the metric (if applicable). This is the list of events that are emitted by the Gobblin runtime:","title":"Runtime Events"},{"location":"metrics/Metrics-for-Gobblin-ETL/#job-progression-events","text":"LockInUse: emitted if a job fails because it fails to get a lock. WorkUnitsMissing: emitted if a job exits because source failed to get work units. WorkUnitsEmpty: emitted if a job exits because there were no work units to process. WorkUnitsCreated: emitted when workunits are created for a task. Metadata: workUnitsCreated(Number of bin-packed workunits created). TasksSubmitted: emitted when tasks are submitted for execution. Metadata: tasksCount(number of tasks submitted). TaskFailed: emitted when a task fails. Metadata: taskId(id of the failed task). Job_Successful: emitted at the end of a successful job. Job_Failed: emitted at the end of a failed job.","title":"Job Progression Events"},{"location":"metrics/Metrics-for-Gobblin-ETL/#job-timing-events","text":"These events give information on timing on certain parts of the execution. Each timing event contains the following metadata: startTime: timestamp when the timed processing started. endTime: timestamp when the timed processing finished. durationMillis: duration in milliseconds of the timed processing. eventType: always \"timingEvent\" for timing events. The following timing events are emitted: FullJobExecutionTimer: times the entire job execution. WorkUnitsCreationTimer: times the creation of work units. WorkUnitsPreparationTime: times the preparation of work units. JobRunTimer: times the actual running of job (i.e. processing of all work units). JobCommitTimer: times the committing of work units. JobCleanupTimer: times the job cleanup. JobLocalSetupTimer: times the setup of a local job. JobMrStagingDataCleanTimer: times the deletion of staging directories from previous work units (MR mode). JobMrDistributedCacheSetupTimer: times the setting up of distributed cache (MR mode). JobMrSetupTimer: times the setup of the MR job (MR mode). JobMrRunTimer: times the execution of the MR job (MR mode).","title":"Job Timing Events"},{"location":"metrics/Metrics-for-Gobblin-ETL/#customizing-instrumentation","text":"","title":"Customizing Instrumentation"},{"location":"metrics/Metrics-for-Gobblin-ETL/#custom-constructs","text":"When using a custom construct (for example a custom extractor for your data source), you will get the above mentioned instrumentation for free. However, you may want to implement additional metrics. To aid with this, instead of extending the usual class Extractor, you can extend the class gobblin.instrumented.extractor.InstrumentedExtractor . Similarly, for each construct there is an instrumented version that allows extension of the default metrics ( InstrumentedExtractor , InstrumentedConverter , InstrumentedForkOperator , InstrumentedRowLevelPolicy , and InstrumentedDataWriter ). All of the instrumented constructs have Javadoc providing with additional information. In general, when extending an instrumented construct, you will have to implement a different method. For example, when extending an InstrumentedExtractor, instead of implementing readRecord , you will implement readRecordImpl . To make this clearer for the user, implementing readRecord will throw a compilation error, and the javadoc of each method specifies the method that should be implemented.","title":"Custom constructs"},{"location":"metrics/Metrics-for-Gobblin-ETL/#instrumentable-interface","text":"Instrumented constructs extend the interface Instrumentable . It contains the following methods: getMetricContext() : get the default metric context generated for that instance of the construct, with all the appropriate tags. Use this metric context to create any additional metrics. isInstrumentationEnabled() : returns true if instrumentation is enabled. switchMetricsContext(List Tag ? ) : switches the default metric context returned by getMetricContext() to a metric context containing the supplied tags. All default metrics will be reported to the new metric context. This method is useful when the state of a construct changes during the execution, and the user desires to reflect that in the emitted tags (for example, Kafka extractor can handle multiple topics in the same extractor, and we want to reflect this in the metrics). switchMetricContext(MetricContext) : similar to the above method, but uses the supplied metric context instead of generating a new metric context. It is the responsibility of the caller to ensure the new metric context has the correct tags and parent. The following method can be re-implemented by the user: * generateTags(State) : this method should return a list of tags to use for metric contexts created for this construct. If overriding this method, it is always a good idea to call super() and only append tags to this list.","title":"Instrumentable Interface"},{"location":"metrics/Metrics-for-Gobblin-ETL/#callback-methods","text":"Instrumented constructs have a set of callback methods that are called at different points in the processing of each record, and which can be used to update metrics. For example, the InstrumentedExtractor has the callbacks beforeRead() , afterRead(D, long) , and onException(Exception) . The javadoc for the instrumented constructs has further descriptions for each callback. Users should always call super() when overriding this callbacks, as default metrics depend on that.","title":"Callback Methods"},{"location":"metrics/Metrics-for-Gobblin-ETL/#custom-reporters","text":"Besides the reporters implemented by default (file, Kafka, and JMX), users can add custom reporters to the classpath and instruct Gobblin to use these reporters. To do this, users should extend the interface CustomReporterFactory , and specify a comma-separated list of CustomReporterFactory classes in the configuration key metrics.reporting.custom.builders . Gobblin will automatically search for these CustomReporterFactory implementations, instantiate each one with a parameter-less constructor, and then call the method newScheduledReporter(MetricContext, Properties) , where the properties contain all of the input configurations supplied to Gobblin. Gobblin will then manage this ScheduledReporter .","title":"Custom Reporters"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/","text":"Table of Contents Table of Contents Advantages of Migrating to Gobblin Kafka Ingestion Related Job Config Properties Config properties for pulling Kafka topics Config properties for compaction Deployment and Checkpoint Management Migrating from Camus to Gobblin in Production This page is a guide for Camus \u2192 Gobblin migration, intended for users and organizations currently using Camus. Camus is LinkedIn's previous-generation Kafka-HDFS pipeline. It is recommended that one read Kafka-HDFS Ingestion before reading this page. This page focuses on the Kafka-related configuration properties in Gobblin vs Camus. Advantages of Migrating to Gobblin Operability : Gobblin is a generic data ingestion pipeline that supports not only Kafka but several other data sources, and new data sources can be easily added. If you have multiple data sources, using a single tool to ingest data from these sources is a lot more pleasant operationally than deploying a separate tool for each source. Performance : The performance of Gobblin in MapReduce mode is comparable to Camus', and faster in some cases (e.g., the average record size of a Kafka topic is not proportional to the average time of pulling a topic) due to a better mapper load balancing algorithm. In the new continuous ingestion mode (currently under development), the performance of Gobblin will further improve. Metrics and Monitoring : Gobblin has a powerful end-to-end metrics collection and reporting module for monitoring purpose, making it much easier to spot problems in time and find the root causes. See the \"Gobblin Metrics\" section in the wiki and this post for more details. Features : In addition to the above, there are several other useful features for Kafka-HDFS ingestion in Gobblin that are not available in Camus, e.g., handling late events in data compaction ; dataset retention management; converter and quality checker; all-or-nothing job commit policy, etc. Also, Gobblin is under active development and new features are added frequently. Kafka Ingestion Related Job Config Properties This list contains Kafka-specific properties. For general configuration properties please refer to Configuration Properties Glossary . Config properties for pulling Kafka topics Gobblin Property Corresponding Camus Property Default value topic.whitelist kafka.whitelist.topics .* topic.blacklist kafka.blacklist.topics a^ mr.job.max.mappers mapred.map.tasks 100 kafka.brokers kafka.host.url (required) topics.move.to.latest.offset kafka.move.to.last.offset.list empty bootstrap.with.offset none latest reset.on.offset.out.of.range none nearest Remarks: topic.whitelist and topic.blacklist supports regex. topics.move.to.latest.offset: Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property is useful in Camus for moving a new topic to the latest offset, but in Gobblin it should rarely, if ever, be used, since you can use bootstrap.with.offset to achieve the same purpose more conveniently. bootstrap with offset: For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip. reset.on.offset.out.of.range: This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition). Config properties for compaction Gobblin compaction is comparable to Camus sweeper, which can deduplicate records in an input folder. Compaction is useful for Kafka-HDFS ingestion for two reasons: Although Gobblin guarantees no loss of data, in rare circumstances where data is published on HDFS but checkpoints failed to be persisted into the state store, it may pull the same records twice. If you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during replication. Below are the configuration properties related to compaction. For more information please visit the MapReduce Compaction section in the Compaction page. Gobblin Property Corresponding Camus Property Default value compaction.input.dir camus.sweeper.source.dir (required) compaction.dest.dir camus.sweeper.dest.dir (required) compaction.input.subdir camus.sweeper.source.dir hourly compaction.dest.subdir camus.sweeper.dest.dir daily compaction.tmp.dest.dir camus.sweeper.tmp.dir /tmp/gobblin-compaction compaction.whitelist camus.sweeper.whitelist .* compaction.blacklist camus.sweeper.blacklist a^ compaction.high.priority.topics none a^ compaction.normal.priority.topics none a^ compaction.input.deduplicated none false compaction.output.deduplicated none true compaction.file.system.uri none compaction.timebased.max.time.ago none 3d compaction.timebased.min.time.ago none 1d compaction.timebased.folder.pattern none YYYY/mm/dd compaction.thread.pool.size num.threads 20 compaction.max.num.reducers max.files 900 compaction.target.output.file.size camus.sweeper.target.file.size 268435456 compaction.mapred.min.split.size mapred.min.split.size 268435456 compaction.mapred.max.split.size mapred.max.split.size 268435456 compaction.mr.job.timeout.minutes none Remarks: The following properties support regex: compaction.whitelist, compaction.blacklist, compaction.high.priority.topics, compaction.normal.priority.topics compaction.input.dir is the parent folder of input topics, e.g., /data/kafka_topics, which contains topic folders such as /data/kafka_topics/Topic1, /data/kafka_topics/Topic2, etc. Note that Camus uses camus.sweeper.source.dir both as the input folder of Camus sweeper (i.e., compaction), and as the output folder for ingesting Kafka topics. In Gobblin, one should use data.publisher.final.dir as the output folder for ingesting Kafka topics. compaction.output.dir is the parent folder of output topics, e.g., /data/compacted_kafka_topics. compaction.input.subdir is the subdir name of output topics, if exists. For example, if the input topics are partitioned by hour, e.g., /data/kafka_topics/Topic1/hourly/2015/10/06/20, then compaction.input.subdir should be 'hourly'. compaction.output.subdir is the subdir name of output topics, if exists. For example, if you want to publish compacted data into day-partitioned folders, e.g., /data/compacted_kafka_topics/Topic1/daily/2015/10/06, then compaction.output.subdir should be 'daily'. There are 3 priority levels: high, normal, low. Topics not included in compaction.high.priority.topics or compaction.normal.priority.topics are considered low priority. compaction.input.deduplicated and compaction.output.deduplicated controls the behavior of the compaction regarding deduplication. Please see the Compaction page for more details. compaction.timebased.max.time.ago and compaction.timebased.min.time.ago controls the earliest and latest input folders to process, when using MRCompactorTimeBasedJobPropCreator . The format is ?m?d?h, e.g., 3m or 2d10h (m = month, not minute). For example, suppose compaction.timebased.max.time.ago=3d , compaction.timebased.min.time.ago=1d and the current time is 10/07 9am. Folders whose timestamps are before 10/04 9am, or folders whose timestamps are after 10/06 9am will not be processed. compaction.timebased.folder.pattern: time pattern in the folder path, when using MRCompactorTimeBasedJobPropCreator . This should come after compaction.input.subdir , e.g., if the input folder to a compaction job is /data/compacted_kafka_topics/Topic1/daily/2015/10/06 , this property should be YYYY/mm/dd . compaction.thread.pool.size: how many compaction MR jobs to run concurrently. compaction.max.num.reducers: max number of reducers for each compaction job compaction.target.output.file.size: This also controls the number of reducers. The number of reducers will be the smaller of compaction.max.num.reducers and input data size / compaction.target.output.file.size . compaction.mapred.min.split.size and compaction.mapred.max.split.size are used to control the number of mappers. Deployment and Checkpoint Management For deploying Gobblin in standalone or MapReduce mode, please see the Deployment page. Gobblin and Camus checkpoint management are similar in the sense that they both create checkpoint files in each run, and the next run will load the checkpoint files created by the previous run and start from there. Their difference is that Gobblin creates a single checkpoint file per job run or per dataset per job run, and provides two job commit policies: full and partial . In full mode, data are only commited for the job/dataset if all workunits of the job/dataset succeeded. Otherwise, the checkpoint of all workunits/datasets will be rolled back. Camus writes one checkpoint file per mapper, and only supports the partial mode. For Gobblin's state management, please refer to the Wiki page for more information. Migrating from Camus to Gobblin in Production If you are currently running in production, you can use the following steps to migrate to Gobblin: Deploy Gobblin based on the instructions in Deployment and Kafka-HDFS Ingestion , and set the properties mentioned in this page as well as other relevant properties in Configuration Glossary to the appropriate values. Whitelist the topics in Gobblin ingestion, and schedule Gobblin to run at your desired frequency. Once Gobblin starts running, blacklist these topics in Camus. If compaction is applicable to you, set up the compaction jobs based on instructions in Kafka-HDFS Ingestion and Compaction . Whitelist the topics you want to migrate in Gobblin and blacklist them in Camus.","title":"Camus to Gobblin Migration"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#table-of-contents","text":"Table of Contents Advantages of Migrating to Gobblin Kafka Ingestion Related Job Config Properties Config properties for pulling Kafka topics Config properties for compaction Deployment and Checkpoint Management Migrating from Camus to Gobblin in Production This page is a guide for Camus \u2192 Gobblin migration, intended for users and organizations currently using Camus. Camus is LinkedIn's previous-generation Kafka-HDFS pipeline. It is recommended that one read Kafka-HDFS Ingestion before reading this page. This page focuses on the Kafka-related configuration properties in Gobblin vs Camus.","title":"Table of Contents"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#advantages-of-migrating-to-gobblin","text":"Operability : Gobblin is a generic data ingestion pipeline that supports not only Kafka but several other data sources, and new data sources can be easily added. If you have multiple data sources, using a single tool to ingest data from these sources is a lot more pleasant operationally than deploying a separate tool for each source. Performance : The performance of Gobblin in MapReduce mode is comparable to Camus', and faster in some cases (e.g., the average record size of a Kafka topic is not proportional to the average time of pulling a topic) due to a better mapper load balancing algorithm. In the new continuous ingestion mode (currently under development), the performance of Gobblin will further improve. Metrics and Monitoring : Gobblin has a powerful end-to-end metrics collection and reporting module for monitoring purpose, making it much easier to spot problems in time and find the root causes. See the \"Gobblin Metrics\" section in the wiki and this post for more details. Features : In addition to the above, there are several other useful features for Kafka-HDFS ingestion in Gobblin that are not available in Camus, e.g., handling late events in data compaction ; dataset retention management; converter and quality checker; all-or-nothing job commit policy, etc. Also, Gobblin is under active development and new features are added frequently.","title":"Advantages of Migrating to Gobblin"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#kafka-ingestion-related-job-config-properties","text":"This list contains Kafka-specific properties. For general configuration properties please refer to Configuration Properties Glossary .","title":"Kafka Ingestion Related Job Config Properties"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#config-properties-for-pulling-kafka-topics","text":"Gobblin Property Corresponding Camus Property Default value topic.whitelist kafka.whitelist.topics .* topic.blacklist kafka.blacklist.topics a^ mr.job.max.mappers mapred.map.tasks 100 kafka.brokers kafka.host.url (required) topics.move.to.latest.offset kafka.move.to.last.offset.list empty bootstrap.with.offset none latest reset.on.offset.out.of.range none nearest Remarks: topic.whitelist and topic.blacklist supports regex. topics.move.to.latest.offset: Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property is useful in Camus for moving a new topic to the latest offset, but in Gobblin it should rarely, if ever, be used, since you can use bootstrap.with.offset to achieve the same purpose more conveniently. bootstrap with offset: For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip. reset.on.offset.out.of.range: This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition).","title":"Config properties for pulling Kafka topics"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#config-properties-for-compaction","text":"Gobblin compaction is comparable to Camus sweeper, which can deduplicate records in an input folder. Compaction is useful for Kafka-HDFS ingestion for two reasons: Although Gobblin guarantees no loss of data, in rare circumstances where data is published on HDFS but checkpoints failed to be persisted into the state store, it may pull the same records twice. If you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during replication. Below are the configuration properties related to compaction. For more information please visit the MapReduce Compaction section in the Compaction page. Gobblin Property Corresponding Camus Property Default value compaction.input.dir camus.sweeper.source.dir (required) compaction.dest.dir camus.sweeper.dest.dir (required) compaction.input.subdir camus.sweeper.source.dir hourly compaction.dest.subdir camus.sweeper.dest.dir daily compaction.tmp.dest.dir camus.sweeper.tmp.dir /tmp/gobblin-compaction compaction.whitelist camus.sweeper.whitelist .* compaction.blacklist camus.sweeper.blacklist a^ compaction.high.priority.topics none a^ compaction.normal.priority.topics none a^ compaction.input.deduplicated none false compaction.output.deduplicated none true compaction.file.system.uri none compaction.timebased.max.time.ago none 3d compaction.timebased.min.time.ago none 1d compaction.timebased.folder.pattern none YYYY/mm/dd compaction.thread.pool.size num.threads 20 compaction.max.num.reducers max.files 900 compaction.target.output.file.size camus.sweeper.target.file.size 268435456 compaction.mapred.min.split.size mapred.min.split.size 268435456 compaction.mapred.max.split.size mapred.max.split.size 268435456 compaction.mr.job.timeout.minutes none Remarks: The following properties support regex: compaction.whitelist, compaction.blacklist, compaction.high.priority.topics, compaction.normal.priority.topics compaction.input.dir is the parent folder of input topics, e.g., /data/kafka_topics, which contains topic folders such as /data/kafka_topics/Topic1, /data/kafka_topics/Topic2, etc. Note that Camus uses camus.sweeper.source.dir both as the input folder of Camus sweeper (i.e., compaction), and as the output folder for ingesting Kafka topics. In Gobblin, one should use data.publisher.final.dir as the output folder for ingesting Kafka topics. compaction.output.dir is the parent folder of output topics, e.g., /data/compacted_kafka_topics. compaction.input.subdir is the subdir name of output topics, if exists. For example, if the input topics are partitioned by hour, e.g., /data/kafka_topics/Topic1/hourly/2015/10/06/20, then compaction.input.subdir should be 'hourly'. compaction.output.subdir is the subdir name of output topics, if exists. For example, if you want to publish compacted data into day-partitioned folders, e.g., /data/compacted_kafka_topics/Topic1/daily/2015/10/06, then compaction.output.subdir should be 'daily'. There are 3 priority levels: high, normal, low. Topics not included in compaction.high.priority.topics or compaction.normal.priority.topics are considered low priority. compaction.input.deduplicated and compaction.output.deduplicated controls the behavior of the compaction regarding deduplication. Please see the Compaction page for more details. compaction.timebased.max.time.ago and compaction.timebased.min.time.ago controls the earliest and latest input folders to process, when using MRCompactorTimeBasedJobPropCreator . The format is ?m?d?h, e.g., 3m or 2d10h (m = month, not minute). For example, suppose compaction.timebased.max.time.ago=3d , compaction.timebased.min.time.ago=1d and the current time is 10/07 9am. Folders whose timestamps are before 10/04 9am, or folders whose timestamps are after 10/06 9am will not be processed. compaction.timebased.folder.pattern: time pattern in the folder path, when using MRCompactorTimeBasedJobPropCreator . This should come after compaction.input.subdir , e.g., if the input folder to a compaction job is /data/compacted_kafka_topics/Topic1/daily/2015/10/06 , this property should be YYYY/mm/dd . compaction.thread.pool.size: how many compaction MR jobs to run concurrently. compaction.max.num.reducers: max number of reducers for each compaction job compaction.target.output.file.size: This also controls the number of reducers. The number of reducers will be the smaller of compaction.max.num.reducers and input data size / compaction.target.output.file.size . compaction.mapred.min.split.size and compaction.mapred.max.split.size are used to control the number of mappers.","title":"Config properties for compaction"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#deployment-and-checkpoint-management","text":"For deploying Gobblin in standalone or MapReduce mode, please see the Deployment page. Gobblin and Camus checkpoint management are similar in the sense that they both create checkpoint files in each run, and the next run will load the checkpoint files created by the previous run and start from there. Their difference is that Gobblin creates a single checkpoint file per job run or per dataset per job run, and provides two job commit policies: full and partial . In full mode, data are only commited for the job/dataset if all workunits of the job/dataset succeeded. Otherwise, the checkpoint of all workunits/datasets will be rolled back. Camus writes one checkpoint file per mapper, and only supports the partial mode. For Gobblin's state management, please refer to the Wiki page for more information.","title":"Deployment and Checkpoint Management"},{"location":"miscellaneous/Camus-to-Gobblin-Migration/#migrating-from-camus-to-gobblin-in-production","text":"If you are currently running in production, you can use the following steps to migrate to Gobblin: Deploy Gobblin based on the instructions in Deployment and Kafka-HDFS Ingestion , and set the properties mentioned in this page as well as other relevant properties in Configuration Glossary to the appropriate values. Whitelist the topics in Gobblin ingestion, and schedule Gobblin to run at your desired frequency. Once Gobblin starts running, blacklist these topics in Camus. If compaction is applicable to you, set up the compaction jobs based on instructions in Kafka-HDFS Ingestion and Compaction . Whitelist the topics you want to migrate in Gobblin and blacklist them in Camus.","title":"Migrating from Camus to Gobblin in Production"},{"location":"miscellaneous/Exactly-Once-Support/","text":"Table of Contents Table of Contents Achieving Exactly-Once Delivery with CommitStepStore Scalability APIs This page outlines the design for exactly-once support in Gobblin. Currently the flow of publishing data in Gobblin is: DataWriter writes to staging folder DataWriter moves files from staging folder to task output folder Publisher moves files from task output folder to job output folder Persists checkpoints (watermarks) to state store Delete staging folder and task-output folder. This flow does not theoretically guarantee exactly-once delivery, rather, it guarantess at least once. Because if something bad happens in step 4, or between steps 3 and 4, it is possible that data is published but checkpoints are not, and the next run will re-extract and re-publish those records. To guarantee exactly-once, steps 3 4 should be atomic. Achieving Exactly-Once Delivery with CommitStepStore The idea is similar as write-head logging. Before doing the atomic steps (i.e., steps 3 4), first write all these steps (referred to as CommitStep s) into a CommitStepStore . In this way, if failure happens during the atomic steps, the next run can continue doing the rest of the steps before ingesting more data for this dataset. Example : Suppose we have a Kafka-HDFS ingestion job, where each Kafka topic is a dataset. Suppose a task generates three output files for topic 'MyTopic': task-output/MyTopic/2015-12-09/1.avro task-output/MyTopic/2015-12-09/2.avro task-output/MyTopic/2015-12-10/1.avro which should be published to job-output/MyTopic/2015-12-09/1.avro job-output/MyTopic/2015-12-09/2.avro job-output/MyTopic/2015-12-10/1.avro And suppose this topic has two partitions, and the their checkpoints, i.e., the actual high watermarks are offset=100 and offset=200 . In this case, there will be 5 CommitSteps for this dataset: FsRenameCommitStep : rename task-output/MyTopic/2015-12-09/1.avro to job-output/MyTopic/2015-12-09/1.avro FsRenameCommitStep : rename task-output/MyTopic/2015-12-09/2.avro to job-output/MyTopic/2015-12-09/2.avro FsRenameCommitStep : rename task-output/MyTopic/2015-12-10/1.avro to job-output/MyTopic/2015-12-10/1.avro HighWatermarkCommitStep : set the high watermark for partition MyTopic:0 = 100 HighWatermarkCommtiStep : set the high watermark for partition MyTopic:1 = 200 If all these CommitStep s are successful, we can proceed with deleting task-output folder and deleting the above CommitStep s from the CommitStepStore . If any of these steps fails, these steps will not be deleted. When the next run starts, for each dataset, it will check whether there are CommitStep s for this dataset in the CommitStepStore. If there are, it means the previous run may not have successfully executed some of these steps, so it will verify whether each step has been done, and re-do the step if not. If the re-do fails for a certain number of times, this dataset will be skipped. Thus the CommitStep interface will have two methods: verify() and execute() . Scalability The above approach potentially affects scalability for two reasons: The driver needs to write all CommitStep s to the CommitStepStore for each dataset, once it determines that all tasks for the dataset have finished. This may cause scalability issues if there are too many CommitStep s, too many datasets, or too many tasks. Upon the start of the next run, the driver needs to verify all CommitStep s and redo the CommitStep s that the previous run failed to do. This may also cause scalability issues if there are too many CommitStep s. Both issues can be resolved by moving the majority of the work to containers, rather than doing it in the driver. For #1, we can make each container responsible for writing CommitStep s for a subset of the datasets. Each container will keep polling the TaskStateStore to determine whether all tasks for each dataset that it is responsible for have finished, and if so, it writes CommitStep s for this dataset to the CommitStepStore . #2 can also easily be parallelized where we have each container responsible for a subset of datasets. APIs CommitStep : /** * A step during committing in a Gobblin job that should be atomically executed with other steps. */ public abstract class CommitStep { private static final Gson GSON = new Gson(); public static abstract class Builder T extends Builder ? { } protected CommitStep(Builder ? builder) { } /** * Verify whether the CommitStep has been done. */ public abstract boolean verify() throws IOException; /** * Execute a CommitStep. */ public abstract boolean execute() throws IOException; public static CommitStep get(String json, Class ? extends CommitStep clazz) throws IOException { return GSON.fromJson(json, clazz); } } CommitSequence : @Slf4j public class CommitSequence { private final String storeName; private final String datasetUrn; private final List CommitStep steps; private final CommitStepStore commitStepStore; public CommitSequence(String storeName, String datasetUrn, List CommitStep steps, CommitStepStore commitStepStore) { this.storeName = storeName; this.datasetUrn = datasetUrn; this.steps = steps; this.commitStepStore = commitStepStore; } public boolean commit() { try { for (CommitStep step : this.steps) { if (!step.verify()) { step.execute(); } } this.commitStepStore.remove(this.storeName, this.datasetUrn); return true; } catch (Throwable t) { log.error( Commit failed for dataset + this.datasetUrn, t); return false; } } } CommitStepStore : /** * A store for {@link CommitStep}s. */ public interface CommitStepStore { /** * Create a store with the given name. */ public boolean create(String storeName) throws IOException; /** * Create a new dataset URN in a store. */ public boolean create(String storeName, String datasetUrn) throws IOException; /** * Whether a dataset URN exists in a store. */ public boolean exists(String storeName, String datasetUrn) throws IOException; /** * Remove a given store. */ public boolean remove(String storeName) throws IOException; /** * Remove all {@link CommitStep}s for the given dataset URN from the store. */ public boolean remove(String storeName, String datasetUrn) throws IOException; /** * Put a {@link CommitStep} with the given dataset URN into the store. */ public boolean put(String storeName, String datasetUrn, CommitStep step) throws IOException; /** * Get the {@link CommitSequence} associated with the given dataset URN in the store. */ public CommitSequence getCommitSequence(String storeName, String datasetUrn) throws IOException; }","title":"Exactly Once Support"},{"location":"miscellaneous/Exactly-Once-Support/#table-of-contents","text":"Table of Contents Achieving Exactly-Once Delivery with CommitStepStore Scalability APIs This page outlines the design for exactly-once support in Gobblin. Currently the flow of publishing data in Gobblin is: DataWriter writes to staging folder DataWriter moves files from staging folder to task output folder Publisher moves files from task output folder to job output folder Persists checkpoints (watermarks) to state store Delete staging folder and task-output folder. This flow does not theoretically guarantee exactly-once delivery, rather, it guarantess at least once. Because if something bad happens in step 4, or between steps 3 and 4, it is possible that data is published but checkpoints are not, and the next run will re-extract and re-publish those records. To guarantee exactly-once, steps 3 4 should be atomic.","title":"Table of Contents"},{"location":"miscellaneous/Exactly-Once-Support/#achieving-exactly-once-delivery-with-commitstepstore","text":"The idea is similar as write-head logging. Before doing the atomic steps (i.e., steps 3 4), first write all these steps (referred to as CommitStep s) into a CommitStepStore . In this way, if failure happens during the atomic steps, the next run can continue doing the rest of the steps before ingesting more data for this dataset. Example : Suppose we have a Kafka-HDFS ingestion job, where each Kafka topic is a dataset. Suppose a task generates three output files for topic 'MyTopic': task-output/MyTopic/2015-12-09/1.avro task-output/MyTopic/2015-12-09/2.avro task-output/MyTopic/2015-12-10/1.avro which should be published to job-output/MyTopic/2015-12-09/1.avro job-output/MyTopic/2015-12-09/2.avro job-output/MyTopic/2015-12-10/1.avro And suppose this topic has two partitions, and the their checkpoints, i.e., the actual high watermarks are offset=100 and offset=200 . In this case, there will be 5 CommitSteps for this dataset: FsRenameCommitStep : rename task-output/MyTopic/2015-12-09/1.avro to job-output/MyTopic/2015-12-09/1.avro FsRenameCommitStep : rename task-output/MyTopic/2015-12-09/2.avro to job-output/MyTopic/2015-12-09/2.avro FsRenameCommitStep : rename task-output/MyTopic/2015-12-10/1.avro to job-output/MyTopic/2015-12-10/1.avro HighWatermarkCommitStep : set the high watermark for partition MyTopic:0 = 100 HighWatermarkCommtiStep : set the high watermark for partition MyTopic:1 = 200 If all these CommitStep s are successful, we can proceed with deleting task-output folder and deleting the above CommitStep s from the CommitStepStore . If any of these steps fails, these steps will not be deleted. When the next run starts, for each dataset, it will check whether there are CommitStep s for this dataset in the CommitStepStore. If there are, it means the previous run may not have successfully executed some of these steps, so it will verify whether each step has been done, and re-do the step if not. If the re-do fails for a certain number of times, this dataset will be skipped. Thus the CommitStep interface will have two methods: verify() and execute() .","title":"Achieving Exactly-Once Delivery with CommitStepStore"},{"location":"miscellaneous/Exactly-Once-Support/#scalability","text":"The above approach potentially affects scalability for two reasons: The driver needs to write all CommitStep s to the CommitStepStore for each dataset, once it determines that all tasks for the dataset have finished. This may cause scalability issues if there are too many CommitStep s, too many datasets, or too many tasks. Upon the start of the next run, the driver needs to verify all CommitStep s and redo the CommitStep s that the previous run failed to do. This may also cause scalability issues if there are too many CommitStep s. Both issues can be resolved by moving the majority of the work to containers, rather than doing it in the driver. For #1, we can make each container responsible for writing CommitStep s for a subset of the datasets. Each container will keep polling the TaskStateStore to determine whether all tasks for each dataset that it is responsible for have finished, and if so, it writes CommitStep s for this dataset to the CommitStepStore . #2 can also easily be parallelized where we have each container responsible for a subset of datasets.","title":"Scalability"},{"location":"miscellaneous/Exactly-Once-Support/#apis","text":"CommitStep : /** * A step during committing in a Gobblin job that should be atomically executed with other steps. */ public abstract class CommitStep { private static final Gson GSON = new Gson(); public static abstract class Builder T extends Builder ? { } protected CommitStep(Builder ? builder) { } /** * Verify whether the CommitStep has been done. */ public abstract boolean verify() throws IOException; /** * Execute a CommitStep. */ public abstract boolean execute() throws IOException; public static CommitStep get(String json, Class ? extends CommitStep clazz) throws IOException { return GSON.fromJson(json, clazz); } } CommitSequence : @Slf4j public class CommitSequence { private final String storeName; private final String datasetUrn; private final List CommitStep steps; private final CommitStepStore commitStepStore; public CommitSequence(String storeName, String datasetUrn, List CommitStep steps, CommitStepStore commitStepStore) { this.storeName = storeName; this.datasetUrn = datasetUrn; this.steps = steps; this.commitStepStore = commitStepStore; } public boolean commit() { try { for (CommitStep step : this.steps) { if (!step.verify()) { step.execute(); } } this.commitStepStore.remove(this.storeName, this.datasetUrn); return true; } catch (Throwable t) { log.error( Commit failed for dataset + this.datasetUrn, t); return false; } } } CommitStepStore : /** * A store for {@link CommitStep}s. */ public interface CommitStepStore { /** * Create a store with the given name. */ public boolean create(String storeName) throws IOException; /** * Create a new dataset URN in a store. */ public boolean create(String storeName, String datasetUrn) throws IOException; /** * Whether a dataset URN exists in a store. */ public boolean exists(String storeName, String datasetUrn) throws IOException; /** * Remove a given store. */ public boolean remove(String storeName) throws IOException; /** * Remove all {@link CommitStep}s for the given dataset URN from the store. */ public boolean remove(String storeName, String datasetUrn) throws IOException; /** * Put a {@link CommitStep} with the given dataset URN into the store. */ public boolean put(String storeName, String datasetUrn, CommitStep step) throws IOException; /** * Get the {@link CommitSequence} associated with the given dataset URN in the store. */ public CommitSequence getCommitSequence(String storeName, String datasetUrn) throws IOException; }","title":"APIs"},{"location":"project/Feature-List/","text":"Currently, Gobblin supports the following feature list: Different Data Sources Source Type Protocol Vendors RDMS JDBC MySQL/SQLServer Files HDFS/SFTP/LocalFS N/A Salesforce REST Salesforce Different Pulling Types SNAPSHOT-ONLY: Pull the snapshot of one dataset. SNAPSHOT-APPEND: Pull delta changes since last run, optionally merge delta changes into snapshot (Delta changes include updates to the dataset since last run). APPEND-ONLY: Pull delta changes since last run, and append to dataset. Different Deployment Types standalone deploy on a single machine cluster deploy on hadoop 2.3.0 Compaction Merge delta changes into snapshot.","title":"Feature List"},{"location":"project/Feature-List/#different-data-sources","text":"Source Type Protocol Vendors RDMS JDBC MySQL/SQLServer Files HDFS/SFTP/LocalFS N/A Salesforce REST Salesforce Different Pulling Types SNAPSHOT-ONLY: Pull the snapshot of one dataset. SNAPSHOT-APPEND: Pull delta changes since last run, optionally merge delta changes into snapshot (Delta changes include updates to the dataset since last run). APPEND-ONLY: Pull delta changes since last run, and append to dataset. Different Deployment Types standalone deploy on a single machine cluster deploy on hadoop 2.3.0 Compaction Merge delta changes into snapshot.","title":"Different Data Sources"},{"location":"project/Posts/","text":"Gobblin Metrics: next generation instrumentation for applications","title":"Posts"},{"location":"project/Talks-and-Tech-Blogs/","text":"Gobblin Talks and Tech Blogs QCon presentation | Presentation | (Nov 5th, 2014) Engineering Blog Post | Presentation | (Nov 25th, 2014) Bigger, Faster, Easier: Building a Real-Time Self Service Data Analytics Ecosystem at LinkedIn | Presentation | (Hadoop Summit 2015) Gobblin: Unifying Data Ingestion for Hadoop | Presentation | (VLDB 2015) Gobblin: Unifying Data Ingestion for Hadoop | Presentation | (VLDB 2015 slides) Ingestion from Kafka using Gobblin | Presentation | (Gobblin Meetup November 2015) Gobblin on Yarn: A Preview | Presentation | (Gobblin Meetup November 2015) Gobblin@NerdWallet External Use Case 1 | Presentation | (Gobblin Meetup November 2015) Gobblin@Intel External Use Case 2 | Presentation | (Gobblin Meetup November 2015) Gobblin: Beyond ingest to big data management | Video | Presentation | (Gobblin Meetup June 2016) Gobblin: Inter cluster replication | Video | Presentation | (Gobblin Meetup June 2016) Gobblin: Configuration and Orchestration | Video | Presentation | (Gobblin Meetup June 2016) Gobblin on AWS | Video | Presentation | (Gobblin Meetup June 2016)","title":"Talks and Tech Blog Posts"},{"location":"sinks/AvroHdfsDataWriter/","text":"Description Writes Avro records to Avro data files on Hadoop file systems. Usage writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder writer.destination.type=HDFS For more info, see AvroHdfsDataWriter Configuration Key Type Description Default Value writer.codec.type One of null,deflate,snappy,bzip2,xz Type of the compression codec deflate writer.deflate.level 1-9 The compression level for the \"deflate\" codec 9","title":"Avro HDFS"},{"location":"sinks/AvroHdfsDataWriter/#description","text":"Writes Avro records to Avro data files on Hadoop file systems.","title":"Description"},{"location":"sinks/AvroHdfsDataWriter/#usage","text":"writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder writer.destination.type=HDFS For more info, see AvroHdfsDataWriter","title":"Usage"},{"location":"sinks/AvroHdfsDataWriter/#configuration","text":"Key Type Description Default Value writer.codec.type One of null,deflate,snappy,bzip2,xz Type of the compression codec deflate writer.deflate.level 1-9 The compression level for the \"deflate\" codec 9","title":"Configuration"},{"location":"sinks/ConsoleWriter/","text":"Description A simple implementation that writes records to Stdout. Usage writer.builder.class=org.apache.gobblin.writer.ConsoleWriterBuilder","title":"Console"},{"location":"sinks/ConsoleWriter/#description","text":"A simple implementation that writes records to Stdout.","title":"Description"},{"location":"sinks/ConsoleWriter/#usage","text":"writer.builder.class=org.apache.gobblin.writer.ConsoleWriterBuilder","title":"Usage"},{"location":"sinks/CouchbaseWriter/","text":"Introduction Record format Configuration General configuration values Authentication No credentials Using certificates Using bucket password Document level expiration 1 - Expiration from write time 2 - Expiration from an origin timestamp Introduction The CouchbaseWriter supports writing documents to a Couchbase bucket though the Couchbase Java SDK . Note that CouchbaseWriter only supports writing to a single bucket as there should be only 1 CouchbaseEnvironment per JVM. Record format Couchbase writer currently support AVRO and JSON as data inputs. On both of them it requires the following structured schema: Document field Description key Unique key used to store the document on the bucket. For more info view Couchbase docs data.data Object or value containing the information associated with the key for this document data.flags Couchbase flags To store JSON on data.data use 0x02 24 for UTF-8 0x04 24 . The following is a sample input record with JSON data { key : myKey123 , data : { data : { field1 : field1Value , field2 : 123 }, flags : 33554432 } } or to store plain text: { key : myKey123 , data : { data : singleValueData , flags : 67108864 } } If using AVRO, use the following schema: { type : record , name : topLevelRecord , fields : [ { name : key , type : string }, { name : data , type : { type : record , name : data , namespace : topLevelRecord , fields : [ { name : data , type : [ bytes , null ] }, { name : flags , type : int } ] } } ] } Note that the key can be other than string if needed. Configuration General configuration values Configuration Key Default Value Description writer.couchbase.bucket Optional Name of the couchbase bucket. Change if using other than default bucket writer.couchbase.default \"default\" Name of the default bucket if writer.couchbase.bucket is not provided writer.couchbase.dnsSrvEnabled \"false\" Enable DNS SRV bootstrapping docs writer.couchbase.bootstrapServers | localhost | URL to bootstrap servers. If using DNS SRV set writer.couchbase.dnsSrvEnabled` to true writer.couchbase.sslEnabled false Use SSL to connect to couchbase writer.couchbase.password Optional Bucket password. Will be ignored if writer.couchbase.certAuthEnabled is true writer.couchbase.certAuthEnabled false Set to true if using certificate authentication. Must also specify writer.couchbase.sslKeystoreFile , writer.couchbase.sslKeystorePassword , writer.couchbase.sslTruststoreFile , and writer.couchbase.sslTruststorePassword writer.couchbase.sslKeystoreFile Optional Path to the keystore file location writer.couchbase.sslKeystorePassword Optional Keystore password writer.couchbase.sslTruststoreFile Optional Path to the trustStore file location writer.couchbase.sslTruststorePassword Optional TrustStore password writer.couchbase.documentTTL 0 Time To Live of each document. Units are specified in writer.couchbase.documentTTLOriginField writer.couchbase.documentTTLUnits SECONDS Unit for writer.couchbase.documentTTL . Must be one of java.util.concurrent.TimeUnit . Case insensitive writer.couchbase.documentTTLOriginField Optional Time To Live of each document. Units are specified in writer.couchbase.documentTTLOriginField writer.couchbase.documentTTLOriginUnits MILLISECONDS Unit for writer.couchbase.documentTTL . Must be one of java.util.concurrent.TimeUnit . Case insensitive. As an example a writer.couchbase.documentTTLOriginField value of 1568240399000 and writer.couchbase.documentTTLOriginUnits value of MILLISECONDS timeunit would be Wed Sep 11 15:19:59 PDT 2019 writer.couchbase.retriesEnabled false Enable write retries on failures writer.couchbase.maxRetries 5 Maximum number of retries writer.couchbase.failureAllowancePercentage 0.0 The percentage of failures that you are willing to tolerate while writing to Couchbase. Gobblin will mark the workunit successful and move on if there are failures but not enough to trip the failure threshold. Only successfully acknowledged writes are counted as successful, all others are considered as failures. The default for the failureAllowancePercentage is set to 0.0. For example, if the value is set to 0.2 This means that as long as 80% of the data is acknowledged by Couchbase, Gobblin will move on. If you want higher guarantees, set this config value to a lower value. e.g. If you want 99% delivery guarantees, set this value to 0.01 operationTimeoutMillis 10000 Global timeout for couchbase communication operations Authentication No credentials NOT RECOMMENDED FOR PRODUCTION. Do not set writer.couchbase.certAuthEnabled nor writer.couchbase.password Using certificates Set writer.couchbase.certAuthEnabled to true and values for writer.couchbase.sslKeystoreFile , writer.couchbase.sslKeystorePassword , writer.couchbase.sslTruststoreFile , and writer.couchbase.sslTruststorePassword . writer.couchbase.password setting will be ignored if writer.couchbase.certAuthEnabled is set Using bucket password Set writer.couchbase.password Document level expiration Couchbase writer allows to set expiration at the document level using the expiry property of the couchbase document. PLease note that current couchbase implementation using timestamps limits it to January 19, 2038 03:14:07 GM given the type of expiry is set to int. CouchbaseWriter only works with global timestamps and does not use relative expiration in seconds ( 30 days) for simplicity. Currently three modes are supported: 1 - Expiration from write time Define only writer.couchbase.documentTTL and writer.couchbase.documentTTLUnits . For example for a 2 days expiration configs would look like: Configuration Key Value writer.couchbase.documentTTL 2 writer.couchbase.documentTTLUnits DAYS 2 - Expiration from an origin timestamp Define only writer.couchbase.documentTTL and writer.couchbase.documentTTLUnits . For example for a 2 days expiration configs using the header.time field that has timestamp in MILLISECONDS would look like: Configuration Key Value writer.couchbase.documentTTL 2 writer.couchbase.documentTTLUnits \"DAYS\" writer.couchbase.documentTTLOriginField \"header.time\" writer.couchbase.documentTTLOriginUnits 1568240399000 So a sample document with origin on 1568240399 (Wed Sep 11 15:19:59 PDT 2019) would expire on 1568413199 (Fri Sep 13 15:19:59 PDT 2019). The following is a sample record format. { key : sampleKey , data : { data : { field1 : field1Value , header : { time : 1568240399000 } }, flags : 33554432 } } }","title":"Couchbase"},{"location":"sinks/CouchbaseWriter/#introduction","text":"The CouchbaseWriter supports writing documents to a Couchbase bucket though the Couchbase Java SDK . Note that CouchbaseWriter only supports writing to a single bucket as there should be only 1 CouchbaseEnvironment per JVM.","title":"Introduction"},{"location":"sinks/CouchbaseWriter/#record-format","text":"Couchbase writer currently support AVRO and JSON as data inputs. On both of them it requires the following structured schema: Document field Description key Unique key used to store the document on the bucket. For more info view Couchbase docs data.data Object or value containing the information associated with the key for this document data.flags Couchbase flags To store JSON on data.data use 0x02 24 for UTF-8 0x04 24 . The following is a sample input record with JSON data { key : myKey123 , data : { data : { field1 : field1Value , field2 : 123 }, flags : 33554432 } } or to store plain text: { key : myKey123 , data : { data : singleValueData , flags : 67108864 } } If using AVRO, use the following schema: { type : record , name : topLevelRecord , fields : [ { name : key , type : string }, { name : data , type : { type : record , name : data , namespace : topLevelRecord , fields : [ { name : data , type : [ bytes , null ] }, { name : flags , type : int } ] } } ] } Note that the key can be other than string if needed.","title":"Record format"},{"location":"sinks/CouchbaseWriter/#configuration","text":"","title":"Configuration"},{"location":"sinks/CouchbaseWriter/#general-configuration-values","text":"Configuration Key Default Value Description writer.couchbase.bucket Optional Name of the couchbase bucket. Change if using other than default bucket writer.couchbase.default \"default\" Name of the default bucket if writer.couchbase.bucket is not provided writer.couchbase.dnsSrvEnabled \"false\" Enable DNS SRV bootstrapping docs writer.couchbase.bootstrapServers | localhost | URL to bootstrap servers. If using DNS SRV set writer.couchbase.dnsSrvEnabled` to true writer.couchbase.sslEnabled false Use SSL to connect to couchbase writer.couchbase.password Optional Bucket password. Will be ignored if writer.couchbase.certAuthEnabled is true writer.couchbase.certAuthEnabled false Set to true if using certificate authentication. Must also specify writer.couchbase.sslKeystoreFile , writer.couchbase.sslKeystorePassword , writer.couchbase.sslTruststoreFile , and writer.couchbase.sslTruststorePassword writer.couchbase.sslKeystoreFile Optional Path to the keystore file location writer.couchbase.sslKeystorePassword Optional Keystore password writer.couchbase.sslTruststoreFile Optional Path to the trustStore file location writer.couchbase.sslTruststorePassword Optional TrustStore password writer.couchbase.documentTTL 0 Time To Live of each document. Units are specified in writer.couchbase.documentTTLOriginField writer.couchbase.documentTTLUnits SECONDS Unit for writer.couchbase.documentTTL . Must be one of java.util.concurrent.TimeUnit . Case insensitive writer.couchbase.documentTTLOriginField Optional Time To Live of each document. Units are specified in writer.couchbase.documentTTLOriginField writer.couchbase.documentTTLOriginUnits MILLISECONDS Unit for writer.couchbase.documentTTL . Must be one of java.util.concurrent.TimeUnit . Case insensitive. As an example a writer.couchbase.documentTTLOriginField value of 1568240399000 and writer.couchbase.documentTTLOriginUnits value of MILLISECONDS timeunit would be Wed Sep 11 15:19:59 PDT 2019 writer.couchbase.retriesEnabled false Enable write retries on failures writer.couchbase.maxRetries 5 Maximum number of retries writer.couchbase.failureAllowancePercentage 0.0 The percentage of failures that you are willing to tolerate while writing to Couchbase. Gobblin will mark the workunit successful and move on if there are failures but not enough to trip the failure threshold. Only successfully acknowledged writes are counted as successful, all others are considered as failures. The default for the failureAllowancePercentage is set to 0.0. For example, if the value is set to 0.2 This means that as long as 80% of the data is acknowledged by Couchbase, Gobblin will move on. If you want higher guarantees, set this config value to a lower value. e.g. If you want 99% delivery guarantees, set this value to 0.01 operationTimeoutMillis 10000 Global timeout for couchbase communication operations","title":"General configuration values"},{"location":"sinks/CouchbaseWriter/#authentication","text":"","title":"Authentication"},{"location":"sinks/CouchbaseWriter/#no-credentials","text":"NOT RECOMMENDED FOR PRODUCTION. Do not set writer.couchbase.certAuthEnabled nor writer.couchbase.password","title":"No credentials"},{"location":"sinks/CouchbaseWriter/#using-certificates","text":"Set writer.couchbase.certAuthEnabled to true and values for writer.couchbase.sslKeystoreFile , writer.couchbase.sslKeystorePassword , writer.couchbase.sslTruststoreFile , and writer.couchbase.sslTruststorePassword . writer.couchbase.password setting will be ignored if writer.couchbase.certAuthEnabled is set","title":"Using certificates"},{"location":"sinks/CouchbaseWriter/#using-bucket-password","text":"Set writer.couchbase.password","title":"Using bucket password"},{"location":"sinks/CouchbaseWriter/#document-level-expiration","text":"Couchbase writer allows to set expiration at the document level using the expiry property of the couchbase document. PLease note that current couchbase implementation using timestamps limits it to January 19, 2038 03:14:07 GM given the type of expiry is set to int. CouchbaseWriter only works with global timestamps and does not use relative expiration in seconds ( 30 days) for simplicity. Currently three modes are supported:","title":"Document level expiration"},{"location":"sinks/CouchbaseWriter/#1-expiration-from-write-time","text":"Define only writer.couchbase.documentTTL and writer.couchbase.documentTTLUnits . For example for a 2 days expiration configs would look like: Configuration Key Value writer.couchbase.documentTTL 2 writer.couchbase.documentTTLUnits DAYS","title":"1 - Expiration from write time"},{"location":"sinks/CouchbaseWriter/#2-expiration-from-an-origin-timestamp","text":"Define only writer.couchbase.documentTTL and writer.couchbase.documentTTLUnits . For example for a 2 days expiration configs using the header.time field that has timestamp in MILLISECONDS would look like: Configuration Key Value writer.couchbase.documentTTL 2 writer.couchbase.documentTTLUnits \"DAYS\" writer.couchbase.documentTTLOriginField \"header.time\" writer.couchbase.documentTTLOriginUnits 1568240399000 So a sample document with origin on 1568240399 (Wed Sep 11 15:19:59 PDT 2019) would expire on 1568413199 (Fri Sep 13 15:19:59 PDT 2019). The following is a sample record format. { key : sampleKey , data : { data : { field1 : field1Value , header : { time : 1568240399000 } }, flags : 33554432 } } }","title":"2 - Expiration from an origin timestamp"},{"location":"sinks/Gobblin-JDBC-Writer/","text":"Table of Contents Table of Contents Gobblin JDBC writer publisher Proposed design Requirements Design summary Design detail WriterInitializer JdbcWriterInitializer AvroFieldsPickConverter AvroToJdbcEntryConverter JdbcWriter Skipping staging table JDBC Publisher Concrete implementations MySQL Writer Teradata Writer Performance and Scalability Important note Gobblin JDBC writer publisher Gobblin is a general data ingestion framework that can extract, convert, and publish data. Currently publishing into JDBC compatible RDBMS is not in Gobblin and here we are introducing JDBC writer (and publisher) so that Gobblin can easily write into JDBC compatible RDBMS by using new JDBC writer at the same time reusing existing extraction, and conversion. Proposed design Requirements User can choose to replace the destination table. User can pass the staging table, for the case user does not have permission to create table. If user chose to use their own staging table, user can also choose to truncate the staging table. User should be able to skip staging table for performance reason. User can choose level of parallelism. For Avro \u2192 JDBC use case, user should be able to cherry pick the fields to be copied. Design summary New JdbcWriter, JdbcPublisher will be introduced, along with AvroFieldsPickConverter and AvroToJdbcEntry. Figure 1 shows Gobblin general flow and figure 2 shows the specific flow; Avro \u2192 RDBMS through JDBC. JdbcWriter will use staging table mainly for failure handling. Since Gobblin breaks down the job into multiple task and process it in parallel, each writer will hold transaction and there will be more than one transaction against the database. If there\u2019s no staging table and writer writes directly to destination table, partial failure among the writers may result partial data push into destination table. Having partial data is mostly bad for consumption and could also result subsequent failure of next data load due to data inconsistency. Also, having partial data also makes it hard to retry on failure. Thus, Gobblin will use staging table so that writer writes into staging table and publisher can copy data from staging table into destination table in one single transaction. Having one transaction from the publisher level enables Gobblin to revert back to original on failure and make it able to be retried from previous state. For performance reason, and from the requirement, user may skip staging. This comes with the cost of giving up failure handling and for this case Gobblin does not guarantee recovery from failure. Will introduce WriterInitializer for the case that writer needs an initialization but needs to be done before going on parallel processing. (more on below) - figure 1. Gobblin general flow - - figure 2. Gobblin Avro \u2192 RDBMS through JDBC specific flow - AvroFieldsPickConverter will cherry pick the columns that user wants. AvroToJdbcEntryConverter will convert Avro to JDBC entry. JdbcWriter will write JdbcEntry to staging table. (user can skip staging which is addressed below in JDBC Writer/Publisher section). JdbcPublisher will write into destination table. Design detail WriterInitializer Note that this is a new interface where its responsibility is to initialize writer, which means it\u2019s tied to writer\u2019s implementation, and clean what it initialized. Reasons of introducing writer initializer: The main reason for this initializer is to perform initialization but not in parallel environment. As writer subjects to run in parallel, certain task that needs to be done only once across all writers is hard to be done. For example, if user chose to skip staging table and also chose to replace destination table, the initialization task would be truncating destination table which needs to be done only once before all writers start writing. This is simply hard to be done in parallel environment and better to avoid trying it for simplicity. Another reason for writer initializer is to simplify the cleanup process. As writer initializer initializes things, it also knows what to clean up. Instead of having other code to figure out what to clean up on which branch and condition all over again, closing writer initializer at the end of the job will just simply clean it up.(This pattern is widely used in JDK(e.g: Stream), where many classes implements interface Closeable). This clean up can be done in writer, but writer is currently being closed in task level, where there\u2019s a case that rest of the flow still needs it. (e.g: staging table should not be cleaned until publish is completed.) Currently, Gobblin has a logic to clean up the the staging data in JobLauncherUtils which is specific to FsDataWriter, where AbstractJobLauncher has the logic to figure out what kind of clean up method of JobLauncherUtils needs to be called. Ideally, we will hide this implementation of clean up behind the interface of WriterInitializer. Figure 3 shows interface of WriterInitializer. WriterInitializer will be extensible in Gobblin via factory method pattern where any writer can plugin their initialization code if needed. - figure 3. Class diagram for WriterInitializer interface - JdbcWriterInitializer JdbcWriterInitializer will be the first class implements WriterInitializer interface. This will be instantiated by AbstractJobLauncher after Source creates WorkUnit and always get closed by AbstractJobLauncher when job is finished regardless fail or success. By default, JdbcWriterInitializer will create staging tables based on the structure of the target table. Therefore it's necessary to create the target table first. Staging tables are created per Workunit, in which there can be more than one staging table. Having multiple staging table will make parallelism easier for publisher when moving data from staging table to destination. Any table created by JdbcWriterInitializer will be remembered and later will be dropped when it\u2019s being closed. Staging will be placed in same destination host, same database, some temporary table name, with same structure. The main purpose of staging table for handling failure. Without staging table, it is hard to recover from failure, because writers writes into table in multiple transactions. Additionally, staging table also brings data integrity check before publishing into destination. Before creating the staging table, JdbcWriterInitializer will validate if user has drop table privilege to make sure it can drop it later on. User can choose to use their own staging table. This is to support the use case when user does not have privilege to create table. When user chooses to use their own staging table, JdbcWriterInitializer will truncate the table later when it\u2019s being closed. Staging table initially should be always empty. User who chose to use user\u2019s own staging table can also choose to truncate staging table. If staging table is not empty and user does not choose to truncate table, JdbcWriterInitializer will make the job fail. If user chose to skip staging table and replace the output, JdbcWriterInitializer will truncate destination table. This is because destination table needs to be emptied prior going into parallel processing as more than one writer will start writing simultaneously. Figure 4 shows overall flow of JdbcWriterInitializer. - figure 4. JdbcWriterInitializer flow - AvroFieldsPickConverter User can define which fields to be copied. Given input from the user, it will narrow down the number of columns by updating schema and data. AvroToJdbcEntryConverter Will convert avro schema and data into JdbcEntrySchema and JdbcEntryData. JdbcEntrySchema consists of pairs of column name and JDBCTypes, and JdbcEntry consists of pairs of column names and object where object can be directly used against PreparedStatement.setObject(). Although Avro schema can be a recursive structure by having record in the record, RDBMS table structure is not a recursive data structure. Thus, AvroToJdbcEntryConverter does not accept Avro schema that has record type inside record type. Both JdbcEntrySchema and JdbcEntry will be case sensitive because Avro field name is case sensitive, and many widely used RDBMS are case sensitive on column name as well. In case there\u2019s a mismatch on column names, AvroToJdbcEntryConverter will take column name mapping between Avro field name and Jdbc column name. JdbcWriter Uses JDBC to persist data into staging table in task level. By default it will persist into staging area (and will be put into final destination by publisher). Staging table is already ready by WriterInitializer. Input column names should exactly match Jdbc column names. User can convert the name using AvroToJdbcEntryConverter. Schema evolution: The number of input columns is expected to be equal or smaller than the number of columns in Jdbc. This is to prevent unintended outcome from schema evolution such as additional column. As underlying Jdbc RDBMS can declare constraints on its schema, writer will allow if number of columns in Jdbc is greater than number of input columns. number of input columns = number of columns in Jdbc Each writer will open one transaction. Having one transaction per writer has its tradeoffs: Pro: Simple on failure handling as you can just simply execute rollback on failure. Basically, it will revert back to previous state so that the job can retry the task. Con: It can lead up to long lived transaction and it can face scalability issue. (Not enough disk space for transaction log, number of record limit on one transaction (2.1B for Postgre sql), etc) JdbcWriter will go with one transaction per writer for it\u2019s simplicity on failure handling. Scalability issue with long transaction can be overcome by increasing partition which makes transaction short. During the design meeting, we\u2019ve discussed that long transaction could be a problem. One suggestion came out during the meeting was commit periodically. This will address long transaction problem, but we also discussed it would be hard on failure handling. Currently, Gobblin does task level retry on failure and there were three options we\u2019ve discussed. (There was no silver bullet solution from the meeting.) Note that these are all with committing periodically. Revert to previous state: For writer, this will be delete the record it wrote. For JdbcWriter, it could use it\u2019s own staging table or could share staging table with other writer. As staging table can be passed by user where we don\u2019t have control of, not able to add partition information, it is hard to revert back to previous state for all cases. Ignore duplicate: The idea is to use Upsert to perform insert or update. As it needs to check the current existence in the dataset, it is expected to show performance degradation. Also, possibility of duplicate entry was also discussed. Water mark: In order to use water mark in task level, writer needs to send same order when retried which is not guaranteed. Data with over 200M record was tested with single transaction and it had no problem in MySQL 5.6. Operation: Write operation will write into staging table. (If user chose to skip staging table, write operation will write into destination table directly.) Commit operation will commit the transaction. Close operation will close database connection. If there was a failure, it will execute rollback before closing connection. Skipping staging table From the requirement, user can choose to skip staging for performance reason. In this case, writer will directly persist into final destination. Without staging table, it is hard to recover from failure as mentioned above and for this reason, if user does not want staging, the framework does not guarantee any recovery from the failure. If user configures \"job.commit.policy=partial\" and \"publish.at.job.level=false\", this means it won't be published in job level and it allows partial success commit. This will make Gobblin to skip staging table as it aligns with the behavior of skipping staging. The reason to reuse these two parameters instead of introducing new parameter is to avoid parameters contradicting each other. Figure 4 shows overall flow of JDBC writer. - figure 5. JDBC Writer flow - JDBC Publisher Uses JDBC to publish final result into output. If user chose to not to use staging table, for performance reason, JDBC publisher won't do anything as the output is already updated by the writer(s). More precisely, the parameter makes Gobblin skip staging table, will make publisher to be skipped. JDBC publisher will copy data from staging table via SQL command that underlying database provides. (Renaming staging table is not viable mainly because 1. it's hard to copy exact structure as original one (constraints, index, sequence, foreign keys, etc). 2. couple of underlying database system, rename requires no active connection to the table.) A publisher will open one transaction. Being processed in single transaction, any failure can be reverted by rolling back transaction. Operation: PublishData operation opens single transaction and writes into destination table from staging table. If there is any error, transaction will be rolled back. Once completed successfully, transaction will be committed. Parallelism: Currently parallelism in publisher level is not supported. For example the MySQL Writer fails on deleting all from table before inserting new data using global transaction with multiple connections. PublishMeta won\u2019t do anything. Figure 6 shows overall flow of JDBC publisher. - figure 6. Gobblin_JDBC_Publisher - Concrete implementations To configure a concrete writer, please refer the JDBC Writer Properties section in the Configuration Glossary . MySQL Writer The MySQL writer uses buffered inserts to increase performance. The sink configuration for MySQL in a Gobblin job is as follows: writer.destination.type=MYSQL writer.builder.class=org.apache.gobblin.writer.JdbcWriterBuilder data.publisher.type=org.apache.gobblin.publisher.JdbcPublisher jdbc.publisher.url=jdbc:mysql://host:3306 jdbc.publisher.driver=com.mysql.jdbc.Driver converter.classes=org.apache.gobblin.converter.jdbc.AvroToJdbcEntryConverter # If field name mapping is needed between the input Avro and the target table: converter.avro.jdbc.entry_fields_pairs={\\ src_fn\\ :\\ firstname\\ ,\\ src_ln\\ :\\ lastname\\ } Teradata Writer Similarly to the MySQL Writer, this writer also inserts data in batches, configured by writer.jdbc.batch_size . Ideally, for performance reasons the target table is advised to be set to type MULTISET, without a primary index. Please note, that the Teradata JDBC drivers are not part of Gobblin, one needs to obtain them from Teradata and pass them as job specific jars to the Gobblin submitter scripts. Teradata may use the FASTLOAD option during the insert if conditions are met. The sink configuration for Teradata in a Gobblin job is as follows: writer.destination.type=TERADATA writer.builder.class=org.apache.gobblin.writer.JdbcWriterBuilder data.publisher.type=org.apache.gobblin.publisher.JdbcPublisher jdbc.publisher.url=jdbc:teradata://host/TMODE=ANSI,CHARSET=UTF16,TYPE=FASTLOAD jdbc.publisher.driver=com.teradata.jdbc.TeraDriver converter.classes=org.apache.gobblin.converter.jdbc.AvroToJdbcEntryConverter # If field name mapping is needed between the input Avro and the target table: converter.avro.jdbc.entry_fields_pairs={\\ src_fn\\ :\\ firstname\\ ,\\ src_ln\\ :\\ lastname\\ } Performance and Scalability As Gobblin can dial up parallelism, the bottleneck of performance will be the underlying RDBMS. Thus, performance and scalability will be mainly up to underlying RDBMS. Benchmark: MySQL Writer performance test on 80k records. Each entry consists of 14 fields with sparse density. Few observations: - Starting from batch insert size 1,000, the performance gain diminishes. - The parallelism does not show much of gain. This is mostly because of the overhead of parallelism. Parallelism is expected to show more performance gain on bigger record sets. Batch insert size Parallelism Elapsed 10 1 17 minutes 30 1 5 minutes 30 seconds 100 1 2 minutes 17 seconds 1,000 1 40 seconds 10,000 1 43 seconds 1,000 2 42 seconds 1,000 4 44 seconds 1,000 8 57 seconds All tests used: Hadoop 2.3 in Pseudo-distributed mode on CPU: 12 cores @1.2GHz, Memory: 64GBytes MySQL server 5.6.21 with InnoDB storage engine, and compression on. Server runs on Dual CPU Intel Xeon 6 cores @2.5GHz, Memory: 48 GBytes, and HDD: 2.4TB (10K RPM) Important note As Gobblin framework comes with parallel processing via Hadoop, it can easily overburden the underlying RDBMS. User needs to choose parallelism level conservativley.","title":"JDBC"},{"location":"sinks/Gobblin-JDBC-Writer/#table-of-contents","text":"Table of Contents Gobblin JDBC writer publisher Proposed design Requirements Design summary Design detail WriterInitializer JdbcWriterInitializer AvroFieldsPickConverter AvroToJdbcEntryConverter JdbcWriter Skipping staging table JDBC Publisher Concrete implementations MySQL Writer Teradata Writer Performance and Scalability Important note","title":"Table of Contents"},{"location":"sinks/Gobblin-JDBC-Writer/#gobblin-jdbc-writer-publisher","text":"Gobblin is a general data ingestion framework that can extract, convert, and publish data. Currently publishing into JDBC compatible RDBMS is not in Gobblin and here we are introducing JDBC writer (and publisher) so that Gobblin can easily write into JDBC compatible RDBMS by using new JDBC writer at the same time reusing existing extraction, and conversion.","title":"Gobblin JDBC writer &amp; publisher"},{"location":"sinks/Gobblin-JDBC-Writer/#proposed-design","text":"","title":"Proposed design"},{"location":"sinks/Gobblin-JDBC-Writer/#requirements","text":"User can choose to replace the destination table. User can pass the staging table, for the case user does not have permission to create table. If user chose to use their own staging table, user can also choose to truncate the staging table. User should be able to skip staging table for performance reason. User can choose level of parallelism. For Avro \u2192 JDBC use case, user should be able to cherry pick the fields to be copied.","title":"Requirements"},{"location":"sinks/Gobblin-JDBC-Writer/#design-summary","text":"New JdbcWriter, JdbcPublisher will be introduced, along with AvroFieldsPickConverter and AvroToJdbcEntry. Figure 1 shows Gobblin general flow and figure 2 shows the specific flow; Avro \u2192 RDBMS through JDBC. JdbcWriter will use staging table mainly for failure handling. Since Gobblin breaks down the job into multiple task and process it in parallel, each writer will hold transaction and there will be more than one transaction against the database. If there\u2019s no staging table and writer writes directly to destination table, partial failure among the writers may result partial data push into destination table. Having partial data is mostly bad for consumption and could also result subsequent failure of next data load due to data inconsistency. Also, having partial data also makes it hard to retry on failure. Thus, Gobblin will use staging table so that writer writes into staging table and publisher can copy data from staging table into destination table in one single transaction. Having one transaction from the publisher level enables Gobblin to revert back to original on failure and make it able to be retried from previous state. For performance reason, and from the requirement, user may skip staging. This comes with the cost of giving up failure handling and for this case Gobblin does not guarantee recovery from failure. Will introduce WriterInitializer for the case that writer needs an initialization but needs to be done before going on parallel processing. (more on below) - figure 1. Gobblin general flow - - figure 2. Gobblin Avro \u2192 RDBMS through JDBC specific flow - AvroFieldsPickConverter will cherry pick the columns that user wants. AvroToJdbcEntryConverter will convert Avro to JDBC entry. JdbcWriter will write JdbcEntry to staging table. (user can skip staging which is addressed below in JDBC Writer/Publisher section). JdbcPublisher will write into destination table.","title":"Design summary"},{"location":"sinks/Gobblin-JDBC-Writer/#design-detail","text":"","title":"Design detail"},{"location":"sinks/Gobblin-JDBC-Writer/#writerinitializer","text":"Note that this is a new interface where its responsibility is to initialize writer, which means it\u2019s tied to writer\u2019s implementation, and clean what it initialized. Reasons of introducing writer initializer: The main reason for this initializer is to perform initialization but not in parallel environment. As writer subjects to run in parallel, certain task that needs to be done only once across all writers is hard to be done. For example, if user chose to skip staging table and also chose to replace destination table, the initialization task would be truncating destination table which needs to be done only once before all writers start writing. This is simply hard to be done in parallel environment and better to avoid trying it for simplicity. Another reason for writer initializer is to simplify the cleanup process. As writer initializer initializes things, it also knows what to clean up. Instead of having other code to figure out what to clean up on which branch and condition all over again, closing writer initializer at the end of the job will just simply clean it up.(This pattern is widely used in JDK(e.g: Stream), where many classes implements interface Closeable). This clean up can be done in writer, but writer is currently being closed in task level, where there\u2019s a case that rest of the flow still needs it. (e.g: staging table should not be cleaned until publish is completed.) Currently, Gobblin has a logic to clean up the the staging data in JobLauncherUtils which is specific to FsDataWriter, where AbstractJobLauncher has the logic to figure out what kind of clean up method of JobLauncherUtils needs to be called. Ideally, we will hide this implementation of clean up behind the interface of WriterInitializer. Figure 3 shows interface of WriterInitializer. WriterInitializer will be extensible in Gobblin via factory method pattern where any writer can plugin their initialization code if needed. - figure 3. Class diagram for WriterInitializer interface -","title":"WriterInitializer"},{"location":"sinks/Gobblin-JDBC-Writer/#jdbcwriterinitializer","text":"JdbcWriterInitializer will be the first class implements WriterInitializer interface. This will be instantiated by AbstractJobLauncher after Source creates WorkUnit and always get closed by AbstractJobLauncher when job is finished regardless fail or success. By default, JdbcWriterInitializer will create staging tables based on the structure of the target table. Therefore it's necessary to create the target table first. Staging tables are created per Workunit, in which there can be more than one staging table. Having multiple staging table will make parallelism easier for publisher when moving data from staging table to destination. Any table created by JdbcWriterInitializer will be remembered and later will be dropped when it\u2019s being closed. Staging will be placed in same destination host, same database, some temporary table name, with same structure. The main purpose of staging table for handling failure. Without staging table, it is hard to recover from failure, because writers writes into table in multiple transactions. Additionally, staging table also brings data integrity check before publishing into destination. Before creating the staging table, JdbcWriterInitializer will validate if user has drop table privilege to make sure it can drop it later on. User can choose to use their own staging table. This is to support the use case when user does not have privilege to create table. When user chooses to use their own staging table, JdbcWriterInitializer will truncate the table later when it\u2019s being closed. Staging table initially should be always empty. User who chose to use user\u2019s own staging table can also choose to truncate staging table. If staging table is not empty and user does not choose to truncate table, JdbcWriterInitializer will make the job fail. If user chose to skip staging table and replace the output, JdbcWriterInitializer will truncate destination table. This is because destination table needs to be emptied prior going into parallel processing as more than one writer will start writing simultaneously. Figure 4 shows overall flow of JdbcWriterInitializer. - figure 4. JdbcWriterInitializer flow -","title":"JdbcWriterInitializer"},{"location":"sinks/Gobblin-JDBC-Writer/#avrofieldspickconverter","text":"User can define which fields to be copied. Given input from the user, it will narrow down the number of columns by updating schema and data.","title":"AvroFieldsPickConverter"},{"location":"sinks/Gobblin-JDBC-Writer/#avrotojdbcentryconverter","text":"Will convert avro schema and data into JdbcEntrySchema and JdbcEntryData. JdbcEntrySchema consists of pairs of column name and JDBCTypes, and JdbcEntry consists of pairs of column names and object where object can be directly used against PreparedStatement.setObject(). Although Avro schema can be a recursive structure by having record in the record, RDBMS table structure is not a recursive data structure. Thus, AvroToJdbcEntryConverter does not accept Avro schema that has record type inside record type. Both JdbcEntrySchema and JdbcEntry will be case sensitive because Avro field name is case sensitive, and many widely used RDBMS are case sensitive on column name as well. In case there\u2019s a mismatch on column names, AvroToJdbcEntryConverter will take column name mapping between Avro field name and Jdbc column name.","title":"AvroToJdbcEntryConverter"},{"location":"sinks/Gobblin-JDBC-Writer/#jdbcwriter","text":"Uses JDBC to persist data into staging table in task level. By default it will persist into staging area (and will be put into final destination by publisher). Staging table is already ready by WriterInitializer. Input column names should exactly match Jdbc column names. User can convert the name using AvroToJdbcEntryConverter. Schema evolution: The number of input columns is expected to be equal or smaller than the number of columns in Jdbc. This is to prevent unintended outcome from schema evolution such as additional column. As underlying Jdbc RDBMS can declare constraints on its schema, writer will allow if number of columns in Jdbc is greater than number of input columns. number of input columns = number of columns in Jdbc Each writer will open one transaction. Having one transaction per writer has its tradeoffs: Pro: Simple on failure handling as you can just simply execute rollback on failure. Basically, it will revert back to previous state so that the job can retry the task. Con: It can lead up to long lived transaction and it can face scalability issue. (Not enough disk space for transaction log, number of record limit on one transaction (2.1B for Postgre sql), etc) JdbcWriter will go with one transaction per writer for it\u2019s simplicity on failure handling. Scalability issue with long transaction can be overcome by increasing partition which makes transaction short. During the design meeting, we\u2019ve discussed that long transaction could be a problem. One suggestion came out during the meeting was commit periodically. This will address long transaction problem, but we also discussed it would be hard on failure handling. Currently, Gobblin does task level retry on failure and there were three options we\u2019ve discussed. (There was no silver bullet solution from the meeting.) Note that these are all with committing periodically. Revert to previous state: For writer, this will be delete the record it wrote. For JdbcWriter, it could use it\u2019s own staging table or could share staging table with other writer. As staging table can be passed by user where we don\u2019t have control of, not able to add partition information, it is hard to revert back to previous state for all cases. Ignore duplicate: The idea is to use Upsert to perform insert or update. As it needs to check the current existence in the dataset, it is expected to show performance degradation. Also, possibility of duplicate entry was also discussed. Water mark: In order to use water mark in task level, writer needs to send same order when retried which is not guaranteed. Data with over 200M record was tested with single transaction and it had no problem in MySQL 5.6. Operation: Write operation will write into staging table. (If user chose to skip staging table, write operation will write into destination table directly.) Commit operation will commit the transaction. Close operation will close database connection. If there was a failure, it will execute rollback before closing connection.","title":"JdbcWriter"},{"location":"sinks/Gobblin-JDBC-Writer/#skipping-staging-table","text":"From the requirement, user can choose to skip staging for performance reason. In this case, writer will directly persist into final destination. Without staging table, it is hard to recover from failure as mentioned above and for this reason, if user does not want staging, the framework does not guarantee any recovery from the failure. If user configures \"job.commit.policy=partial\" and \"publish.at.job.level=false\", this means it won't be published in job level and it allows partial success commit. This will make Gobblin to skip staging table as it aligns with the behavior of skipping staging. The reason to reuse these two parameters instead of introducing new parameter is to avoid parameters contradicting each other. Figure 4 shows overall flow of JDBC writer. - figure 5. JDBC Writer flow -","title":"Skipping staging table"},{"location":"sinks/Gobblin-JDBC-Writer/#jdbc-publisher","text":"Uses JDBC to publish final result into output. If user chose to not to use staging table, for performance reason, JDBC publisher won't do anything as the output is already updated by the writer(s). More precisely, the parameter makes Gobblin skip staging table, will make publisher to be skipped. JDBC publisher will copy data from staging table via SQL command that underlying database provides. (Renaming staging table is not viable mainly because 1. it's hard to copy exact structure as original one (constraints, index, sequence, foreign keys, etc). 2. couple of underlying database system, rename requires no active connection to the table.) A publisher will open one transaction. Being processed in single transaction, any failure can be reverted by rolling back transaction. Operation: PublishData operation opens single transaction and writes into destination table from staging table. If there is any error, transaction will be rolled back. Once completed successfully, transaction will be committed. Parallelism: Currently parallelism in publisher level is not supported. For example the MySQL Writer fails on deleting all from table before inserting new data using global transaction with multiple connections. PublishMeta won\u2019t do anything. Figure 6 shows overall flow of JDBC publisher. - figure 6. Gobblin_JDBC_Publisher -","title":"JDBC Publisher"},{"location":"sinks/Gobblin-JDBC-Writer/#concrete-implementations","text":"To configure a concrete writer, please refer the JDBC Writer Properties section in the Configuration Glossary .","title":"Concrete implementations"},{"location":"sinks/Gobblin-JDBC-Writer/#mysql-writer","text":"The MySQL writer uses buffered inserts to increase performance. The sink configuration for MySQL in a Gobblin job is as follows: writer.destination.type=MYSQL writer.builder.class=org.apache.gobblin.writer.JdbcWriterBuilder data.publisher.type=org.apache.gobblin.publisher.JdbcPublisher jdbc.publisher.url=jdbc:mysql://host:3306 jdbc.publisher.driver=com.mysql.jdbc.Driver converter.classes=org.apache.gobblin.converter.jdbc.AvroToJdbcEntryConverter # If field name mapping is needed between the input Avro and the target table: converter.avro.jdbc.entry_fields_pairs={\\ src_fn\\ :\\ firstname\\ ,\\ src_ln\\ :\\ lastname\\ }","title":"MySQL Writer"},{"location":"sinks/Gobblin-JDBC-Writer/#teradata-writer","text":"Similarly to the MySQL Writer, this writer also inserts data in batches, configured by writer.jdbc.batch_size . Ideally, for performance reasons the target table is advised to be set to type MULTISET, without a primary index. Please note, that the Teradata JDBC drivers are not part of Gobblin, one needs to obtain them from Teradata and pass them as job specific jars to the Gobblin submitter scripts. Teradata may use the FASTLOAD option during the insert if conditions are met. The sink configuration for Teradata in a Gobblin job is as follows: writer.destination.type=TERADATA writer.builder.class=org.apache.gobblin.writer.JdbcWriterBuilder data.publisher.type=org.apache.gobblin.publisher.JdbcPublisher jdbc.publisher.url=jdbc:teradata://host/TMODE=ANSI,CHARSET=UTF16,TYPE=FASTLOAD jdbc.publisher.driver=com.teradata.jdbc.TeraDriver converter.classes=org.apache.gobblin.converter.jdbc.AvroToJdbcEntryConverter # If field name mapping is needed between the input Avro and the target table: converter.avro.jdbc.entry_fields_pairs={\\ src_fn\\ :\\ firstname\\ ,\\ src_ln\\ :\\ lastname\\ }","title":"Teradata Writer"},{"location":"sinks/Gobblin-JDBC-Writer/#performance-and-scalability","text":"As Gobblin can dial up parallelism, the bottleneck of performance will be the underlying RDBMS. Thus, performance and scalability will be mainly up to underlying RDBMS. Benchmark: MySQL Writer performance test on 80k records. Each entry consists of 14 fields with sparse density. Few observations: - Starting from batch insert size 1,000, the performance gain diminishes. - The parallelism does not show much of gain. This is mostly because of the overhead of parallelism. Parallelism is expected to show more performance gain on bigger record sets. Batch insert size Parallelism Elapsed 10 1 17 minutes 30 1 5 minutes 30 seconds 100 1 2 minutes 17 seconds 1,000 1 40 seconds 10,000 1 43 seconds 1,000 2 42 seconds 1,000 4 44 seconds 1,000 8 57 seconds All tests used: Hadoop 2.3 in Pseudo-distributed mode on CPU: 12 cores @1.2GHz, Memory: 64GBytes MySQL server 5.6.21 with InnoDB storage engine, and compression on. Server runs on Dual CPU Intel Xeon 6 cores @2.5GHz, Memory: 48 GBytes, and HDD: 2.4TB (10K RPM)","title":"Performance and Scalability"},{"location":"sinks/Gobblin-JDBC-Writer/#important-note","text":"As Gobblin framework comes with parallel processing via Hadoop, it can easily overburden the underlying RDBMS. User needs to choose parallelism level conservativley.","title":"Important note"},{"location":"sinks/Http/","text":"Table of Contents Table of Contents Introduction Note Constructs HttpOperation AsyncRequestBuilder HttpClient ResponseHandler Build an asynchronous writer AvroHttpWriterBuilder R2RestWriterBuilder Build a synchronous writer Introduction Writing to a http based sink is done by sending a http or restful request and handling the response. Given the endpoint uri, query parameters, and body, it is straightforward to construct a http request. The idea is to build a writer that writes a http record, which contains those elements of a request. The writer builds a http or rest request from multiple http records, sends the request with a client that knows the server, and handles the response. Note The old http write framework under AbstractHttpWriter and AbstractHttpWriterBuilder is deprecated (Deprecation date: 05/15/2018)! Use AsyncHttpWriter and AsyncHttpWriterBuilder instead Constructs Figure 1. Http write flow HttpOperation A http record is represented as a HttpOperation object. It has 4 fields. Field Name Description Example keys Optional, a key/value map to interpolate the url template {\"memberId\": \"123\"} queryParams Optional, a map from query parameter to its value {\"action\": \"update\"} headers Optional, a map from header key to ts value {\"version\": \"2.0\"} body Optional, the request body in string or json string format \"{\\\"email\\\": \\\"httpwrite@test.com\\\"}\" Given an url template, http://www.test.com/profiles/${memberId} , from job configuration, the resolved example request url with keys and queryParams information will be http://www.test.com/profiles/123?action=update . AsyncRequestBuilder An AsyncRequestBuilder builds an AsyncRequest from a collection of HttpOperation records. It could build one request per record or batch multiple records into a single request. A builder is also responsible for putting the headers and setting the body to the request. HttpClient A HttpClient sends a request and returns a response. If necessary, it should setup the connection to the server, for example, sending an authorization request to get access token. How authorization is done is per use case. Gobblin does not provide general support for authorization yet. ResponseHandler A ResponseHandler handles a response of a request. It returns a ResponseStatus object to the framework, which would resend the request if it's a SERVER_ERROR . Build an asynchronous writer AsyncHttpWriterBuilder is the base builder to build an asynchronous http writer. A specific writer can be created by providing the 3 major components: a HttpClient , a AsyncRequestBuilder , and a ResponseHandler . Gobblin offers 2 implementations of async http writers. As long as your write requirement can be expressed as a HttpOperation through a Converter , the 2 implementations should work with configurations. AvroHttpWriterBuilder An AvroHttpWriterBuilder builds an AsyncHttpWriter on top of the apache httpcomponents framework , sending vanilla http request. The 3 major components are: ApacheHttpClient . It uses CloseableHttpClient to send HttpUriRequest and receive CloseableHttpResponse ApacheHttpRequestBuilder . It builds a ApacheHttpRequest , which is an AsyncRequest that wraps the HttpUriRequest , from one HttpOperation ApacheHttpResponseHandler . It handles a HttpResponse Configurations for the builder are: Configuration Description Example gobblin.writer.http.urlTemplate Required, the url template(schema and port included), together with keys and queryParams , to be resolved to request url http://www.test.com/profiles/${memberId} gobblin.writer.http.verb Required, http verbs get, update, delete, etc gobblin.writer.http.errorCodeWhitelist Optional, http error codes allowed to pass through 404, 500, etc. No error code is allowed by default gobblin.writer.http.maxAttempts Optional, max number of attempts including initial send Default is 3 gobblin.writer.http.contentType Optional, content type of the request body \"application/json\" , which is the default value R2RestWriterBuilder A R2RestWriterBuilder builds an AsyncHttpWriter on top of restli r2 framework , sending rest request. The 3 major components are: R2Client . It uses a R2 Client to send RestRequest and receive RestResponse R2RestRequestBuilder . It builds a R2Request , which is an AsyncRequest that wraps the RestRequest , from one HttpOperation R2RestResponseHandler . It handles a RestResponse R2RestWriterBuilder has d2 and ssl support. Configurations( (d2.) part should be added in d2 mode) for the builder are: Configuration Description Example gobblin.writer.http.urlTemplate Required, the url template(schema and port included), together with keys and queryParams , to be resolved to request url. If the schema is d2 , d2 is enabled http://www.test.com/profiles/${memberId} gobblin.writer.http.verb Required, rest(rest.li) verbs get, update, put, delete, etc gobblin.writer.http.maxAttempts Optional, max number of attempts including initial send Default is 3 gobblin.writer.http.errorCodeWhitelist Optional, http error codes allowed to pass through 404, 500, etc. No error code is allowed by default gobblin.writer.http.d2.zkHosts Required for d2, the zookeeper address gobblin.writer.http.(d2.)ssl Optional, enable ssl Default is false gobblin.writer.http.(d2.)keyStoreFilePath Required for ssl /tmp/identity.p12 gobblin.writer.http.(d2.)keyStoreType Required for ssl PKCS12 gobblin.writer.http.(d2.)keyStorePassword Required for ssl gobblin.writer.http.(d2.)trustStoreFilePath Required for ssl gobblin.writer.http.(d2.)trustStorePassword Required for ssl gobblin.writer.http.protocolVersion Optional, protocol version of rest.li 2.0.0 , which is the default value R2RestWriterBuilder isn't ingegrated with PasswordManager to process encrypted passwords yet. The task is tracked as https://issues.apache.org/jira/browse/GOBBLIN-487 Build a synchronous writer The idea is to reuse an asynchronous writer to build its synchronous version. The technical difference between them is the size of outstanding writes. Set gobblin.writer.http.maxOutstandingWrites to be 1 (default value is 1000 ) to make a synchronous writer","title":"HTTP"},{"location":"sinks/Http/#table-of-contents","text":"Table of Contents Introduction Note Constructs HttpOperation AsyncRequestBuilder HttpClient ResponseHandler Build an asynchronous writer AvroHttpWriterBuilder R2RestWriterBuilder Build a synchronous writer","title":"Table of Contents"},{"location":"sinks/Http/#introduction","text":"Writing to a http based sink is done by sending a http or restful request and handling the response. Given the endpoint uri, query parameters, and body, it is straightforward to construct a http request. The idea is to build a writer that writes a http record, which contains those elements of a request. The writer builds a http or rest request from multiple http records, sends the request with a client that knows the server, and handles the response.","title":"Introduction"},{"location":"sinks/Http/#note","text":"The old http write framework under AbstractHttpWriter and AbstractHttpWriterBuilder is deprecated (Deprecation date: 05/15/2018)! Use AsyncHttpWriter and AsyncHttpWriterBuilder instead","title":"Note"},{"location":"sinks/Http/#constructs","text":"Figure 1. Http write flow","title":"Constructs"},{"location":"sinks/Http/#httpoperation","text":"A http record is represented as a HttpOperation object. It has 4 fields. Field Name Description Example keys Optional, a key/value map to interpolate the url template {\"memberId\": \"123\"} queryParams Optional, a map from query parameter to its value {\"action\": \"update\"} headers Optional, a map from header key to ts value {\"version\": \"2.0\"} body Optional, the request body in string or json string format \"{\\\"email\\\": \\\"httpwrite@test.com\\\"}\" Given an url template, http://www.test.com/profiles/${memberId} , from job configuration, the resolved example request url with keys and queryParams information will be http://www.test.com/profiles/123?action=update .","title":"HttpOperation"},{"location":"sinks/Http/#asyncrequestbuilder","text":"An AsyncRequestBuilder builds an AsyncRequest from a collection of HttpOperation records. It could build one request per record or batch multiple records into a single request. A builder is also responsible for putting the headers and setting the body to the request.","title":"AsyncRequestBuilder"},{"location":"sinks/Http/#httpclient","text":"A HttpClient sends a request and returns a response. If necessary, it should setup the connection to the server, for example, sending an authorization request to get access token. How authorization is done is per use case. Gobblin does not provide general support for authorization yet.","title":"HttpClient"},{"location":"sinks/Http/#responsehandler","text":"A ResponseHandler handles a response of a request. It returns a ResponseStatus object to the framework, which would resend the request if it's a SERVER_ERROR .","title":"ResponseHandler"},{"location":"sinks/Http/#build-an-asynchronous-writer","text":"AsyncHttpWriterBuilder is the base builder to build an asynchronous http writer. A specific writer can be created by providing the 3 major components: a HttpClient , a AsyncRequestBuilder , and a ResponseHandler . Gobblin offers 2 implementations of async http writers. As long as your write requirement can be expressed as a HttpOperation through a Converter , the 2 implementations should work with configurations.","title":"Build an asynchronous writer"},{"location":"sinks/Http/#avrohttpwriterbuilder","text":"An AvroHttpWriterBuilder builds an AsyncHttpWriter on top of the apache httpcomponents framework , sending vanilla http request. The 3 major components are: ApacheHttpClient . It uses CloseableHttpClient to send HttpUriRequest and receive CloseableHttpResponse ApacheHttpRequestBuilder . It builds a ApacheHttpRequest , which is an AsyncRequest that wraps the HttpUriRequest , from one HttpOperation ApacheHttpResponseHandler . It handles a HttpResponse Configurations for the builder are: Configuration Description Example gobblin.writer.http.urlTemplate Required, the url template(schema and port included), together with keys and queryParams , to be resolved to request url http://www.test.com/profiles/${memberId} gobblin.writer.http.verb Required, http verbs get, update, delete, etc gobblin.writer.http.errorCodeWhitelist Optional, http error codes allowed to pass through 404, 500, etc. No error code is allowed by default gobblin.writer.http.maxAttempts Optional, max number of attempts including initial send Default is 3 gobblin.writer.http.contentType Optional, content type of the request body \"application/json\" , which is the default value","title":"AvroHttpWriterBuilder"},{"location":"sinks/Http/#r2restwriterbuilder","text":"A R2RestWriterBuilder builds an AsyncHttpWriter on top of restli r2 framework , sending rest request. The 3 major components are: R2Client . It uses a R2 Client to send RestRequest and receive RestResponse R2RestRequestBuilder . It builds a R2Request , which is an AsyncRequest that wraps the RestRequest , from one HttpOperation R2RestResponseHandler . It handles a RestResponse R2RestWriterBuilder has d2 and ssl support. Configurations( (d2.) part should be added in d2 mode) for the builder are: Configuration Description Example gobblin.writer.http.urlTemplate Required, the url template(schema and port included), together with keys and queryParams , to be resolved to request url. If the schema is d2 , d2 is enabled http://www.test.com/profiles/${memberId} gobblin.writer.http.verb Required, rest(rest.li) verbs get, update, put, delete, etc gobblin.writer.http.maxAttempts Optional, max number of attempts including initial send Default is 3 gobblin.writer.http.errorCodeWhitelist Optional, http error codes allowed to pass through 404, 500, etc. No error code is allowed by default gobblin.writer.http.d2.zkHosts Required for d2, the zookeeper address gobblin.writer.http.(d2.)ssl Optional, enable ssl Default is false gobblin.writer.http.(d2.)keyStoreFilePath Required for ssl /tmp/identity.p12 gobblin.writer.http.(d2.)keyStoreType Required for ssl PKCS12 gobblin.writer.http.(d2.)keyStorePassword Required for ssl gobblin.writer.http.(d2.)trustStoreFilePath Required for ssl gobblin.writer.http.(d2.)trustStorePassword Required for ssl gobblin.writer.http.protocolVersion Optional, protocol version of rest.li 2.0.0 , which is the default value R2RestWriterBuilder isn't ingegrated with PasswordManager to process encrypted passwords yet. The task is tracked as https://issues.apache.org/jira/browse/GOBBLIN-487","title":"R2RestWriterBuilder"},{"location":"sinks/Http/#build-a-synchronous-writer","text":"The idea is to reuse an asynchronous writer to build its synchronous version. The technical difference between them is the size of outstanding writes. Set gobblin.writer.http.maxOutstandingWrites to be 1 (default value is 1000 ) to make a synchronous writer","title":"Build a synchronous writer"},{"location":"sinks/Kafka/","text":"Table of Contents Table of Contents Introduction Pre-requisites Steps Configuration Details What Next? Introduction The Kafka writer allows users to create pipelines that ingest data from Gobblin sources into Kafka. This also enables Gobblin users to seamlessly transition their pipelines from ingesting directly to HDFS to ingesting into Kafka first, and then ingesting from Kafka to HDFS. Pre-requisites The following guide assumes that you are somewhat familiar with running Gobblin. If not, you should follow the Getting Started page first, then come back to this guide. Before you can use the Kafka writer, you need to set up a Kafka cluster to write to. You can follow any of the guides listed by the Kafka project such as the Apache Kafka quickstart guide . Steps Edit the wikipedia-kafka.pull example to get started with setting up ingestion into Kafka. This is a very similar pipeline to the wikipedia.pull example which pulls pages from 5 titles from Wikipedia to HDFS. The main differences to note are: The writer.builder.class is set to gobblin.kafka.writer.KafkaDataWriterBuilder . This is the class that creates a Kafka writer. The writer.kafka.topic is set to WikipediaExample . This is the topic that the writer will write the records to. The writer.kafka.producerConfig.bootstrap.servers is set to localhost:9092 . This is the address of the kafka broker(s) that the writer must write to. There is no partitioner class specified. This implementation of the Kafka writer does not support partitioning and will use the default Kafka partitioner. The data.publisher.type is set to gobblin.publisher.NoopPublisher . This is because Kafka doesn't offer transactional semantics, so it isn't possible to have a separate publish step to finally commit the data. There is configuration for setting up the Schema Registry and Serializers that you will be using to write the data to Kafka. If you're using the Apache Kafka distribution, this file should work out of the box. If you're using the Confluent distribution and want to use the Confluent schema registry, comment out the Local Schema Registry section and un-comment the Confluent schema registry section. The result should match the text below for Confluent users. #Confluent Schema Registry and serializers writer.kafka.producerConfig.value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer writer.kafka.producerConfig.key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer writer.kafka.producerConfig.schema.registry.url=http://localhost:8081 #Set this to the correct schema-reg url ##Use Local Schema Registry and serializers #writer.kafka.producerConfig.value.serializer=org.apache.gobblin.kafka.serialize.LiAvroSerializer #writer.kafka.producerConfig.kafka.schemaRegistry.class=org.apache.gobblin.kafka.schemareg.ConfigDrivenMd5SchemaRegistry #writer.kafka.producerConfig.schemaRegistry.schema.name=WikipediaExample #writer.kafka.producerConfig.schemaRegistry.schema.value={ namespace : example.wikipedia.avro , type : record , name : WikipediaArticle , fields : [{ name : pageid , type : [ double , null ]},{ name : title , type : [ string , null ]},{ name : user , type : [ string , null ]},{ name : anon , type : [ string , null ]},{ name : userid , type : [ double , null ]},{ name : timestamp , type : [ string , null ]},{ name : size , type : [ double , null ]},{ name : contentformat , type : [ string , null ]},{ name : contentmodel , type : [ string , null ]},{ name : content , type : [ string , null ]}]} Run the standalone launcher with the wikipedia-kafka.pull file. You should see something like this. INFO [TaskExecutor-0] gobblin.example.wikipedia.WikipediaExtractor 243 - 5 record(s) retrieved for title LinkedIn INFO [TaskExecutor-0] gobblin.example.wikipedia.WikipediaExtractor 243 - 5 record(s) retrieved for title Parris_Cues INFO [TaskExecutor-0] gobblin.example.wikipedia.WikipediaExtractor 243 - 5 record(s) retrieved for title Barbara_Corcoran INFO [TaskExecutor-0] gobblin.runtime.Task 176 - Extracted 20 data records INFO [TaskExecutor-0] gobblin.runtime.Task 177 - Row quality checker finished with results: INFO [TaskExecutor-0] gobblin.publisher.TaskPublisher 43 - All components finished successfully, checking quality tests INFO [TaskExecutor-0] gobblin.publisher.TaskPublisher 45 - All required test passed for this task passed. INFO [TaskExecutor-0] gobblin.publisher.TaskPublisher 47 - Cleanup for task publisher executed successfully. INFO [TaskExecutor-0] gobblin.runtime.Fork 261 - Committing data for fork 0 of task task_PullFromWikipediaToKafka_1472246706122_0 INFO [TaskExecutor-0] gobblin.kafka.writer.KafkaDataWriter 211 - Successfully committed 20 records. To verify that the records have indeed been ingested into Kafka, you can run a kafka console consumer or run Gobblin's kafka-console pull file which prints the events from Kafka onto the console. Configuration Details At this time, Gobblin supports integration with Kafka 0.8 and 0.9. The Kafka writer supports all the configuration parameters supported by the version-specific Kafka Producer (e.g. Latest Producer Configs ). All you have to do is prefix writer.kafka.producerConfig. to each configuration property that the producer supports. For example, if you want to set the acks parameter to all to ensure full acknowledgement of writes, you would set writer.kafka.producerConfig.acks=all in your pull file. For a comprehensive list of all the configuration properties supported by the producers, go through the official documentation here . Note: Since Gobblin is currently built against Kafka 0.8.2, the configuration options apply to the new 0.8.2 java producer. There are a few key parameters at the Gobblin level that control the behavior of the data writer. Property Name Semantics writer.kafka.topic The topic that the writer will be writing to. At this time, the writer can only write to a single topic per pipeline. writer.kafka.failureAllowancePercentage The percentage of failures that you are willing to tolerate while writing to Kafka. Gobblin will mark the workunit successful and move on if there are failures but not enough to trip the failure threshold. Only successfully acknowledged writes are counted as successful, all others are considered as failures. The default for the failureAllowancePercentage is set to 20.0. This means that as long as 80% of the data is acknowledged by Kafka, Gobblin will move on. If you want higher guarantees, set this config value to a lower value. e.g. If you want 99% delivery guarantees, set this value to 1.0 writer.kafka.commitTimeoutMillis The amount of time that the Gobblin committer will wait before abandoning its wait for unacknowledged writes. This defaults to 1 minute. writer.kafka.keyed When set to true, enables key-based writes to Kafka. This defaults to false. If you set this to true, make sure to set the keyField configuration property. Serialization of the key is controlled by the Kafka Producer specific configuration property ( writer.kafka.producerConfig.key.serializer ) writer.kafka.keyField The field of the record to use as the key for writing to Kafka. The field path follows a nested notation. So a top-level field \"name\" would be set to \"name\", a nested field \"name\" within a top-level struct \"header\" would be named \"header.name\" writer.kafka.typeMapperClass The class that the writer should use to extract keys and values from the input record. The default if not specified assumes that AvroGenericRecordTypeMapper is being used writer.kafka.valueField The field of the record to use as the value for writing to Kafka. Defaults to \"*\" which indicates that the entire record should be written. For nested records such as a pair of key, value, one would set the value of this configuration to the field-name for the value structure. What Next? You can now set up Kafka as the destination for any of your sources. All you have to do is set up the writer configuration correctly in your pull files. Happy Ingesting!","title":"Kafka"},{"location":"sinks/Kafka/#table-of-contents","text":"Table of Contents Introduction Pre-requisites Steps Configuration Details What Next?","title":"Table of Contents"},{"location":"sinks/Kafka/#introduction","text":"The Kafka writer allows users to create pipelines that ingest data from Gobblin sources into Kafka. This also enables Gobblin users to seamlessly transition their pipelines from ingesting directly to HDFS to ingesting into Kafka first, and then ingesting from Kafka to HDFS.","title":"Introduction"},{"location":"sinks/Kafka/#pre-requisites","text":"The following guide assumes that you are somewhat familiar with running Gobblin. If not, you should follow the Getting Started page first, then come back to this guide. Before you can use the Kafka writer, you need to set up a Kafka cluster to write to. You can follow any of the guides listed by the Kafka project such as the Apache Kafka quickstart guide .","title":"Pre-requisites"},{"location":"sinks/Kafka/#steps","text":"Edit the wikipedia-kafka.pull example to get started with setting up ingestion into Kafka. This is a very similar pipeline to the wikipedia.pull example which pulls pages from 5 titles from Wikipedia to HDFS. The main differences to note are: The writer.builder.class is set to gobblin.kafka.writer.KafkaDataWriterBuilder . This is the class that creates a Kafka writer. The writer.kafka.topic is set to WikipediaExample . This is the topic that the writer will write the records to. The writer.kafka.producerConfig.bootstrap.servers is set to localhost:9092 . This is the address of the kafka broker(s) that the writer must write to. There is no partitioner class specified. This implementation of the Kafka writer does not support partitioning and will use the default Kafka partitioner. The data.publisher.type is set to gobblin.publisher.NoopPublisher . This is because Kafka doesn't offer transactional semantics, so it isn't possible to have a separate publish step to finally commit the data. There is configuration for setting up the Schema Registry and Serializers that you will be using to write the data to Kafka. If you're using the Apache Kafka distribution, this file should work out of the box. If you're using the Confluent distribution and want to use the Confluent schema registry, comment out the Local Schema Registry section and un-comment the Confluent schema registry section. The result should match the text below for Confluent users. #Confluent Schema Registry and serializers writer.kafka.producerConfig.value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer writer.kafka.producerConfig.key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer writer.kafka.producerConfig.schema.registry.url=http://localhost:8081 #Set this to the correct schema-reg url ##Use Local Schema Registry and serializers #writer.kafka.producerConfig.value.serializer=org.apache.gobblin.kafka.serialize.LiAvroSerializer #writer.kafka.producerConfig.kafka.schemaRegistry.class=org.apache.gobblin.kafka.schemareg.ConfigDrivenMd5SchemaRegistry #writer.kafka.producerConfig.schemaRegistry.schema.name=WikipediaExample #writer.kafka.producerConfig.schemaRegistry.schema.value={ namespace : example.wikipedia.avro , type : record , name : WikipediaArticle , fields : [{ name : pageid , type : [ double , null ]},{ name : title , type : [ string , null ]},{ name : user , type : [ string , null ]},{ name : anon , type : [ string , null ]},{ name : userid , type : [ double , null ]},{ name : timestamp , type : [ string , null ]},{ name : size , type : [ double , null ]},{ name : contentformat , type : [ string , null ]},{ name : contentmodel , type : [ string , null ]},{ name : content , type : [ string , null ]}]} Run the standalone launcher with the wikipedia-kafka.pull file. You should see something like this. INFO [TaskExecutor-0] gobblin.example.wikipedia.WikipediaExtractor 243 - 5 record(s) retrieved for title LinkedIn INFO [TaskExecutor-0] gobblin.example.wikipedia.WikipediaExtractor 243 - 5 record(s) retrieved for title Parris_Cues INFO [TaskExecutor-0] gobblin.example.wikipedia.WikipediaExtractor 243 - 5 record(s) retrieved for title Barbara_Corcoran INFO [TaskExecutor-0] gobblin.runtime.Task 176 - Extracted 20 data records INFO [TaskExecutor-0] gobblin.runtime.Task 177 - Row quality checker finished with results: INFO [TaskExecutor-0] gobblin.publisher.TaskPublisher 43 - All components finished successfully, checking quality tests INFO [TaskExecutor-0] gobblin.publisher.TaskPublisher 45 - All required test passed for this task passed. INFO [TaskExecutor-0] gobblin.publisher.TaskPublisher 47 - Cleanup for task publisher executed successfully. INFO [TaskExecutor-0] gobblin.runtime.Fork 261 - Committing data for fork 0 of task task_PullFromWikipediaToKafka_1472246706122_0 INFO [TaskExecutor-0] gobblin.kafka.writer.KafkaDataWriter 211 - Successfully committed 20 records. To verify that the records have indeed been ingested into Kafka, you can run a kafka console consumer or run Gobblin's kafka-console pull file which prints the events from Kafka onto the console.","title":"Steps"},{"location":"sinks/Kafka/#configuration-details","text":"At this time, Gobblin supports integration with Kafka 0.8 and 0.9. The Kafka writer supports all the configuration parameters supported by the version-specific Kafka Producer (e.g. Latest Producer Configs ). All you have to do is prefix writer.kafka.producerConfig. to each configuration property that the producer supports. For example, if you want to set the acks parameter to all to ensure full acknowledgement of writes, you would set writer.kafka.producerConfig.acks=all in your pull file. For a comprehensive list of all the configuration properties supported by the producers, go through the official documentation here . Note: Since Gobblin is currently built against Kafka 0.8.2, the configuration options apply to the new 0.8.2 java producer. There are a few key parameters at the Gobblin level that control the behavior of the data writer. Property Name Semantics writer.kafka.topic The topic that the writer will be writing to. At this time, the writer can only write to a single topic per pipeline. writer.kafka.failureAllowancePercentage The percentage of failures that you are willing to tolerate while writing to Kafka. Gobblin will mark the workunit successful and move on if there are failures but not enough to trip the failure threshold. Only successfully acknowledged writes are counted as successful, all others are considered as failures. The default for the failureAllowancePercentage is set to 20.0. This means that as long as 80% of the data is acknowledged by Kafka, Gobblin will move on. If you want higher guarantees, set this config value to a lower value. e.g. If you want 99% delivery guarantees, set this value to 1.0 writer.kafka.commitTimeoutMillis The amount of time that the Gobblin committer will wait before abandoning its wait for unacknowledged writes. This defaults to 1 minute. writer.kafka.keyed When set to true, enables key-based writes to Kafka. This defaults to false. If you set this to true, make sure to set the keyField configuration property. Serialization of the key is controlled by the Kafka Producer specific configuration property ( writer.kafka.producerConfig.key.serializer ) writer.kafka.keyField The field of the record to use as the key for writing to Kafka. The field path follows a nested notation. So a top-level field \"name\" would be set to \"name\", a nested field \"name\" within a top-level struct \"header\" would be named \"header.name\" writer.kafka.typeMapperClass The class that the writer should use to extract keys and values from the input record. The default if not specified assumes that AvroGenericRecordTypeMapper is being used writer.kafka.valueField The field of the record to use as the value for writing to Kafka. Defaults to \"*\" which indicates that the entire record should be written. For nested records such as a pair of key, value, one would set the value of this configuration to the field-name for the value structure.","title":"Configuration Details"},{"location":"sinks/Kafka/#what-next","text":"You can now set up Kafka as the destination for any of your sources. All you have to do is set up the writer configuration correctly in your pull files. Happy Ingesting!","title":"What Next?"},{"location":"sinks/ParquetHdfsDataWriter/","text":"Description An extension to FsDataWriter that writes in Parquet format in the form of either Avro, Protobuf or ParquetGroup . This implementation allows users to specify the CodecFactory to use through the configuration property writer.codec.type . By default, the snappy codec is used. See Developer Notes to make sure you are using the right Gobblin jar. Usage writer.builder.class=org.apache.gobblin.writer.ParquetDataWriterBuilder writer.destination.type=HDFS writer.output.format=PARQUET Example Pipeline Configuration example-parquet.pull contains an example of generating test data and writing to Parquet files. Configuration Key Description Default Value Required writer.parquet.page.size The page size threshold. 1048576 No writer.parquet.dictionary.page.size The block size threshold for the dictionary pages. 134217728 No writer.parquet.dictionary To turn dictionary encoding on. Parquet has a dictionary encoding for data with a small number of unique values ( 10^5 ) that aids in significant compression and boosts processing speed. true No writer.parquet.validate To turn on validation using the schema. This validation is done by ParquetWriter not by Gobblin. false No writer.parquet.version Version of parquet writer to use. Available versions are v1 and v2. v1 No writer.parquet.format In-memory format of the record being written to Parquet. Options are AVRO, PROTOBUF and GROUP GROUP No Developer Notes Gobblin provides integration with two different versions of Parquet through its modules. Use the appropriate jar based on the Parquet library you use in your code. Jar Dependency Gobblin Release gobblin-parquet com.twitter:parquet-hadoop-bundle = 0.12.0 gobblin-parquet-apache org.apache.parquet:parquet-hadoop = 0.15.0 If you want to look at the code, check out: Module File gobblin-parquet ParquetHdfsDataWriter gobblin-parquet ParquetDataWriterBuilder gobblin-parquet-apache ParquetHdfsDataWriter gobblin-parquet-apache ParquetDataWriterBuilder","title":"Parquet HDFS"},{"location":"sinks/ParquetHdfsDataWriter/#description","text":"An extension to FsDataWriter that writes in Parquet format in the form of either Avro, Protobuf or ParquetGroup . This implementation allows users to specify the CodecFactory to use through the configuration property writer.codec.type . By default, the snappy codec is used. See Developer Notes to make sure you are using the right Gobblin jar.","title":"Description"},{"location":"sinks/ParquetHdfsDataWriter/#usage","text":"writer.builder.class=org.apache.gobblin.writer.ParquetDataWriterBuilder writer.destination.type=HDFS writer.output.format=PARQUET","title":"Usage"},{"location":"sinks/ParquetHdfsDataWriter/#example-pipeline-configuration","text":"example-parquet.pull contains an example of generating test data and writing to Parquet files.","title":"Example Pipeline Configuration"},{"location":"sinks/ParquetHdfsDataWriter/#configuration","text":"Key Description Default Value Required writer.parquet.page.size The page size threshold. 1048576 No writer.parquet.dictionary.page.size The block size threshold for the dictionary pages. 134217728 No writer.parquet.dictionary To turn dictionary encoding on. Parquet has a dictionary encoding for data with a small number of unique values ( 10^5 ) that aids in significant compression and boosts processing speed. true No writer.parquet.validate To turn on validation using the schema. This validation is done by ParquetWriter not by Gobblin. false No writer.parquet.version Version of parquet writer to use. Available versions are v1 and v2. v1 No writer.parquet.format In-memory format of the record being written to Parquet. Options are AVRO, PROTOBUF and GROUP GROUP No","title":"Configuration"},{"location":"sinks/ParquetHdfsDataWriter/#developer-notes","text":"Gobblin provides integration with two different versions of Parquet through its modules. Use the appropriate jar based on the Parquet library you use in your code. Jar Dependency Gobblin Release gobblin-parquet com.twitter:parquet-hadoop-bundle = 0.12.0 gobblin-parquet-apache org.apache.parquet:parquet-hadoop = 0.15.0 If you want to look at the code, check out: Module File gobblin-parquet ParquetHdfsDataWriter gobblin-parquet ParquetDataWriterBuilder gobblin-parquet-apache ParquetHdfsDataWriter gobblin-parquet-apache ParquetDataWriterBuilder","title":"Developer Notes"},{"location":"sinks/SimpleBytesWriter/","text":"Description A simple writer for byte arrays to a Hadoop file system file. The byte arrays can be optionally prefixed by a long-sized length and/or record delimiter byte. Usage writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder Configuration Key Type Description Default Value simple.writer.delimiter character An optional character to be used as records separator simple.writer.prepend.size boolean Enables/disables pre-pending the bytes written with a long size false","title":"HDFS Byte array"},{"location":"sinks/SimpleBytesWriter/#description","text":"A simple writer for byte arrays to a Hadoop file system file. The byte arrays can be optionally prefixed by a long-sized length and/or record delimiter byte.","title":"Description"},{"location":"sinks/SimpleBytesWriter/#usage","text":"writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder","title":"Usage"},{"location":"sinks/SimpleBytesWriter/#configuration","text":"Key Type Description Default Value simple.writer.delimiter character An optional character to be used as records separator simple.writer.prepend.size boolean Enables/disables pre-pending the bytes written with a long size false","title":"Configuration"},{"location":"sources/AvroFileSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Avro files"},{"location":"sources/AvroFileSource/#description","text":"TODO","title":"Description"},{"location":"sources/AvroFileSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/AvroFileSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/CopySource/","text":"Description TODO Usage TODO Configuration TODO","title":"File copy"},{"location":"sources/CopySource/#description","text":"TODO","title":"Description"},{"location":"sources/CopySource/#usage","text":"TODO","title":"Usage"},{"location":"sources/CopySource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/GoogleAnalyticsSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Google Analytics"},{"location":"sources/GoogleAnalyticsSource/#description","text":"TODO","title":"Description"},{"location":"sources/GoogleAnalyticsSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/GoogleAnalyticsSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/GoogleDriveSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Google Drive"},{"location":"sources/GoogleDriveSource/#description","text":"TODO","title":"Description"},{"location":"sources/GoogleDriveSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/GoogleDriveSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/GoogleWebmaster/","text":"Table of Contents Table of Contents Introduction Implementation Summary Entities Work Flow Configuration Introduction The Google Search Console data ingestion project is to download query and analytics data from Google Search Console for the purpose of doing search analytics of your verified sites. Available analytics measures are clicks, impressions, CTR and position. Used dimensions are dates, pages, countries and queries. Details about this Google service and API can be found at https://developers.google.com/webmaster-tools/ .This service can be run on a daily or weekly basis to download data at a daily granularity. Other useful links: API Java documentation: https://developers.google.com/resources/api-libraries/documentation/webmasters/v3/java/latest/ Google API Manager: https://console.developers.google.com/apis/dashboard Implementation Summary This connector implements sources, extractors, and iterators for the extractors, where each iterator is responsible for downloading data of each market. Due to the limitations of the Google API, the whole service has to deal with a lot of asynchronous API calls to figure out the problems like what is the total size of all unique pages what is the full list of all unique pages how to download queries and analytics data for each page how to improve the overall performance There are two implementations for this service to download analytics data for each page, V1 and V2. V1 is the initial design, which is very straight forward. After we get a full list of all unique URL pages, we send a request for the queries and analytics data for that page with a page filter saying that the page needs to exactly equals that page. However, if the amount of pages is large, for example, above 100,000 pages; given the actual API request speed(less than 4 pages/second), the amount of time to process all pages can easily go beyond 10 hours. So a faster version is in demand, the V2, which is to send requests based on tries. This greatly reduces the amount of requests that need to be sent. The idea is to group a bunch of pages sharing the common prefix together and send just one request for that page group by utilizing the page filter that contains the common prefix. In order to achieve this, we first need to save all pages into a URL trie and then implement a trie iterator to iterate through the trie to return groups of pages based on a group size. This new implementation can easily improve the performance by over 40 times. This graph can visually explain how the performance gain is achieved. In short, large group size can convert large percentage of pages into groups, each of which results in a single API call. The user still has the ability to choose which algorithm or implementation to use when starting the service by configuring the key source.google_webmasters.request.tuning.get_queries.apply_trie . Entities Here is a table explaining the responsibility of each class briefly Name Description GoogleWebmasterClient GoogleWebmasterClient provides basic accesses to Google Search Console by utilizing Google Webmaster API. GoogleWebmasterDataFetcher GoogleWebmasterDataFetcher implements the features to get all pages, and download analytics data (e.g. queries, clicks, impressions, CTR, position) for a given set of constraints like dates, pages, and countries. GoogleWebmasterFilter This is a util class providing enums and utility functions relevant to Google webmaster filters. GoogleWebMasterSource This is an abstract class that extends Gobblin's standard QueryBasedSource. It provides basic checks and configuration processing for google-webmaster-pull configuration files. GoogleWebMasterSourceDaily This implementation gives you the ability to do a daily extract from Google Search Console. GoogleWebmasterExtractor An implementation of Gobblin's extractor. It relies on a bunch of GoogleWebmasterExtractorIterator generated for each market to extract the data. GoogleWebmasterExtractorIterator The core piece used by GoogleWebmasterExtractor to iterate through the downloaded dataset. GoogleWebmasterDayPartitioner The output partitioner that partitions output by the date of fetched data set ProducerJob This is a partitionable request unit used by GoogleWebmasterExtractorIterator for sending detailed API requests to Google Search Console. It includes the filter dimensions like date range, page URL and page URL filter type(e.g. contains, non-contains, equals). These jobs are generated in a producer thread while requesting queries and analytics data for pages. They are placed into a ConcurrentLinkedDeque and dispatched or processed by a pool of working threads. The downloaded data will be put into a LinkedBlockingDeque which is shared with the GoogleWebmasterExtractorIterator. GoogleWebmasterExtractorIterator will then pass the data to GoogleWebmasterExtractor, and then to the rest of the Gobblin framework. It is an abstract class and currently has two implementations, SimpleProducerJob and TrieBasedProducerJob. It provides the default logic about how to partition a ProducerJob. When this producer job has a date range, then divide the job evenly into two minor producer jobs covering the original date range; otherwise, this producer job is not partitionable. SimpleProducerJob SimpleProducerJob is a basic implementation of ProducerJob, utilizing the default partition logic. TrieBasedProducerJob TrieBasedProducerJob is a trie-based implementation of ProducerJob. For the partition logic, it first tries to partition the pages by splitting the trie into smaller ones based on a new group size, which is a half of previous value. When it is not partitionable at the page level, the parition logic falls back to the default one provided by the base class. UrlTrie The trie that keeps all URL pages. Save all fetched pages into a trie in order to use the TrieBasedProducerJobs UrlTrieNode The trie node in the URL trie UrlTriePostOrderIterator This is a post-order iterator that traverses the nodes on the URL trie with a stopping rule, which is, it will not go deeper into the nodes whose size(defined as the number of descendant URLs and itself if itself is a URL page) is less than or equal to the stopping size. In other words, those nodes with size less than or equal to the stopping size will be treated as leaf nodes. UrlTriePrefixGrouper UrlTriePrefixGrouper will package the URL pages/nodes into groups given the group size while traversing the UrlTrie by utilizing a TrieIterator. If the current node is not a \"leaf\" node defined by the TrieIterator, then a \"fake\" group of size 1 will be created by only including this node. A group of URL pages will share the same common longest prefix and will be sent in one API request by using the \"containing\" page filter. A fake group containing only one page uses the \"equals\" page filter. Work Flow Starting with GoogleWebMasterSource, it consumes the job or pull configuration file, does some basic checks, and decides the date range to work on based on the type of GoogleWebMasterSource specified. Then it passes the date range and the list of markets to GoogleWebmasterExtractor to work on. The GoogleWebmasterExtractor will create a GoogleWebmasterExtractorIterator for each market and start the downloading process, which is the same for every market. For the downloading process, firstly, it figures out the size of the unique URL pages by utilizing GoogleWebmasterDataFetcher and GoogleWebmasterClient. Then it tries to get all unique pages and give warnings if not all pages can be found. Based on the version or underlying algorithm the user chooses to use, GoogleWebmasterExtractorIterator starts downloading queries and analytics data for each page and pass the data back to GoogleWebmasterExtractor in an asynchronous mode. Configuration Configuration Key Default Value Description source.google_webmasters.property_url Must Provide Provide the property site URL whose google search analytics data you want to download source.google_webmasters.request.filters Optional The filters that will be passed to all your API requests. Filter format is [GoogleWebmasterFilter.Dimension].[DimensionValue]. Currently, this filter operator is \"EQUALS\" and only Country dimension is supported. Will extend this feature according to more use cases in the future. source.google_webmasters.request.dimensions Must Provide Allowed dimensions are DATE, PAGE, COUNTRY, QUERY, DEVICE, SEARCH_TYPE, SEARCH_APPEARANCE source.google_webmasters.request.metrics Must Provide Allowed metrics are CLICKS, IMPRESSIONS, CTR, POSITION source.google_webmasters.request.page_limit 5000 The response row limits when you ask for pages. Set it to 5000 when you want to get all pages. Default to 5000, which is the maximum allowed. source.google_webmasters.request.query_limit 5000 The response row limits when you ask for queries. Default to 5000, which is the maximum allowed. source.google_webmasters.request.hot_start Optional Hot start this service with pre-set pages. Once this is set, the service will ignore source.google_webmasters.request.page_limit, and won't get all pages, but use the pre-set pages instead. This is useful for debugging or resuming your failed work. source.google_webmasters.request.tuning.get_queries.time_out 120 Set the time out in minutes for each round. source.google_webmasters.request.tuning.get_queries.max_retries 30 Tune the maximum rounds of retries allowed when API calls failed because of exceeding quota. source.google_webmasters.request.tuning.get_queries.cool_down_time 250 Tune the cool down time in millisecond between each round. source.google_webmasters.request.tuning.get_queries.batches_per_second 2.25 Tune the speed of API requests in batches source.google_webmasters.request.tuning.get_queries.batch_size 2 Tune the size of a batch. Batch API calls together to reduce the number of HTTP connections. Note: A set of n requests batched together counts toward your usage limit as n requests, not as one request. The batch request is taken apart into a set of requests before processing. Read more at https://developers.google.com/webmaster-tools/v3/how-tos/batch . source.google_webmasters.request.tuning.get_queries.trie_group_size 500 Set the group size for the URL trie source.google_webmasters.request.tuning.get_queries.apply_trie false Set to true to use the Trie based algorithm. Otherwise, set to false. If set to true, you also need to set page_limit to 5000 indicating that you want to get all pages because trie based algorithm won't give you expected results if you just need a subset of all pages. source.google_webmasters.request.tuning.get_pages.requests_per_second 5.0 Tune the speed of API requests while getting all pages. source.google_webmasters.request.tuning.get_pages.max_retries 120 Tune the number of maximum retries while getting all pages. Consider the following affecting factors while setting this number: the length of shared prefix path may be very long. the Quota Exceeded exception. source.google_webmasters.request.tuning.get_pages.time_out 2 Set the time out in minutes while getting all pages.","title":"Google Webmaster"},{"location":"sources/GoogleWebmaster/#table-of-contents","text":"Table of Contents Introduction Implementation Summary Entities Work Flow Configuration","title":"Table of Contents"},{"location":"sources/GoogleWebmaster/#introduction","text":"The Google Search Console data ingestion project is to download query and analytics data from Google Search Console for the purpose of doing search analytics of your verified sites. Available analytics measures are clicks, impressions, CTR and position. Used dimensions are dates, pages, countries and queries. Details about this Google service and API can be found at https://developers.google.com/webmaster-tools/ .This service can be run on a daily or weekly basis to download data at a daily granularity. Other useful links: API Java documentation: https://developers.google.com/resources/api-libraries/documentation/webmasters/v3/java/latest/ Google API Manager: https://console.developers.google.com/apis/dashboard","title":"Introduction"},{"location":"sources/GoogleWebmaster/#implementation","text":"","title":"Implementation"},{"location":"sources/GoogleWebmaster/#summary","text":"This connector implements sources, extractors, and iterators for the extractors, where each iterator is responsible for downloading data of each market. Due to the limitations of the Google API, the whole service has to deal with a lot of asynchronous API calls to figure out the problems like what is the total size of all unique pages what is the full list of all unique pages how to download queries and analytics data for each page how to improve the overall performance There are two implementations for this service to download analytics data for each page, V1 and V2. V1 is the initial design, which is very straight forward. After we get a full list of all unique URL pages, we send a request for the queries and analytics data for that page with a page filter saying that the page needs to exactly equals that page. However, if the amount of pages is large, for example, above 100,000 pages; given the actual API request speed(less than 4 pages/second), the amount of time to process all pages can easily go beyond 10 hours. So a faster version is in demand, the V2, which is to send requests based on tries. This greatly reduces the amount of requests that need to be sent. The idea is to group a bunch of pages sharing the common prefix together and send just one request for that page group by utilizing the page filter that contains the common prefix. In order to achieve this, we first need to save all pages into a URL trie and then implement a trie iterator to iterate through the trie to return groups of pages based on a group size. This new implementation can easily improve the performance by over 40 times. This graph can visually explain how the performance gain is achieved. In short, large group size can convert large percentage of pages into groups, each of which results in a single API call. The user still has the ability to choose which algorithm or implementation to use when starting the service by configuring the key source.google_webmasters.request.tuning.get_queries.apply_trie .","title":"Summary"},{"location":"sources/GoogleWebmaster/#entities","text":"Here is a table explaining the responsibility of each class briefly Name Description GoogleWebmasterClient GoogleWebmasterClient provides basic accesses to Google Search Console by utilizing Google Webmaster API. GoogleWebmasterDataFetcher GoogleWebmasterDataFetcher implements the features to get all pages, and download analytics data (e.g. queries, clicks, impressions, CTR, position) for a given set of constraints like dates, pages, and countries. GoogleWebmasterFilter This is a util class providing enums and utility functions relevant to Google webmaster filters. GoogleWebMasterSource This is an abstract class that extends Gobblin's standard QueryBasedSource. It provides basic checks and configuration processing for google-webmaster-pull configuration files. GoogleWebMasterSourceDaily This implementation gives you the ability to do a daily extract from Google Search Console. GoogleWebmasterExtractor An implementation of Gobblin's extractor. It relies on a bunch of GoogleWebmasterExtractorIterator generated for each market to extract the data. GoogleWebmasterExtractorIterator The core piece used by GoogleWebmasterExtractor to iterate through the downloaded dataset. GoogleWebmasterDayPartitioner The output partitioner that partitions output by the date of fetched data set ProducerJob This is a partitionable request unit used by GoogleWebmasterExtractorIterator for sending detailed API requests to Google Search Console. It includes the filter dimensions like date range, page URL and page URL filter type(e.g. contains, non-contains, equals). These jobs are generated in a producer thread while requesting queries and analytics data for pages. They are placed into a ConcurrentLinkedDeque and dispatched or processed by a pool of working threads. The downloaded data will be put into a LinkedBlockingDeque which is shared with the GoogleWebmasterExtractorIterator. GoogleWebmasterExtractorIterator will then pass the data to GoogleWebmasterExtractor, and then to the rest of the Gobblin framework. It is an abstract class and currently has two implementations, SimpleProducerJob and TrieBasedProducerJob. It provides the default logic about how to partition a ProducerJob. When this producer job has a date range, then divide the job evenly into two minor producer jobs covering the original date range; otherwise, this producer job is not partitionable. SimpleProducerJob SimpleProducerJob is a basic implementation of ProducerJob, utilizing the default partition logic. TrieBasedProducerJob TrieBasedProducerJob is a trie-based implementation of ProducerJob. For the partition logic, it first tries to partition the pages by splitting the trie into smaller ones based on a new group size, which is a half of previous value. When it is not partitionable at the page level, the parition logic falls back to the default one provided by the base class. UrlTrie The trie that keeps all URL pages. Save all fetched pages into a trie in order to use the TrieBasedProducerJobs UrlTrieNode The trie node in the URL trie UrlTriePostOrderIterator This is a post-order iterator that traverses the nodes on the URL trie with a stopping rule, which is, it will not go deeper into the nodes whose size(defined as the number of descendant URLs and itself if itself is a URL page) is less than or equal to the stopping size. In other words, those nodes with size less than or equal to the stopping size will be treated as leaf nodes. UrlTriePrefixGrouper UrlTriePrefixGrouper will package the URL pages/nodes into groups given the group size while traversing the UrlTrie by utilizing a TrieIterator. If the current node is not a \"leaf\" node defined by the TrieIterator, then a \"fake\" group of size 1 will be created by only including this node. A group of URL pages will share the same common longest prefix and will be sent in one API request by using the \"containing\" page filter. A fake group containing only one page uses the \"equals\" page filter.","title":"Entities"},{"location":"sources/GoogleWebmaster/#work-flow","text":"Starting with GoogleWebMasterSource, it consumes the job or pull configuration file, does some basic checks, and decides the date range to work on based on the type of GoogleWebMasterSource specified. Then it passes the date range and the list of markets to GoogleWebmasterExtractor to work on. The GoogleWebmasterExtractor will create a GoogleWebmasterExtractorIterator for each market and start the downloading process, which is the same for every market. For the downloading process, firstly, it figures out the size of the unique URL pages by utilizing GoogleWebmasterDataFetcher and GoogleWebmasterClient. Then it tries to get all unique pages and give warnings if not all pages can be found. Based on the version or underlying algorithm the user chooses to use, GoogleWebmasterExtractorIterator starts downloading queries and analytics data for each page and pass the data back to GoogleWebmasterExtractor in an asynchronous mode.","title":"Work Flow"},{"location":"sources/GoogleWebmaster/#configuration","text":"Configuration Key Default Value Description source.google_webmasters.property_url Must Provide Provide the property site URL whose google search analytics data you want to download source.google_webmasters.request.filters Optional The filters that will be passed to all your API requests. Filter format is [GoogleWebmasterFilter.Dimension].[DimensionValue]. Currently, this filter operator is \"EQUALS\" and only Country dimension is supported. Will extend this feature according to more use cases in the future. source.google_webmasters.request.dimensions Must Provide Allowed dimensions are DATE, PAGE, COUNTRY, QUERY, DEVICE, SEARCH_TYPE, SEARCH_APPEARANCE source.google_webmasters.request.metrics Must Provide Allowed metrics are CLICKS, IMPRESSIONS, CTR, POSITION source.google_webmasters.request.page_limit 5000 The response row limits when you ask for pages. Set it to 5000 when you want to get all pages. Default to 5000, which is the maximum allowed. source.google_webmasters.request.query_limit 5000 The response row limits when you ask for queries. Default to 5000, which is the maximum allowed. source.google_webmasters.request.hot_start Optional Hot start this service with pre-set pages. Once this is set, the service will ignore source.google_webmasters.request.page_limit, and won't get all pages, but use the pre-set pages instead. This is useful for debugging or resuming your failed work. source.google_webmasters.request.tuning.get_queries.time_out 120 Set the time out in minutes for each round. source.google_webmasters.request.tuning.get_queries.max_retries 30 Tune the maximum rounds of retries allowed when API calls failed because of exceeding quota. source.google_webmasters.request.tuning.get_queries.cool_down_time 250 Tune the cool down time in millisecond between each round. source.google_webmasters.request.tuning.get_queries.batches_per_second 2.25 Tune the speed of API requests in batches source.google_webmasters.request.tuning.get_queries.batch_size 2 Tune the size of a batch. Batch API calls together to reduce the number of HTTP connections. Note: A set of n requests batched together counts toward your usage limit as n requests, not as one request. The batch request is taken apart into a set of requests before processing. Read more at https://developers.google.com/webmaster-tools/v3/how-tos/batch . source.google_webmasters.request.tuning.get_queries.trie_group_size 500 Set the group size for the URL trie source.google_webmasters.request.tuning.get_queries.apply_trie false Set to true to use the Trie based algorithm. Otherwise, set to false. If set to true, you also need to set page_limit to 5000 indicating that you want to get all pages because trie based algorithm won't give you expected results if you just need a subset of all pages. source.google_webmasters.request.tuning.get_pages.requests_per_second 5.0 Tune the speed of API requests while getting all pages. source.google_webmasters.request.tuning.get_pages.max_retries 120 Tune the number of maximum retries while getting all pages. Consider the following affecting factors while setting this number: the length of shared prefix path may be very long. the Quota Exceeded exception. source.google_webmasters.request.tuning.get_pages.time_out 2 Set the time out in minutes while getting all pages.","title":"Configuration"},{"location":"sources/HadoopTextInputSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Hadoop Text Input"},{"location":"sources/HadoopTextInputSource/#description","text":"TODO","title":"Description"},{"location":"sources/HadoopTextInputSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/HadoopTextInputSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/HelloWorldSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Hello World"},{"location":"sources/HelloWorldSource/#description","text":"TODO","title":"Description"},{"location":"sources/HelloWorldSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/HelloWorldSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/HiveAvroToOrcSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Hive Avro-to-ORC"},{"location":"sources/HiveAvroToOrcSource/#description","text":"TODO","title":"Description"},{"location":"sources/HiveAvroToOrcSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/HiveAvroToOrcSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/HivePurgerSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Hive compliance purging"},{"location":"sources/HivePurgerSource/#description","text":"TODO","title":"Description"},{"location":"sources/HivePurgerSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/HivePurgerSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/HiveSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Description"},{"location":"sources/HiveSource/#description","text":"TODO","title":"Description"},{"location":"sources/HiveSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/HiveSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/KafkaSource/","text":"Description See Kafka-to-HDFS case study Usage TODO Configuration TODO","title":"Kafka"},{"location":"sources/KafkaSource/#description","text":"See Kafka-to-HDFS case study","title":"Description"},{"location":"sources/KafkaSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/KafkaSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/MySQLSource/","text":"Description TODO Usage TODO Configuration TODO","title":"MySQL"},{"location":"sources/MySQLSource/#description","text":"TODO","title":"Description"},{"location":"sources/MySQLSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/MySQLSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/OracleSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Oracle"},{"location":"sources/OracleSource/#description","text":"TODO","title":"Description"},{"location":"sources/OracleSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/OracleSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/QueryBasedSource/","text":"Introduction Constructs QueryBasedSource QueryBasedExtractor build readRecord Configuration Introduction QueryBasedSource represents a category of sources whose data is pulled by sending queries. A dataset of a source is identified as a SourceEntity . Query can be done by sending HTTP requests or SQL commands. A source often, but not always, has a corresponding QueryBasedExtractor , which defines the way and implements common routines to extract data from the source. Constructs QueryBasedSource Figure 1: Query based sources Like other categories of sources, a QueryBasedSource focuses on creating work units as well. The way it does follows the general pattern: calculate low watermark of current run based on previous runs compute a high watermark partition datasets of current run into work units pick up previously failed work units. At last, it will group several work units as MultiWorkUnit according to the mr.job.max.mappers configuration (Note: other categories of source might have a different approach to group work units into MultiWorkUnit ). QueryBasedExtractor Figure 2: Query based extractors Currently in Gobblin, depending on how an extractor communicates with a source (or different communication protocols ), a QueryBasedExtractor falls into 2 categories: RestApiExtractor and JdbcExtractor . A specific extractor has to provide some source specific logic in order to successfully extract information from the source. build Building a query based extractor may involve three queries: Figure 3: Query based extractor build queries extractMetadata sends a query to fetch the data schema. For example: select col.column_name, col.data_type, case when CHARACTER_OCTET_LENGTH is null then 0 else 0 end as length, case when NUMERIC_PRECISION is null then 0 else NUMERIC_PRECISION end as precesion, case when NUMERIC_SCALE is null then 0 else NUMERIC_SCALE end as scale, case when is_nullable='NO' then 'false' else 'true' end as nullable, '' as format, case when col.column_comment is null then '' else col.column_comment end as comment from information_schema.COLUMNS col WHERE upper(col.table_name)=upper(?) AND upper(col.table_schema)=upper(?) order by col.ORDINAL_POSITION getMaxWatermark sends a query for calculating the latest high watermark. For example: SELECT max(SystemModTime) FROM salesforce.campaign where (SystemModTime = '2014-01-01 00:00:00' and SystemModTime = '2017-03-09 10:42:10') getSourceCount sends a query for the total count of records to be pulled from the source. For example: SELECT COUNT(1) FROM salesforce.campaign where (SystemModTime = '2014-01-01 00:00:00' and SystemModTime = '2017-03-01 19:03:07') The actual implementations of those methods are pushed to an upper layer, which uses its own protocol(e.g. Rest Api or Jdbc. The examples given are using Jdbc.) to query the source. readRecord While querying the record set for the last work unit, the upper bounds will be removed if appropriate. For a daily open-ended full dump job, it will fetch a more complete data set as there might be some new data generated or existing data changes between the data query creation and execution. Two separate approaches to fetch record set: getRecordSet : A standard way to send a query, e.g. Rest api or Jdbc SELECT id,name,budget,systemmodtime FROM salesforce.campaign where (SystemModTime = '2014-01-01 00:00:00') getRecordSetFromSourceApi : A specific way to send a query based on source api, e.g. Salesforce Likewise, the actual implementations of those methods are pushed to an upper layer. See chapters: Rest Api , Salesforce . Configuration Configuration Key Default Value Description source.querybased.schema Must Provide Database name source.entity Must Provide Name of the source entity that will be pulled from the source. It could be a database table, a source topic, a restful entity, etc. source.max.number.of.partitions 20 Maximum number of partitions or work units to split this current run across. Only used by the QueryBasedSource and FileBasedSource . source.querybased.watermark.type Must Provide The format of the watermark that is used when extracting data from the source. Possible types are timestamp, date, hour, simple. source.querybased.start.value Must provide Value for the watermark to start pulling data from, also the default watermark if the previous watermark cannot be found in the old task states. source.querybased.end.value Optional The high watermark the job should pull up to. extract.delta.fields Optional List of columns that are associated with the watermark. source.querybased.skip.count.calc False Disable calculating the total counts of records to be pulled from the source. source.querybased.is.specific.api.active False True if this pull needs to use source specific apis instead of standard protocols. Ex: Use salesforce bulk api instead of rest api","title":"Query based"},{"location":"sources/QueryBasedSource/#introduction","text":"QueryBasedSource represents a category of sources whose data is pulled by sending queries. A dataset of a source is identified as a SourceEntity . Query can be done by sending HTTP requests or SQL commands. A source often, but not always, has a corresponding QueryBasedExtractor , which defines the way and implements common routines to extract data from the source.","title":"Introduction"},{"location":"sources/QueryBasedSource/#constructs","text":"","title":"Constructs"},{"location":"sources/QueryBasedSource/#querybasedsource","text":"Figure 1: Query based sources Like other categories of sources, a QueryBasedSource focuses on creating work units as well. The way it does follows the general pattern: calculate low watermark of current run based on previous runs compute a high watermark partition datasets of current run into work units pick up previously failed work units. At last, it will group several work units as MultiWorkUnit according to the mr.job.max.mappers configuration (Note: other categories of source might have a different approach to group work units into MultiWorkUnit ).","title":"QueryBasedSource"},{"location":"sources/QueryBasedSource/#querybasedextractor","text":"Figure 2: Query based extractors Currently in Gobblin, depending on how an extractor communicates with a source (or different communication protocols ), a QueryBasedExtractor falls into 2 categories: RestApiExtractor and JdbcExtractor . A specific extractor has to provide some source specific logic in order to successfully extract information from the source.","title":"QueryBasedExtractor"},{"location":"sources/QueryBasedSource/#build","text":"Building a query based extractor may involve three queries: Figure 3: Query based extractor build queries extractMetadata sends a query to fetch the data schema. For example: select col.column_name, col.data_type, case when CHARACTER_OCTET_LENGTH is null then 0 else 0 end as length, case when NUMERIC_PRECISION is null then 0 else NUMERIC_PRECISION end as precesion, case when NUMERIC_SCALE is null then 0 else NUMERIC_SCALE end as scale, case when is_nullable='NO' then 'false' else 'true' end as nullable, '' as format, case when col.column_comment is null then '' else col.column_comment end as comment from information_schema.COLUMNS col WHERE upper(col.table_name)=upper(?) AND upper(col.table_schema)=upper(?) order by col.ORDINAL_POSITION getMaxWatermark sends a query for calculating the latest high watermark. For example: SELECT max(SystemModTime) FROM salesforce.campaign where (SystemModTime = '2014-01-01 00:00:00' and SystemModTime = '2017-03-09 10:42:10') getSourceCount sends a query for the total count of records to be pulled from the source. For example: SELECT COUNT(1) FROM salesforce.campaign where (SystemModTime = '2014-01-01 00:00:00' and SystemModTime = '2017-03-01 19:03:07') The actual implementations of those methods are pushed to an upper layer, which uses its own protocol(e.g. Rest Api or Jdbc. The examples given are using Jdbc.) to query the source.","title":"build"},{"location":"sources/QueryBasedSource/#readrecord","text":"While querying the record set for the last work unit, the upper bounds will be removed if appropriate. For a daily open-ended full dump job, it will fetch a more complete data set as there might be some new data generated or existing data changes between the data query creation and execution. Two separate approaches to fetch record set: getRecordSet : A standard way to send a query, e.g. Rest api or Jdbc SELECT id,name,budget,systemmodtime FROM salesforce.campaign where (SystemModTime = '2014-01-01 00:00:00') getRecordSetFromSourceApi : A specific way to send a query based on source api, e.g. Salesforce Likewise, the actual implementations of those methods are pushed to an upper layer. See chapters: Rest Api , Salesforce .","title":"readRecord"},{"location":"sources/QueryBasedSource/#configuration","text":"Configuration Key Default Value Description source.querybased.schema Must Provide Database name source.entity Must Provide Name of the source entity that will be pulled from the source. It could be a database table, a source topic, a restful entity, etc. source.max.number.of.partitions 20 Maximum number of partitions or work units to split this current run across. Only used by the QueryBasedSource and FileBasedSource . source.querybased.watermark.type Must Provide The format of the watermark that is used when extracting data from the source. Possible types are timestamp, date, hour, simple. source.querybased.start.value Must provide Value for the watermark to start pulling data from, also the default watermark if the previous watermark cannot be found in the old task states. source.querybased.end.value Optional The high watermark the job should pull up to. extract.delta.fields Optional List of columns that are associated with the watermark. source.querybased.skip.count.calc False Disable calculating the total counts of records to be pulled from the source. source.querybased.is.specific.api.active False True if this pull needs to use source specific apis instead of standard protocols. Ex: Use salesforce bulk api instead of rest api","title":"Configuration"},{"location":"sources/RestApiSource/","text":"Introduction Constructs RestApiSource RestApiExtractor Configuration Introduction A RestApiSource is a QueryBasedSource which uses RESTful Api for query. RestApiExtractor is a QueryBasedExtractor that uses REST to communicate with the source. To establish the communication, a RestApiConnector is required. Constructs RestApiSource Coming soon... RestApiExtractor A RestApiExtractor sets up the common routines to query information from a REST source, for example, extractMetadata , getMaxWatermark , getSourceCount , getRecordSet , which are mentioned in chapter QueryBasedSource . In terms of constructing the actual query and extracting the data from the response, the source specific layer holds the truth, for example, SalesforceExtractor . A simplified general flow of routines is depicted in Figure 1: Figure 1: RestApiExtractor general routine flow Depends on the routines, [getX], [constructGetXQuery], [extractXFromResponse] are Description [getX] [constructGetXQuery] [extractXFromResponse] Get data schema extractMetadata getSchemaMetadata getSchema Calculate latest high watermark getMaxWatermark getHighWatermarkMetadata getHighWatermark Get total counts of records to be pulled getSourceCount getCountMetadata getCount Get records getRecordSet getDataMetadata getData There are other interactions between the RestApiExtractor layer and SourceSpecificLayer . The key points are: A ProtocolSpecificLayer , such as RestApiExtractor , understands the protocol and sets up a routine to communicate with the source A SourceSpecificLayer , such as SalesforceExtractor , knows the source and fits into the routine by providing and analyzing source specific information Configuration Configuration Key Default Value Description source.querybased.query Optional The query that the extractor should execute to pull data. source.querybased.excluded.columns Options Names of columns excluded while pulling data. extract.delta.fields Optional List of columns that are associated with the watermark. extract.primary.key.fields Optional List of columns that will be used as the primary key for the data.","title":"Rest Api"},{"location":"sources/RestApiSource/#introduction","text":"A RestApiSource is a QueryBasedSource which uses RESTful Api for query. RestApiExtractor is a QueryBasedExtractor that uses REST to communicate with the source. To establish the communication, a RestApiConnector is required.","title":"Introduction"},{"location":"sources/RestApiSource/#constructs","text":"","title":"Constructs"},{"location":"sources/RestApiSource/#restapisource","text":"Coming soon...","title":"RestApiSource"},{"location":"sources/RestApiSource/#restapiextractor","text":"A RestApiExtractor sets up the common routines to query information from a REST source, for example, extractMetadata , getMaxWatermark , getSourceCount , getRecordSet , which are mentioned in chapter QueryBasedSource . In terms of constructing the actual query and extracting the data from the response, the source specific layer holds the truth, for example, SalesforceExtractor . A simplified general flow of routines is depicted in Figure 1: Figure 1: RestApiExtractor general routine flow Depends on the routines, [getX], [constructGetXQuery], [extractXFromResponse] are Description [getX] [constructGetXQuery] [extractXFromResponse] Get data schema extractMetadata getSchemaMetadata getSchema Calculate latest high watermark getMaxWatermark getHighWatermarkMetadata getHighWatermark Get total counts of records to be pulled getSourceCount getCountMetadata getCount Get records getRecordSet getDataMetadata getData There are other interactions between the RestApiExtractor layer and SourceSpecificLayer . The key points are: A ProtocolSpecificLayer , such as RestApiExtractor , understands the protocol and sets up a routine to communicate with the source A SourceSpecificLayer , such as SalesforceExtractor , knows the source and fits into the routine by providing and analyzing source specific information","title":"RestApiExtractor"},{"location":"sources/RestApiSource/#configuration","text":"Configuration Key Default Value Description source.querybased.query Optional The query that the extractor should execute to pull data. source.querybased.excluded.columns Options Names of columns excluded while pulling data. extract.delta.fields Optional List of columns that are associated with the watermark. extract.primary.key.fields Optional List of columns that will be used as the primary key for the data.","title":"Configuration"},{"location":"sources/SalesforceSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Salesforce"},{"location":"sources/SalesforceSource/#description","text":"TODO","title":"Description"},{"location":"sources/SalesforceSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/SalesforceSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/SftpSource/","text":"Description TODO Usage TODO Configuration TODO","title":"SFTP"},{"location":"sources/SftpSource/#description","text":"TODO","title":"Description"},{"location":"sources/SftpSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/SftpSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/SimpleJsonSource/","text":"Description TODO Usage TODO Configuration TODO","title":"JSON"},{"location":"sources/SimpleJsonSource/#description","text":"TODO","title":"Description"},{"location":"sources/SimpleJsonSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/SimpleJsonSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/SqlServerSource/","text":"Description TODO Usage TODO Configuration TODO","title":"SQL Server"},{"location":"sources/SqlServerSource/#description","text":"TODO","title":"Description"},{"location":"sources/SqlServerSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/SqlServerSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/TeradataSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Teradata"},{"location":"sources/TeradataSource/#description","text":"TODO","title":"Description"},{"location":"sources/TeradataSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/TeradataSource/#configuration","text":"TODO","title":"Configuration"},{"location":"sources/WikipediaSource/","text":"Description TODO Usage TODO Configuration TODO","title":"Wikipedia"},{"location":"sources/WikipediaSource/#description","text":"TODO","title":"Description"},{"location":"sources/WikipediaSource/#usage","text":"TODO","title":"Usage"},{"location":"sources/WikipediaSource/#configuration","text":"TODO","title":"Configuration"},{"location":"user-guide/Azure-Kubernetes-Deployment/","text":"GaaS on Azure Deployment Steps Create Azure Container Registry [Optional] 1) Log into Azure Container Registry $ az acr login --name gobblintest 2) Tag docker images to container registry $ docker tag gaas_image_id gobblintest.azurecr.io/gobblin-service $ docker tag standalone_image_id gobblintest.azurecr.io/gobblin-standalone 3) Push the images $ docker push gobblintest.azurecr.io/gobblin-service $ docker push gobblintest.azurecr.io/gobblin-standalone The images should now be hosted on azure with the tag:latest Deploy the base K8s cluster 1) Create a resource group on Azure 2) Create a cluster and deploy it onto the resource group az aks create --resource-group resource_group_name --name GaaS-cluster-test --node-count 1 --enable-addons monitoring --generate-ssh-keys 3) Switch kubectl to use azure 4) Check status of cluster $ kubectl get pods Install the nginx ingress to connect to the Azure Cluster 1) Install helm if you don't currently have it brew install helm helm init 2) Deploy the nginx helm chart to create the ingress helm install stable/nginx-ingress If this is the first time deploying helm (v2.0), you will need to set up the tiller, which is a helm serviceaccount with sudo permissions that lives inside of the cluster. Otherwise you'll run into this issue . Error: configmaps is forbidden: User \"system:serviceaccount:kube-system:default\" cannot list configmaps in the namespace \"kube-system\" To set up the tiller (steps are also found in the issue link) kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl edit deploy --namespace kube-system tiller-deploy #and add the line serviceAccount: tiller to spec/template/spec 3) Deploy the ingress controller in gobblin-kubernetes/gobblin-service/azure-cluster 4) Run kubectl get services , and the output should look something like this: gaas-svc ClusterIP 10.0.176.58 none 6956/TCP 16h honorary-possum-nginx-ingress-controller LoadBalancer 10.0.182.255 EXTERNAL_IP 80:30488/TCP,443:31835/TCP 6m13s honorary-possum-nginx-ingress-default-backend ClusterIP 10.0.236.153 none 80/TCP 6m13s kubernetes ClusterIP 10.0.0.1 none 443/TCP 10d 5) Send a request to the IP for the honorary-possum-nginx-ingress-controller","title":"GaaS on Azure Deployment Steps"},{"location":"user-guide/Azure-Kubernetes-Deployment/#gaas-on-azure-deployment-steps","text":"","title":"GaaS on Azure Deployment Steps"},{"location":"user-guide/Azure-Kubernetes-Deployment/#create-azure-container-registry-optional","text":"1) Log into Azure Container Registry $ az acr login --name gobblintest 2) Tag docker images to container registry $ docker tag gaas_image_id gobblintest.azurecr.io/gobblin-service $ docker tag standalone_image_id gobblintest.azurecr.io/gobblin-standalone 3) Push the images $ docker push gobblintest.azurecr.io/gobblin-service $ docker push gobblintest.azurecr.io/gobblin-standalone The images should now be hosted on azure with the tag:latest","title":"Create Azure Container Registry [Optional]"},{"location":"user-guide/Azure-Kubernetes-Deployment/#deploy-the-base-k8s-cluster","text":"1) Create a resource group on Azure 2) Create a cluster and deploy it onto the resource group az aks create --resource-group resource_group_name --name GaaS-cluster-test --node-count 1 --enable-addons monitoring --generate-ssh-keys 3) Switch kubectl to use azure 4) Check status of cluster $ kubectl get pods","title":"Deploy the base K8s cluster"},{"location":"user-guide/Azure-Kubernetes-Deployment/#install-the-nginx-ingress-to-connect-to-the-azure-cluster","text":"1) Install helm if you don't currently have it brew install helm helm init 2) Deploy the nginx helm chart to create the ingress helm install stable/nginx-ingress If this is the first time deploying helm (v2.0), you will need to set up the tiller, which is a helm serviceaccount with sudo permissions that lives inside of the cluster. Otherwise you'll run into this issue . Error: configmaps is forbidden: User \"system:serviceaccount:kube-system:default\" cannot list configmaps in the namespace \"kube-system\" To set up the tiller (steps are also found in the issue link) kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl edit deploy --namespace kube-system tiller-deploy #and add the line serviceAccount: tiller to spec/template/spec 3) Deploy the ingress controller in gobblin-kubernetes/gobblin-service/azure-cluster 4) Run kubectl get services , and the output should look something like this: gaas-svc ClusterIP 10.0.176.58 none 6956/TCP 16h honorary-possum-nginx-ingress-controller LoadBalancer 10.0.182.255 EXTERNAL_IP 80:30488/TCP,443:31835/TCP 6m13s honorary-possum-nginx-ingress-default-backend ClusterIP 10.0.236.153 none 80/TCP 6m13s kubernetes ClusterIP 10.0.0.1 none 443/TCP 10d 5) Send a request to the IP for the honorary-possum-nginx-ingress-controller","title":"Install the nginx ingress to connect to the Azure Cluster"},{"location":"user-guide/Building-Gobblin-as-a-Service/","text":"Table of Contents Table of Contents Introduction Running Gobblin as a Service Running Gobblin as a Service with Docker Running Gobblin as a Service with Kubernetes Introduction Gobblin as a service is a service that takes in a user request (a logical flow) and converts it into a series of Gobblin Jobs, and monitors these jobs in a distributed manner. The design of the service can be found here: https://cwiki.apache.org/confluence/display/GOBBLIN/Gobblin+as+a+Service Running Gobblin as a Service [Build Gobblin] (./Building-Gobblin.md) or use one of the [provided distributions] (https://github.com/apache/incubator-gobblin/releases) Untar the build file tar -xvf apache-gobblin-incubating-bin-${GOBBLIN_VERSION}.tar.gz Execute the start script ./gobblin-dist/bin/gobblin-service.sh View output in service.out Currently the setup only runs a portion of the service, but work will be done to have a basic end-to-end workflow soon. The service can now be accessed on localhost:6956 Running Gobblin as a Service with Docker There are also Dockerfiles to create new images of Gobblin based on the source code that can be easily run independently. The Docker compose is set up to easily create a working end-to-end workflow of Gobblin as a Service, which communicates Gobblin Standalone through a local volume filesystem. To run the full docker compose: export GOBBLIN_ROOT_DIR= root_directory_of_gobblin export LOCAL_DATAPACK_DIR= local_directory_of_templateUris export LOCAL_JOB_DIR= local_directory_to_read_and_write_jobs docker compose -f gobblin-docker/gobblin-service/alpine-gaas-latest/docker-compose.yml build docker compose -f gobblin-docker/gobblin-service/alpine-gaas-latest/docker-compose.yml up The docker container exposes the endpoints from Gobblin as a Service which can be accessed on localhost:6956 Running Gobblin as a Service with Kubernetes Gobblin as a service also has a kubernetes cluster, which can be deployed to any K8s environment. Currently, the yamls use Kustomize for configuration management. In the future, we may utilise Helm instead. To cluster is split into 3 environments 1) base-cluster (deploys one pod of GaaS and Gobblin standalone, where GaaS writes jobSpecs to a folder tracked by the standalone instance) 2) mysql-cluster (utilises MySQL for storing specStores instead of FS, future work may involve writing to a job queue to be picked by gobblin standalone) 3) azure-cluster (deploys Dev on Microsoft Azure), more docs here To add any flow config template for GaaS to use, add the .template file to gobblin-kubernetes/gobblin-service/base-cluster/ and add the file to the configmap. For production purposes, flow config templates should be stored in a proper file system or a database instead of being added to the configmap. To deploy any of these clusters, run the following command from the repository root. kubectl apply -k gobblin-kubernetes/gobblin-service/ ENV / There, find the external IP of the cluster and start sending requests.","title":"Building Gobblin as a Service"},{"location":"user-guide/Building-Gobblin-as-a-Service/#table-of-contents","text":"Table of Contents Introduction Running Gobblin as a Service Running Gobblin as a Service with Docker Running Gobblin as a Service with Kubernetes","title":"Table of Contents"},{"location":"user-guide/Building-Gobblin-as-a-Service/#introduction","text":"Gobblin as a service is a service that takes in a user request (a logical flow) and converts it into a series of Gobblin Jobs, and monitors these jobs in a distributed manner. The design of the service can be found here: https://cwiki.apache.org/confluence/display/GOBBLIN/Gobblin+as+a+Service","title":"Introduction"},{"location":"user-guide/Building-Gobblin-as-a-Service/#running-gobblin-as-a-service","text":"[Build Gobblin] (./Building-Gobblin.md) or use one of the [provided distributions] (https://github.com/apache/incubator-gobblin/releases) Untar the build file tar -xvf apache-gobblin-incubating-bin-${GOBBLIN_VERSION}.tar.gz Execute the start script ./gobblin-dist/bin/gobblin-service.sh View output in service.out Currently the setup only runs a portion of the service, but work will be done to have a basic end-to-end workflow soon. The service can now be accessed on localhost:6956","title":"Running Gobblin as a Service"},{"location":"user-guide/Building-Gobblin-as-a-Service/#running-gobblin-as-a-service-with-docker","text":"There are also Dockerfiles to create new images of Gobblin based on the source code that can be easily run independently. The Docker compose is set up to easily create a working end-to-end workflow of Gobblin as a Service, which communicates Gobblin Standalone through a local volume filesystem. To run the full docker compose: export GOBBLIN_ROOT_DIR= root_directory_of_gobblin export LOCAL_DATAPACK_DIR= local_directory_of_templateUris export LOCAL_JOB_DIR= local_directory_to_read_and_write_jobs docker compose -f gobblin-docker/gobblin-service/alpine-gaas-latest/docker-compose.yml build docker compose -f gobblin-docker/gobblin-service/alpine-gaas-latest/docker-compose.yml up The docker container exposes the endpoints from Gobblin as a Service which can be accessed on localhost:6956","title":"Running Gobblin as a Service with Docker"},{"location":"user-guide/Building-Gobblin-as-a-Service/#running-gobblin-as-a-service-with-kubernetes","text":"Gobblin as a service also has a kubernetes cluster, which can be deployed to any K8s environment. Currently, the yamls use Kustomize for configuration management. In the future, we may utilise Helm instead. To cluster is split into 3 environments 1) base-cluster (deploys one pod of GaaS and Gobblin standalone, where GaaS writes jobSpecs to a folder tracked by the standalone instance) 2) mysql-cluster (utilises MySQL for storing specStores instead of FS, future work may involve writing to a job queue to be picked by gobblin standalone) 3) azure-cluster (deploys Dev on Microsoft Azure), more docs here To add any flow config template for GaaS to use, add the .template file to gobblin-kubernetes/gobblin-service/base-cluster/ and add the file to the configmap. For production purposes, flow config templates should be stored in a proper file system or a database instead of being added to the configmap. To deploy any of these clusters, run the following command from the repository root. kubectl apply -k gobblin-kubernetes/gobblin-service/ ENV / There, find the external IP of the cluster and start sending requests.","title":"Running Gobblin as a Service with Kubernetes"},{"location":"user-guide/Building-Gobblin/","text":"Table of Contents Table of Contents Introduction Options Versions Hadoop Version Hive Version Pegasus Version Byteman Version Exclude Hadoop Dependencies from gobblin-dist.tar.gz Exclude Hive Dependencies from gobblin-dist.tar.gz Custom Gradle Tasks Print Project Dependencies Useful Gradle Commands Introduction This page outlines all the options that can be specified when building Gobblin using Gradle. The typical way of building Gobblin is to first checkout the code-base from GitHub and then build the code-base using Gradle. git clone https://github.com/apache/incubator-gobblin.git cd gobblin ./gradlew assemble If one wants to compile the code as well as run the tests, use ./gradle assemble test or ./gradlew build . There are a number of parameters that can be passed into the above command to customize the build process. Options These options just need to be added to the command above to take effect. Versions Hadoop Version The Hadoop version can be specified by adding the option -PhadoopVersion=[my-hadoop-version] . Hive Version The Hive version can be specified by adding the option -PhiveVersion=[my-hive-version] . Pegasus Version The Pegasus version can be specified by adding the option -PpegasusVersion=[my-pegasus-version] . Byteman Version The Byteman version can be specified by adding the option -PbytemanVersion=[my-byteman-version] . Exclude Hadoop Dependencies from gobblin-dist.tar.gz Add the option -PexcludeHadoopDeps to exclude all Hadoop libraries from gobblin-dist.tar.gz . Exclude Hive Dependencies from gobblin-dist.tar.gz Add the option -PexcludeHiveDeps to exclude all Hive libraries from gobblin-dist.tar.gz . Custom Gradle Tasks A few custom built Gradle tasks. Print Project Dependencies Executing this command will print out all the dependencies between the different Gobblin Gradle sub-projects: ./gradlew dotProjectDependencies . Useful Gradle Commands These commands make working with Gradle a little easier.","title":"Building Gobblin"},{"location":"user-guide/Building-Gobblin/#table-of-contents","text":"Table of Contents Introduction Options Versions Hadoop Version Hive Version Pegasus Version Byteman Version Exclude Hadoop Dependencies from gobblin-dist.tar.gz Exclude Hive Dependencies from gobblin-dist.tar.gz Custom Gradle Tasks Print Project Dependencies Useful Gradle Commands","title":"Table of Contents"},{"location":"user-guide/Building-Gobblin/#introduction","text":"This page outlines all the options that can be specified when building Gobblin using Gradle. The typical way of building Gobblin is to first checkout the code-base from GitHub and then build the code-base using Gradle. git clone https://github.com/apache/incubator-gobblin.git cd gobblin ./gradlew assemble If one wants to compile the code as well as run the tests, use ./gradle assemble test or ./gradlew build . There are a number of parameters that can be passed into the above command to customize the build process.","title":"Introduction"},{"location":"user-guide/Building-Gobblin/#options","text":"These options just need to be added to the command above to take effect.","title":"Options"},{"location":"user-guide/Building-Gobblin/#versions","text":"","title":"Versions"},{"location":"user-guide/Building-Gobblin/#hadoop-version","text":"The Hadoop version can be specified by adding the option -PhadoopVersion=[my-hadoop-version] .","title":"Hadoop Version"},{"location":"user-guide/Building-Gobblin/#hive-version","text":"The Hive version can be specified by adding the option -PhiveVersion=[my-hive-version] .","title":"Hive Version"},{"location":"user-guide/Building-Gobblin/#pegasus-version","text":"The Pegasus version can be specified by adding the option -PpegasusVersion=[my-pegasus-version] .","title":"Pegasus Version"},{"location":"user-guide/Building-Gobblin/#byteman-version","text":"The Byteman version can be specified by adding the option -PbytemanVersion=[my-byteman-version] .","title":"Byteman Version"},{"location":"user-guide/Building-Gobblin/#exclude-hadoop-dependencies-from-gobblin-disttargz","text":"Add the option -PexcludeHadoopDeps to exclude all Hadoop libraries from gobblin-dist.tar.gz .","title":"Exclude Hadoop Dependencies from gobblin-dist.tar.gz"},{"location":"user-guide/Building-Gobblin/#exclude-hive-dependencies-from-gobblin-disttargz","text":"Add the option -PexcludeHiveDeps to exclude all Hive libraries from gobblin-dist.tar.gz .","title":"Exclude Hive Dependencies from gobblin-dist.tar.gz"},{"location":"user-guide/Building-Gobblin/#custom-gradle-tasks","text":"A few custom built Gradle tasks.","title":"Custom Gradle Tasks"},{"location":"user-guide/Building-Gobblin/#print-project-dependencies","text":"Executing this command will print out all the dependencies between the different Gobblin Gradle sub-projects: ./gradlew dotProjectDependencies .","title":"Print Project Dependencies"},{"location":"user-guide/Building-Gobblin/#useful-gradle-commands","text":"These commands make working with Gradle a little easier.","title":"Useful Gradle Commands"},{"location":"user-guide/Compaction/","text":"Table of Contents Table of Contents MapReduce Compactor Example Use Case Basic Usage Non-deduping Compaction via Map-only Jobs Handling Late Records Verifying Data Completeness Before Compaction Hive Compactor Basic Usage Global Config Properties (example: compaction.properties) Job Config Properties (example: jobconf/task1.conf) Running a Compaction Job Compaction can be used to post-process files pulled by Gobblin with certain semantics. Deduplication is one of the common reasons to do compaction, e.g., you may want to deduplicate on all fields of the records. deduplicate on key fields of the records, keep the one with the latest timestamp for records with the same key. This is because duplicates can be generated for multiple reasons including both intended and unintended: For ingestion from data sources with mutable records (e.g., relational databases), instead of ingesting a full snapshot of a table every time, one may wish to ingest only the records that were changed since the previous run (i.e., delta records), and merge these delta records with previously generated snapshots in a compaction. In this case, for records with the same primary key, the one with the latest timestamp should be kept. The data source you ingest from may have duplicate records, e.g., if you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during the replication. In some data sources duplicate records may also be produced by the data producer. In rare circumstances, Gobblin may pull the same data twice, thus creating duplicate records. This may happen if Gobblin publishes the data successfully, but for some reason fails to persist the checkpoints (watermarks) into the state store. Gobblin provides two compactors out-of-the-box, a MapReduce compactor and a Hive compactor. MapReduce Compactor The MapReduce compactor can be used to deduplicate on all or certain fields of the records. For duplicate records, one of them will be preserved; there is no guarantee which one will be preserved. A use case of MapReduce Compactor is for Kafka records deduplication. We will use the following example use case to explain the MapReduce Compactor. Example Use Case Suppose we ingest data from a Kafka broker, and we would like to publish the data by hour and by day, both of which are deduplicated: Data in the Kafka broker is first ingested into an hourly_staging folder, e.g., /data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29/08... A compaction with deduplication runs hourly, consumes data in hourly_staging and publish data into hourly , e.g., /data/kafka_topics/PageViewEvent/hourly/2015/10/29/08... A non-deduping compaction runs daily, consumes data in hourly and publish data into daily , e.g., /data/kafka_topics/PageViewEvent/daily/2015/10/29... Basic Usage MRCompactor.compact() is the entry point for MapReduce-based compaction. The compaction unit is Dataset . MRCompactor uses a DatasetsFinder to find all datasets eligible for compaction. Implementations of DatasetsFinder include SimpleDatasetsFinder and TimeBasedSubDirDatasetsFinder . In the above example use case, for hourly compaction, each dataset contains an hour's data in the hourly_staging folder, e.g., /data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29/08 ; for daily compaction, each dataset contains 24 hourly folder of a day, e.g., /data/kafka_topics/PageViewEvent/hourly/2015/10/29 . In hourly compaction, you may use the following config properties: compaction.datasets.finder=org.apache.gobblin.compaction.dataset.TimeBasedSubDirDatasetsFinder compaction.input.dir=/data/kafka_topics compaction.dest.dir=/data/kafka_topics compaction.input.subdir=hourly_staging compaction.dest.subdir=hourly compaction.folder.pattern=YYYY/MM/dd compaction.timebased.max.time.ago=3h compaction.timebased.min.time.ago=1h compaction.jobprops.creator.class=org.apache.gobblin.compaction.mapreduce.MRCompactorTimeBasedJobPropCreator compaction.job.runner.class=org.apache.gobblin.compaction.mapreduce.avro.MRCompactorAvroKeyDedupJobRunner (if your data is Avro) If your data format is not Avro, you can implement a different job runner class for deduplicating your data format. Non-deduping Compaction via Map-only Jobs There are two types of Non-deduping compaction. Type 1 : deduplication is not needed, for example you simply want to consolidate files in 24 hourly folders into a single daily folder. Type 2 : deduplication is needed, i.e., the published data should not contain duplicates, but the input data are already deduplicated. The daily compaction in the above example use case is of this type. Property compaction.input.deduplicated specifies whether the input data are deduplicated (default is false), and property compaction.output.deduplicated specifies whether the output data should be deduplicated (default is true). For type 1 deduplication, set both to false. For type 2 deduplication, set both to true. The reason these two types of compaction need to be separated is because of late data handling, which we will explain next. Handling Late Records Late records are records that arrived at a folder after compaction on this folder has started. We explain how Gobblin handles late records using the following example. In this use case, both hourly compaction and daily compaction need a mechanism to handle late records. For hourly compaction, late records are records that arrived at an hourly_staging folder after the hourly compaction of that folder has started. It is similar for daily compaction. Compaction with Deduplication For a compaction with deduplication (i.e., hourly compaction in the above use case), there are two options to deal with late data: Option 1 : if there are late data, re-do the compaction. For example, you may run the hourly compaction multiple times per hour. The first run will do the normal compaction, and in each subsequent run, if it detects late data in a folder, it will re-do compaction for that folder. To do so, set compaction.job.overwrite.output.dir=true and compaction.recompact.from.input.for.late.data=true . Please note the following when you use this option: (1) this means that your already-published data will be re-published if late data are detected; (2) this is potentially dangerous if your input folders have short retention periods. For example, suppose hourly_staging folders have a 2-day retention period, i.e., folder /data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29 will be deleted on 2015/10/31. If, after 2015/10/31, new data arrived at this folder and you re-compact this folder and publish the data to hourly , all original data will be gone. To avoid this problem you may set compaction.timebased.max.time.ago=2d so that compaction will not be performed on a folder more than 2 days ago. However, this means that if a late record is late for more than 2 days, it will never be published into hourly . Option 2 : (this is the default option) if there are late data, copy the late data into a [output_subdir]/_late folder, e.g., for hourly compaction, late data in hourly_staging will be copied to hourly_late folders, e.g., /data/kafka_topics/PageViewEvent/hourly_late/2015/10/29... . If re-compaction is not necessary, this is all you need to do. If re-compaction is needed, you may schedule or manually invoke a re-compaction job which will re-compact by consuming data in both hourly and hourly_late . For this job, you need to set compaction.job.overwrite.output.dir=true and compaction.recompact.from.dest.paths=true . Note that this re-compaction is different from the re-compaction in Option 1: this re-compaction consumes data in output folders (i.e., hourly ) whereas the re-compaction in Option 1 consumes data in input folders (i.e., hourly_staging ). Compaction without Deduplication For a compaction without deduplication, if it is type 2, the same two options above apply. If it is type 1, late data will simply be copied to the output folder. How to Determine if a Data File is Late Every time a compaction finishes (except the case below), Gobblin will create a file named _COMPACTION_COMPLETE in the compaction output folder. This file contains the timestamp of when the compaction job starts. All files in the input folder with earlier modification timestamps have been compacted. Next time the compaction runs, files in the input folder with later timestamps are considered late data. The _COMPACTION_COMPLETE file will be only be created if it is a regular compaction that consumes input data (including compaction jobs that just copy late data to the output folder or the [output_subdir]/_late folder without launching an MR job). It will not be created if it is a re-compaction that consumes output data. This is because whether a file in the input folder is a late file depends on whether it has been compacted or moved into the output folder, which is not affected by a re-compaction that consumes output data. One way of reducing the chance of seeing late records is to verify data completeness before running compaction, which will be explained next. Verifying Data Completeness Before Compaction Besides aborting the compaction job for a dataset if new data in the input folder is found, another way to reduce the chance of seeing late events is to verify the completeness of input data before running compaction. To do so, set compaction.completeness.verification.enabled=true , extend DataCompletenessVerifier.AbstractRunner and put in your verification logic, and pass it via compaction.completeness.verification.class . When data completeness verification is enabled, MRCompactor will verify data completeness for the input datasets, and meanwhile speculatively start the compaction MR jobs. When the compaction MR job for a dataset finishes, if the completeness of the dataset is verified, its compacted data will be published, otherwise it is discarded, and the compaction MR job for this dataset will be launched again with a reduced priority. It is possible to control which topics should or should not be verified via compaction.completeness.verification.whitelist and compaction.completeness.verification.blacklist . It is also possible to set a timeout for data completeness verification via compaction.completeness.verification.timeout.minutes . A dataset whose completeness verification timed out can be configured to be either compacted anyway or not compacted. Hive Compactor The Hive compactor can be used to merge a snapshot with one or multiple deltas. It assumes the snapshot and the deltas meet the following requirements: Snapshot and all deltas are in Avro format. Snapshot and all deltas have the same primary key attributes (they do not need to have the same schema). Snapshot is pulled earlier than all deltas. Therefore if a key appears in both snapshot and deltas, the one in the snapshot should be discarded. The deltas are pulled one after another, and ordered in ascending order of pull time. If a key appears in both the ith delta and the jth delta (i j), the one in the jth delta survives. The merged data will be written to the HDFS directory specified in output.datalocation , as one or more Avro files. The schema of the output data will be the same as the schema of the last delta (which is the last pulled data and thus has the latest schema). In the near future we also plan to support selecting records by timestamps (rather than which file they appear). This is useful if the snapshot and the deltas are pulled in parallel, where if a key has multiple occurrences we should keep the one with the latest timestamp. Note that since delta tables don't have information of deleted records, such information is only available the next time the full snapshot is pulled. Basic Usage A Hive Compactor job consists of one global configuration file which refers to one or more job configuration(s). Global Config Properties (example: compaction.properties) (1) Required: compaction.config.dir This is the the compaction jobconfig directory. Each file in this directory should be a jobconfig file (described in the next section). (2) Optional: hadoop.configfile. * Hadoop configuration files that should be loaded (e.g., hadoop.configfile.coresite.xml=/export/apps/hadoop/latest/etc/hadoop/core-site.xml) hdfs.uri If property fs.defaultFS (or fs.default.name ) is specified in the hadoop config file, then this property is not needed. However, if it is specified, it will override fs.defaultFS (or fs.default.name ). If fs.defaultFS or fs.default.name is not specified in the hadoop config file, and this property is also not specified, then the default value \"hdfs://localhost:9000\" will be used. hiveserver.version (default: 2) Either 1 or 2. hiveserver.connection.string hiveserver.url hiveserver.user (default: \"\") hiveserver.password (default: \"\") If hiveserver.connection.string is specified, it will be used to connect to hiveserver. If hiveserver.connection.string is not specified but hiveserver.url is specified, then it uses ( hiveserver.url , hiveserver.user , hiveserver.password ) to connect to hiveserver. If neither hiveserver.connection.string nor hiveserver.url is specified, then embedded hiveserver will be used (i.e., jdbc:hive:// if hiveserver.version=1 , jdbc:hive2:// if hiveserver.version=2 ) hivesite.dir Directory that contains hive-site.xml, if hive-site.xml should be loaded. hive. * Any hive config property. (e.g., hive.join.cache.size ). If specified, it will override the corresponding property in hive-site.xml. Job Config Properties (example: jobconf/task1.conf) (1) Required: snapshot.pkey comma separated primary key attributes of the snapshot table snapshot.datalocation snapshot data directory in HDFS delta.i.pkey (i = 1, 2...) the primary key of ith delta table (the primary key of snapshot and all deltas should be the same) delta.i.datalocation (i = 1, 2...) ith delta table's data directory in HDFS output.datalocation the HDFS data directory for the output (make sure you have write permission on this directory) (2) Optional: snapshot.name (default: randomly generated name) prefix name of the snapshot table. The table name will be snapshot.name + random suffix snapshot.schemalocation snapshot table's schema location in HDFS. If not specified, schema will be extracted from the data. delta.i.name (default: randomly generated name) prefix name of the ith delta table. The table name will be delta.i.name + random suffix delta.i.schemalocation ith delta table's schema location in HDFS. If not specified, schema will be extracted from the data. output.name (default: randomly generated name) prefix name of the output table. The table name will be output.name + random suffix hive.db.name (default: default) the database name to be used. This database should already exist, and you should have write permission on it. hive.queue.name (default: default) queue name to be used. hive.use.mapjoin (default: if not specified in the global config file, then false) whether map-side join should be turned on. If specified both in this property and in the global config file (hive.*), this property takes precedences. hive.mapjoin.smalltable.filesize (default: if not specified in the global config file, then use Hive's default value) if hive.use.mapjoin = true, mapjoin will be used if the small table size is smaller than hive.mapjoin.smalltable.filesize (in bytes). If specified both in this property and in the global config file (hive.*), this property takes precedences. hive.tmpschema.dir (default: the parent dir of the data location dir where the data is used to extract the schema) If we need to extract schema from data, this dir is for the extracted schema. Note that if you do not have write permission on the default dir, you must specify this property as a dir where you do have write permission. snapshot.copydata (default: false) Set to true if you don't want to (or are unable to) create external table on snapshot.datalocation. A copy of the snapshot data will be created in hive.tmpdata.dir , and will be removed after the compaction. This property should be set to true if either of the following two situations applies: (i) You don't have write permission to snapshot.datalocation . If so, once you create an external table on snapshot.datalocation , you may not be able to drop it. This is a Hive bug and for more information, see this page , which includes a Hive patch for the bug. (ii) You want to use a certain subset of files in snapshot.datalocation (e.g., snapshot.datalocation contains both .csv and .avro files but you only want to use .avro files) delta.i.copydata (i = 1, 2...) (default: false) Similar as snapshot.copydata hive.tmpdata.dir (default: \"/\") If snapshot.copydata = true or delta.i.copydata = true, the data will be copied to this dir. You should have write permission to this dir. snapshot.dataformat.extension.name (default: \"\") If snapshot.copydata = true, then only those data files whose extension is snapshot.dataformat will be moved to hive.tmpdata.dir . delta.i.dataformat.extension.name (default: \"\") Similar as snapshot.dataformat.extension.name . mapreduce.job.num.reducers Number of reducers for the job. timing.file (default: time.txt) A file where the running time of each compaction job is printed. Running a Compaction Job Both the MapReduce and Hive-based compaction configurations can be executed with bin/gobblin-compaction.sh . The usage is as follows: gobblin-compaction.sh [OPTION] --type compaction type: hive or mr --conf compaction configuration file Where OPTION can be: --projectversion version Gobblin version to be used. If set, overrides the distribution build version --logdir log dir Gobblin's log directory: if not set, taken from ${GOBBLIN_LOG_DIR} if present. --help Display this help and exit Example: cd gobblin-dist bin/gobblin-compaction.sh --type hive --conf compaction.properties The log4j configuration is read from conf/log4j-compaction.xml . Please note that in case of a Hive-compaction for drop table queries ( DROP TABLE IF EXISTS tablename ), the Hive JDBC client will throw NoSuchObjectException if the table doesn't exist. This is normal and such exceptions should be ignored.","title":"Compaction"},{"location":"user-guide/Compaction/#table-of-contents","text":"Table of Contents MapReduce Compactor Example Use Case Basic Usage Non-deduping Compaction via Map-only Jobs Handling Late Records Verifying Data Completeness Before Compaction Hive Compactor Basic Usage Global Config Properties (example: compaction.properties) Job Config Properties (example: jobconf/task1.conf) Running a Compaction Job Compaction can be used to post-process files pulled by Gobblin with certain semantics. Deduplication is one of the common reasons to do compaction, e.g., you may want to deduplicate on all fields of the records. deduplicate on key fields of the records, keep the one with the latest timestamp for records with the same key. This is because duplicates can be generated for multiple reasons including both intended and unintended: For ingestion from data sources with mutable records (e.g., relational databases), instead of ingesting a full snapshot of a table every time, one may wish to ingest only the records that were changed since the previous run (i.e., delta records), and merge these delta records with previously generated snapshots in a compaction. In this case, for records with the same primary key, the one with the latest timestamp should be kept. The data source you ingest from may have duplicate records, e.g., if you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during the replication. In some data sources duplicate records may also be produced by the data producer. In rare circumstances, Gobblin may pull the same data twice, thus creating duplicate records. This may happen if Gobblin publishes the data successfully, but for some reason fails to persist the checkpoints (watermarks) into the state store. Gobblin provides two compactors out-of-the-box, a MapReduce compactor and a Hive compactor.","title":"Table of Contents"},{"location":"user-guide/Compaction/#mapreduce-compactor","text":"The MapReduce compactor can be used to deduplicate on all or certain fields of the records. For duplicate records, one of them will be preserved; there is no guarantee which one will be preserved. A use case of MapReduce Compactor is for Kafka records deduplication. We will use the following example use case to explain the MapReduce Compactor.","title":"MapReduce Compactor"},{"location":"user-guide/Compaction/#example-use-case","text":"Suppose we ingest data from a Kafka broker, and we would like to publish the data by hour and by day, both of which are deduplicated: Data in the Kafka broker is first ingested into an hourly_staging folder, e.g., /data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29/08... A compaction with deduplication runs hourly, consumes data in hourly_staging and publish data into hourly , e.g., /data/kafka_topics/PageViewEvent/hourly/2015/10/29/08... A non-deduping compaction runs daily, consumes data in hourly and publish data into daily , e.g., /data/kafka_topics/PageViewEvent/daily/2015/10/29...","title":"Example Use Case"},{"location":"user-guide/Compaction/#basic-usage","text":"MRCompactor.compact() is the entry point for MapReduce-based compaction. The compaction unit is Dataset . MRCompactor uses a DatasetsFinder to find all datasets eligible for compaction. Implementations of DatasetsFinder include SimpleDatasetsFinder and TimeBasedSubDirDatasetsFinder . In the above example use case, for hourly compaction, each dataset contains an hour's data in the hourly_staging folder, e.g., /data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29/08 ; for daily compaction, each dataset contains 24 hourly folder of a day, e.g., /data/kafka_topics/PageViewEvent/hourly/2015/10/29 . In hourly compaction, you may use the following config properties: compaction.datasets.finder=org.apache.gobblin.compaction.dataset.TimeBasedSubDirDatasetsFinder compaction.input.dir=/data/kafka_topics compaction.dest.dir=/data/kafka_topics compaction.input.subdir=hourly_staging compaction.dest.subdir=hourly compaction.folder.pattern=YYYY/MM/dd compaction.timebased.max.time.ago=3h compaction.timebased.min.time.ago=1h compaction.jobprops.creator.class=org.apache.gobblin.compaction.mapreduce.MRCompactorTimeBasedJobPropCreator compaction.job.runner.class=org.apache.gobblin.compaction.mapreduce.avro.MRCompactorAvroKeyDedupJobRunner (if your data is Avro) If your data format is not Avro, you can implement a different job runner class for deduplicating your data format.","title":"Basic Usage"},{"location":"user-guide/Compaction/#non-deduping-compaction-via-map-only-jobs","text":"There are two types of Non-deduping compaction. Type 1 : deduplication is not needed, for example you simply want to consolidate files in 24 hourly folders into a single daily folder. Type 2 : deduplication is needed, i.e., the published data should not contain duplicates, but the input data are already deduplicated. The daily compaction in the above example use case is of this type. Property compaction.input.deduplicated specifies whether the input data are deduplicated (default is false), and property compaction.output.deduplicated specifies whether the output data should be deduplicated (default is true). For type 1 deduplication, set both to false. For type 2 deduplication, set both to true. The reason these two types of compaction need to be separated is because of late data handling, which we will explain next.","title":"Non-deduping Compaction via Map-only Jobs"},{"location":"user-guide/Compaction/#handling-late-records","text":"Late records are records that arrived at a folder after compaction on this folder has started. We explain how Gobblin handles late records using the following example. In this use case, both hourly compaction and daily compaction need a mechanism to handle late records. For hourly compaction, late records are records that arrived at an hourly_staging folder after the hourly compaction of that folder has started. It is similar for daily compaction. Compaction with Deduplication For a compaction with deduplication (i.e., hourly compaction in the above use case), there are two options to deal with late data: Option 1 : if there are late data, re-do the compaction. For example, you may run the hourly compaction multiple times per hour. The first run will do the normal compaction, and in each subsequent run, if it detects late data in a folder, it will re-do compaction for that folder. To do so, set compaction.job.overwrite.output.dir=true and compaction.recompact.from.input.for.late.data=true . Please note the following when you use this option: (1) this means that your already-published data will be re-published if late data are detected; (2) this is potentially dangerous if your input folders have short retention periods. For example, suppose hourly_staging folders have a 2-day retention period, i.e., folder /data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29 will be deleted on 2015/10/31. If, after 2015/10/31, new data arrived at this folder and you re-compact this folder and publish the data to hourly , all original data will be gone. To avoid this problem you may set compaction.timebased.max.time.ago=2d so that compaction will not be performed on a folder more than 2 days ago. However, this means that if a late record is late for more than 2 days, it will never be published into hourly . Option 2 : (this is the default option) if there are late data, copy the late data into a [output_subdir]/_late folder, e.g., for hourly compaction, late data in hourly_staging will be copied to hourly_late folders, e.g., /data/kafka_topics/PageViewEvent/hourly_late/2015/10/29... . If re-compaction is not necessary, this is all you need to do. If re-compaction is needed, you may schedule or manually invoke a re-compaction job which will re-compact by consuming data in both hourly and hourly_late . For this job, you need to set compaction.job.overwrite.output.dir=true and compaction.recompact.from.dest.paths=true . Note that this re-compaction is different from the re-compaction in Option 1: this re-compaction consumes data in output folders (i.e., hourly ) whereas the re-compaction in Option 1 consumes data in input folders (i.e., hourly_staging ). Compaction without Deduplication For a compaction without deduplication, if it is type 2, the same two options above apply. If it is type 1, late data will simply be copied to the output folder. How to Determine if a Data File is Late Every time a compaction finishes (except the case below), Gobblin will create a file named _COMPACTION_COMPLETE in the compaction output folder. This file contains the timestamp of when the compaction job starts. All files in the input folder with earlier modification timestamps have been compacted. Next time the compaction runs, files in the input folder with later timestamps are considered late data. The _COMPACTION_COMPLETE file will be only be created if it is a regular compaction that consumes input data (including compaction jobs that just copy late data to the output folder or the [output_subdir]/_late folder without launching an MR job). It will not be created if it is a re-compaction that consumes output data. This is because whether a file in the input folder is a late file depends on whether it has been compacted or moved into the output folder, which is not affected by a re-compaction that consumes output data. One way of reducing the chance of seeing late records is to verify data completeness before running compaction, which will be explained next.","title":"Handling Late Records"},{"location":"user-guide/Compaction/#verifying-data-completeness-before-compaction","text":"Besides aborting the compaction job for a dataset if new data in the input folder is found, another way to reduce the chance of seeing late events is to verify the completeness of input data before running compaction. To do so, set compaction.completeness.verification.enabled=true , extend DataCompletenessVerifier.AbstractRunner and put in your verification logic, and pass it via compaction.completeness.verification.class . When data completeness verification is enabled, MRCompactor will verify data completeness for the input datasets, and meanwhile speculatively start the compaction MR jobs. When the compaction MR job for a dataset finishes, if the completeness of the dataset is verified, its compacted data will be published, otherwise it is discarded, and the compaction MR job for this dataset will be launched again with a reduced priority. It is possible to control which topics should or should not be verified via compaction.completeness.verification.whitelist and compaction.completeness.verification.blacklist . It is also possible to set a timeout for data completeness verification via compaction.completeness.verification.timeout.minutes . A dataset whose completeness verification timed out can be configured to be either compacted anyway or not compacted.","title":"Verifying Data Completeness Before Compaction"},{"location":"user-guide/Compaction/#hive-compactor","text":"The Hive compactor can be used to merge a snapshot with one or multiple deltas. It assumes the snapshot and the deltas meet the following requirements: Snapshot and all deltas are in Avro format. Snapshot and all deltas have the same primary key attributes (they do not need to have the same schema). Snapshot is pulled earlier than all deltas. Therefore if a key appears in both snapshot and deltas, the one in the snapshot should be discarded. The deltas are pulled one after another, and ordered in ascending order of pull time. If a key appears in both the ith delta and the jth delta (i j), the one in the jth delta survives. The merged data will be written to the HDFS directory specified in output.datalocation , as one or more Avro files. The schema of the output data will be the same as the schema of the last delta (which is the last pulled data and thus has the latest schema). In the near future we also plan to support selecting records by timestamps (rather than which file they appear). This is useful if the snapshot and the deltas are pulled in parallel, where if a key has multiple occurrences we should keep the one with the latest timestamp. Note that since delta tables don't have information of deleted records, such information is only available the next time the full snapshot is pulled.","title":"Hive Compactor"},{"location":"user-guide/Compaction/#basic-usage_1","text":"A Hive Compactor job consists of one global configuration file which refers to one or more job configuration(s).","title":"Basic Usage"},{"location":"user-guide/Compaction/#global-config-properties-example-compactionproperties","text":"(1) Required: compaction.config.dir This is the the compaction jobconfig directory. Each file in this directory should be a jobconfig file (described in the next section). (2) Optional: hadoop.configfile. * Hadoop configuration files that should be loaded (e.g., hadoop.configfile.coresite.xml=/export/apps/hadoop/latest/etc/hadoop/core-site.xml) hdfs.uri If property fs.defaultFS (or fs.default.name ) is specified in the hadoop config file, then this property is not needed. However, if it is specified, it will override fs.defaultFS (or fs.default.name ). If fs.defaultFS or fs.default.name is not specified in the hadoop config file, and this property is also not specified, then the default value \"hdfs://localhost:9000\" will be used. hiveserver.version (default: 2) Either 1 or 2. hiveserver.connection.string hiveserver.url hiveserver.user (default: \"\") hiveserver.password (default: \"\") If hiveserver.connection.string is specified, it will be used to connect to hiveserver. If hiveserver.connection.string is not specified but hiveserver.url is specified, then it uses ( hiveserver.url , hiveserver.user , hiveserver.password ) to connect to hiveserver. If neither hiveserver.connection.string nor hiveserver.url is specified, then embedded hiveserver will be used (i.e., jdbc:hive:// if hiveserver.version=1 , jdbc:hive2:// if hiveserver.version=2 ) hivesite.dir Directory that contains hive-site.xml, if hive-site.xml should be loaded. hive. * Any hive config property. (e.g., hive.join.cache.size ). If specified, it will override the corresponding property in hive-site.xml.","title":"Global Config Properties (example: compaction.properties)"},{"location":"user-guide/Compaction/#job-config-properties-example-jobconftask1conf","text":"(1) Required: snapshot.pkey comma separated primary key attributes of the snapshot table snapshot.datalocation snapshot data directory in HDFS delta.i.pkey (i = 1, 2...) the primary key of ith delta table (the primary key of snapshot and all deltas should be the same) delta.i.datalocation (i = 1, 2...) ith delta table's data directory in HDFS output.datalocation the HDFS data directory for the output (make sure you have write permission on this directory) (2) Optional: snapshot.name (default: randomly generated name) prefix name of the snapshot table. The table name will be snapshot.name + random suffix snapshot.schemalocation snapshot table's schema location in HDFS. If not specified, schema will be extracted from the data. delta.i.name (default: randomly generated name) prefix name of the ith delta table. The table name will be delta.i.name + random suffix delta.i.schemalocation ith delta table's schema location in HDFS. If not specified, schema will be extracted from the data. output.name (default: randomly generated name) prefix name of the output table. The table name will be output.name + random suffix hive.db.name (default: default) the database name to be used. This database should already exist, and you should have write permission on it. hive.queue.name (default: default) queue name to be used. hive.use.mapjoin (default: if not specified in the global config file, then false) whether map-side join should be turned on. If specified both in this property and in the global config file (hive.*), this property takes precedences. hive.mapjoin.smalltable.filesize (default: if not specified in the global config file, then use Hive's default value) if hive.use.mapjoin = true, mapjoin will be used if the small table size is smaller than hive.mapjoin.smalltable.filesize (in bytes). If specified both in this property and in the global config file (hive.*), this property takes precedences. hive.tmpschema.dir (default: the parent dir of the data location dir where the data is used to extract the schema) If we need to extract schema from data, this dir is for the extracted schema. Note that if you do not have write permission on the default dir, you must specify this property as a dir where you do have write permission. snapshot.copydata (default: false) Set to true if you don't want to (or are unable to) create external table on snapshot.datalocation. A copy of the snapshot data will be created in hive.tmpdata.dir , and will be removed after the compaction. This property should be set to true if either of the following two situations applies: (i) You don't have write permission to snapshot.datalocation . If so, once you create an external table on snapshot.datalocation , you may not be able to drop it. This is a Hive bug and for more information, see this page , which includes a Hive patch for the bug. (ii) You want to use a certain subset of files in snapshot.datalocation (e.g., snapshot.datalocation contains both .csv and .avro files but you only want to use .avro files) delta.i.copydata (i = 1, 2...) (default: false) Similar as snapshot.copydata hive.tmpdata.dir (default: \"/\") If snapshot.copydata = true or delta.i.copydata = true, the data will be copied to this dir. You should have write permission to this dir. snapshot.dataformat.extension.name (default: \"\") If snapshot.copydata = true, then only those data files whose extension is snapshot.dataformat will be moved to hive.tmpdata.dir . delta.i.dataformat.extension.name (default: \"\") Similar as snapshot.dataformat.extension.name . mapreduce.job.num.reducers Number of reducers for the job. timing.file (default: time.txt) A file where the running time of each compaction job is printed.","title":"Job Config Properties (example: jobconf/task1.conf)"},{"location":"user-guide/Compaction/#running-a-compaction-job","text":"Both the MapReduce and Hive-based compaction configurations can be executed with bin/gobblin-compaction.sh . The usage is as follows: gobblin-compaction.sh [OPTION] --type compaction type: hive or mr --conf compaction configuration file Where OPTION can be: --projectversion version Gobblin version to be used. If set, overrides the distribution build version --logdir log dir Gobblin's log directory: if not set, taken from ${GOBBLIN_LOG_DIR} if present. --help Display this help and exit Example: cd gobblin-dist bin/gobblin-compaction.sh --type hive --conf compaction.properties The log4j configuration is read from conf/log4j-compaction.xml . Please note that in case of a Hive-compaction for drop table queries ( DROP TABLE IF EXISTS tablename ), the Hive JDBC client will throw NoSuchObjectException if the table doesn't exist. This is normal and such exceptions should be ignored.","title":"Running a Compaction Job"},{"location":"user-guide/Config-Management/","text":"Table of Contents Table of Contents Introduction Dataset Config Management Requirement Data Model Versioning Client library Config Store Current Dataset Config Management Implementation Data model Client application File System layout Example of a config store Introduction There are multiple challenges in dataset configuration management in the context of ETL data processing as ETL infrastructure employs multi-state processing flows to ingest and publish data on HDFS. Here are some examples types of datasets and types of processing: OLTP Snapshots: ingest, publishing, replication, retention management, compliance post-processing OLTP Increments: ingest, publishing, replication, roll-up, compaction, retention management Streaming data: ingest, publishing, roll-up of streaming data, retention management, compliance post-processing Opaque (derived) data: replication, retention management A typical dataset could be a database table, a Kafka topic, etc. Current the customization of the dataset processing is typically achieved through file/directory blacklists/whitelists in job/flow level configurations. This approach suffers from a number issues: Dataset unaware - control is done through low-level file/directory wildcards which can be hard to understand for more complex data layouts. Difficulty in using/applying policies, i.e. applying the same configuration settings to large number of datasets. Non-intuitive - the use of blacklists and whitelists can lead to properties whose effect is not always clear. A large potential of inconsistencies across different jobs/flows. Lack of version control Lack of easy, aggregated view of the setup/configuration for a given dataset across all flows (including consumer access) We want to have a new way to customize the processing of each dataset like enabling/disabling certain types of processing, specific SLAs, access restrictions, retention policies, etc. without previous mentioned problems. Dataset Config Management Requirement Design a backend and flexible client library for storing, managing and accessing configuration that can be used to customize the process of thousands of datasets across multiple systems/flows. Data Model (Dataset) configs are identified by a string config key Each config key is mapped to a config object (a collection of properties) The config object should be extensible, i.e. we should be able to add properties with arbitrary names Hierarchical system for overrides Global default values Children datasets override/inherit parent dataset configs Ability to group properties that are specific for a group of datasets (aka tags). For example, we should be able to tag a group of Kafka datasets as \"High-priority\" and associate specific configuration properties with these settings. Tags should can be applied to both datasets and other tags. Support for references to other properties (parameter expansion) Late expansion of references (at time of access) Versioning Generated configs have a monotonically increasing version number Generated configs have a date of generation Once published configurations are immutable Easy rollback to a previous version Even if rolled back, a configuration is still available for processes that already use it Audit traces about changes to configuration Client library Always loads the latest (non-rollbacked) configuration version (as of time of initialization). Once the version is fixed, the same version should be used for the remained of the processing. The consistency is needed only within a process. Non-requirement: cross-process version consistency. The application needs to enforce consistency across processes if necessary, e.g. by copying the config to a stable location. Ability to list the tags (policies) associated with a dataset. Ability to list datasets associated with a tag. For example, we should be able to have a flow which can discover and process only \"High-priority\" datasets. Debug info how values were derived in generated configs (e.g. from where a property value was inherited) Config Store Each config store is represented by an URI path The URI path is significant so that configs can be associated at every level. For example, for the dataset URI hdfs_store:/data/databases/DB/Table , we should be able to associate config at every level: /data/databases/DB/Table, /data/databases/DB/, /data/databases/, etc. Current Dataset Config Management Implementation At a very high-level, we extend typesafe config with: Support for logical include URIs Abstraction of a Config Store Config versioning Ability to traverse the \u201dimport\u201d relationships Data model Config key (configuration node) / config value For our use cases, we can define each configuration node per data set. All the configuration related to that dataset are specified together. Essentially, the system provides a mapping from a config key to a config object. Each config key is represented through a URI. The config object is a map from property name to a property value. We refer to this as own config (object) and refer to it through the function own_config(K, property_name) = property_value. A config key K can import one or more config keys I1, I2, ... . The config key K will inherit any properties from I1, I2, \u2026 that are not defined in K. The inheritance is resolved in the order of the keys I1, I2, \u2026 etc., i.e. the property will be resolved to the value in the last one that defines the property. This is similar to including configs in typesafe config. We will refer to resulting configuration as own config (object) and denote it though the function resolved_config(K, property_name) = property_value . We also use the path in the config key URI for implicit tagging. For example, /data/tracking/TOPIC implicitly imports /data/tracking/ , which implicitly imports /data/ which implicitly imports / . Note that all these URI are considered as config Key so their path level implicitly indicates importation. For a given config key, all implicit imports are before the explicit imports, i.e. they have lower priority in resolution. Typical use case for this implicit importation can be a global default configuration file in root path applied to all files under it. Files in this root path can have their own setting overriding the default value inherited from root path's file. Tags For our use cases, we can define the static tags in a well known file per dataset. Dynamic tags Some tags cannot be applied statically at \u201ccompile\u201d time. For example, such are cluster-specific tags since they are on the environment where the client application runs. We will support such tags about allowing the use of limited number of variables when importing another config key. For example, such a variable can be \u201clocal_cluster.name\u201d. Then, importing /data/tracking/${local_cluster.name} can provide cluster-specific overrides. Config Store The configuration is partitioned in a number of Config Stores . Each Config Store is: mapped to a unique URI scheme; responsible for managing the mapping of config keys (represented through URIs with the Config Store scheme) to unresolved configs; Client application The client application interacts using the ConfigClient API . The ConfigClient maintains a set of ConfigStoreAccessor objects which interact through the ConfigStore API with the appropriate ConfigStore implementation depending on the scheme of the ConfigStore URI . There can be a native implementation of the API like the HadoopFS ConfigStore or an adapter to an existing config/metadata store like the Hive MetaStore, etc File System layout All configurations in one configuration store reside in it\u2019s ROOT directory _CONFIG_STORE file in ROOT directory (identification file for configuration store) One or multiple version directories under ROOT In each version, each directory represented as one configuration node In each directory, the main.conf file specify the configuration for that node In each directory, the includes file specify the imports links Example of a config store ROOT \u251c\u2500\u2500 _CONFIG_STORE (contents = latest non-rolled-back version) \u2514\u2500\u2500 1.0.53 (version directory) \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 tracking \u2502 \u251c\u2500\u2500 TOPIC \u2502 \u2502 \u251c\u2500\u2500 includes (imports links) \u2502 \u2502 \u2514\u2500\u2500 main.conf (configuration file) \u2502 \u251c\u2500\u2500 includes \u2502 \u2514\u2500\u2500 main.conf \u2514\u2500\u2500 tags \u251c\u2500\u2500 tracking \u2502 \u2514\u2500\u2500 retention \u2502 \u2514\u2500\u2500 LONG \u2502 \u2502 \u251c\u2500\u2500 includes \u2502 \u2502 \u2514\u2500\u2500 main.conf \u2502 \u2514\u2500\u2500 main.conf \u2514\u2500\u2500 acl \u2514\u2500\u2500 restricted \u251c\u2500\u2500 main.conf \u2514\u2500\u2500 secdata \u251c\u2500\u2500 includes \u2514\u2500\u2500 main.conf","title":"Config Management"},{"location":"user-guide/Config-Management/#table-of-contents","text":"Table of Contents Introduction Dataset Config Management Requirement Data Model Versioning Client library Config Store Current Dataset Config Management Implementation Data model Client application File System layout Example of a config store","title":"Table of Contents"},{"location":"user-guide/Config-Management/#introduction","text":"There are multiple challenges in dataset configuration management in the context of ETL data processing as ETL infrastructure employs multi-state processing flows to ingest and publish data on HDFS. Here are some examples types of datasets and types of processing: OLTP Snapshots: ingest, publishing, replication, retention management, compliance post-processing OLTP Increments: ingest, publishing, replication, roll-up, compaction, retention management Streaming data: ingest, publishing, roll-up of streaming data, retention management, compliance post-processing Opaque (derived) data: replication, retention management A typical dataset could be a database table, a Kafka topic, etc. Current the customization of the dataset processing is typically achieved through file/directory blacklists/whitelists in job/flow level configurations. This approach suffers from a number issues: Dataset unaware - control is done through low-level file/directory wildcards which can be hard to understand for more complex data layouts. Difficulty in using/applying policies, i.e. applying the same configuration settings to large number of datasets. Non-intuitive - the use of blacklists and whitelists can lead to properties whose effect is not always clear. A large potential of inconsistencies across different jobs/flows. Lack of version control Lack of easy, aggregated view of the setup/configuration for a given dataset across all flows (including consumer access) We want to have a new way to customize the processing of each dataset like enabling/disabling certain types of processing, specific SLAs, access restrictions, retention policies, etc. without previous mentioned problems.","title":"Introduction"},{"location":"user-guide/Config-Management/#dataset-config-management-requirement","text":"Design a backend and flexible client library for storing, managing and accessing configuration that can be used to customize the process of thousands of datasets across multiple systems/flows.","title":"Dataset Config Management Requirement"},{"location":"user-guide/Config-Management/#data-model","text":"(Dataset) configs are identified by a string config key Each config key is mapped to a config object (a collection of properties) The config object should be extensible, i.e. we should be able to add properties with arbitrary names Hierarchical system for overrides Global default values Children datasets override/inherit parent dataset configs Ability to group properties that are specific for a group of datasets (aka tags). For example, we should be able to tag a group of Kafka datasets as \"High-priority\" and associate specific configuration properties with these settings. Tags should can be applied to both datasets and other tags. Support for references to other properties (parameter expansion) Late expansion of references (at time of access)","title":"Data Model"},{"location":"user-guide/Config-Management/#versioning","text":"Generated configs have a monotonically increasing version number Generated configs have a date of generation Once published configurations are immutable Easy rollback to a previous version Even if rolled back, a configuration is still available for processes that already use it Audit traces about changes to configuration","title":"Versioning"},{"location":"user-guide/Config-Management/#client-library","text":"Always loads the latest (non-rollbacked) configuration version (as of time of initialization). Once the version is fixed, the same version should be used for the remained of the processing. The consistency is needed only within a process. Non-requirement: cross-process version consistency. The application needs to enforce consistency across processes if necessary, e.g. by copying the config to a stable location. Ability to list the tags (policies) associated with a dataset. Ability to list datasets associated with a tag. For example, we should be able to have a flow which can discover and process only \"High-priority\" datasets. Debug info how values were derived in generated configs (e.g. from where a property value was inherited)","title":"Client library"},{"location":"user-guide/Config-Management/#config-store","text":"Each config store is represented by an URI path The URI path is significant so that configs can be associated at every level. For example, for the dataset URI hdfs_store:/data/databases/DB/Table , we should be able to associate config at every level: /data/databases/DB/Table, /data/databases/DB/, /data/databases/, etc.","title":"Config Store"},{"location":"user-guide/Config-Management/#current-dataset-config-management-implementation","text":"At a very high-level, we extend typesafe config with: Support for logical include URIs Abstraction of a Config Store Config versioning Ability to traverse the \u201dimport\u201d relationships","title":"Current Dataset Config Management Implementation"},{"location":"user-guide/Config-Management/#data-model_1","text":"Config key (configuration node) / config value For our use cases, we can define each configuration node per data set. All the configuration related to that dataset are specified together. Essentially, the system provides a mapping from a config key to a config object. Each config key is represented through a URI. The config object is a map from property name to a property value. We refer to this as own config (object) and refer to it through the function own_config(K, property_name) = property_value. A config key K can import one or more config keys I1, I2, ... . The config key K will inherit any properties from I1, I2, \u2026 that are not defined in K. The inheritance is resolved in the order of the keys I1, I2, \u2026 etc., i.e. the property will be resolved to the value in the last one that defines the property. This is similar to including configs in typesafe config. We will refer to resulting configuration as own config (object) and denote it though the function resolved_config(K, property_name) = property_value . We also use the path in the config key URI for implicit tagging. For example, /data/tracking/TOPIC implicitly imports /data/tracking/ , which implicitly imports /data/ which implicitly imports / . Note that all these URI are considered as config Key so their path level implicitly indicates importation. For a given config key, all implicit imports are before the explicit imports, i.e. they have lower priority in resolution. Typical use case for this implicit importation can be a global default configuration file in root path applied to all files under it. Files in this root path can have their own setting overriding the default value inherited from root path's file. Tags For our use cases, we can define the static tags in a well known file per dataset. Dynamic tags Some tags cannot be applied statically at \u201ccompile\u201d time. For example, such are cluster-specific tags since they are on the environment where the client application runs. We will support such tags about allowing the use of limited number of variables when importing another config key. For example, such a variable can be \u201clocal_cluster.name\u201d. Then, importing /data/tracking/${local_cluster.name} can provide cluster-specific overrides. Config Store The configuration is partitioned in a number of Config Stores . Each Config Store is: mapped to a unique URI scheme; responsible for managing the mapping of config keys (represented through URIs with the Config Store scheme) to unresolved configs;","title":"Data model"},{"location":"user-guide/Config-Management/#client-application","text":"The client application interacts using the ConfigClient API . The ConfigClient maintains a set of ConfigStoreAccessor objects which interact through the ConfigStore API with the appropriate ConfigStore implementation depending on the scheme of the ConfigStore URI . There can be a native implementation of the API like the HadoopFS ConfigStore or an adapter to an existing config/metadata store like the Hive MetaStore, etc","title":"Client application"},{"location":"user-guide/Config-Management/#file-system-layout","text":"All configurations in one configuration store reside in it\u2019s ROOT directory _CONFIG_STORE file in ROOT directory (identification file for configuration store) One or multiple version directories under ROOT In each version, each directory represented as one configuration node In each directory, the main.conf file specify the configuration for that node In each directory, the includes file specify the imports links","title":"File System layout"},{"location":"user-guide/Config-Management/#example-of-a-config-store","text":"ROOT \u251c\u2500\u2500 _CONFIG_STORE (contents = latest non-rolled-back version) \u2514\u2500\u2500 1.0.53 (version directory) \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 tracking \u2502 \u251c\u2500\u2500 TOPIC \u2502 \u2502 \u251c\u2500\u2500 includes (imports links) \u2502 \u2502 \u2514\u2500\u2500 main.conf (configuration file) \u2502 \u251c\u2500\u2500 includes \u2502 \u2514\u2500\u2500 main.conf \u2514\u2500\u2500 tags \u251c\u2500\u2500 tracking \u2502 \u2514\u2500\u2500 retention \u2502 \u2514\u2500\u2500 LONG \u2502 \u2502 \u251c\u2500\u2500 includes \u2502 \u2502 \u2514\u2500\u2500 main.conf \u2502 \u2514\u2500\u2500 main.conf \u2514\u2500\u2500 acl \u2514\u2500\u2500 restricted \u251c\u2500\u2500 main.conf \u2514\u2500\u2500 secdata \u251c\u2500\u2500 includes \u2514\u2500\u2500 main.conf","title":"Example of a config store"},{"location":"user-guide/Configuration-Properties-Glossary/","text":"Configuration properties are key/value pairs that are set in text files. They include system properties that control how Gobblin will pull data, and control what source Gobblin will pull the data from. Configuration files end in some user-specified suffix (by default text files ending in .pull or .job are recognized as configs files, although this is configurable). Each file represents some unit of work that needs to be done in Gobblin. For example, there will typically be a separate configuration file for each table that needs to be pulled from a database. The first section of this document contains all the required properties needed to run a basic Gobblin job. The rest of the document is dedicated to other properties that can be used to configure Gobbin jobs. The description of each configuration parameter will often refer to core Gobblin concepts and terms. If any of these terms are confusing, check out the Gobblin Architecture page for a more detailed explanation of how Gobblin works. The GitHub repo also contains sample config files for specific sources. For example, there are sample config files to connect to MySQL databases and SFTP servers . Gobblin also allows you to specify a global configuration file that contains common properties that are shared across all jobs. The Job Launcher Properties section has more information on how to specify a global properties file. Table of Contents Properties File Format Creating a Basic Properties File Job Launcher Properties Common Job Launcher Properties SchedulerDaemon Properties CliMRJobLauncher Properties AzkabanJobLauncher Properties Job Type Properties Common Job Type Properties LocalJobLauncher Properties MRJobLauncher Properties Retry Properties Task Execution Properties State Store Properties Metrics Properties Email Alert Properties Source Properties Common Source Properties Distcp CopySource Properties RecursiveCopyableDataset Properties DistcpFileSplitter Properties WorkUnitBinPacker Properties QueryBasedExtractor Properties JdbcExtractor Properties FileBasedExtractor Properties SftpExtractor Properties Converter Properties CsvToJsonConverter Properties JsonIntermediateToAvroConverter Properties JsonStringToJsonIntermediateConverter Properties AvroFilterConverter Properties AvroFieldRetrieverConverter Properties AvroFieldsPickConverter Properties AvroToJdbcEntryConverter Properties Fork Properties Quality Checker Properties Writer Properties Data Publisher Properties Generic Properties FileBasedJobLock Properties ZookeeperBasedJobLock Properties JDBC Writer Properties Properties File Format Configuration properties files follow the Java Properties text file format . Further, file includes and variable expansion/interpolation as defined in Apache Commons Configuration are also supported. Example: common.properties writer.staging.dir=/path/to/staging/dir/ writer.output.dir=/path/to/output/dir/ my-job.properties include=common.properties job.name=MyFirstJob Creating a Basic Properties File In order to create a basic configuration property there is a small set of required properties that need to be set. The following properties are required to run any Gobblin job: job.name - Name of the job source.class - Fully qualified path to the Source class responsible for connecting to the data source writer.staging.dir - The directory each task will write staging data to writer.output.dir - The directory each task will commit data to data.publisher.final.dir - The final directory where all the data will be published state.store.dir - The directory where state-store files will be written For more information on each property, check out the comprehensive list below. If only these properties are set, then by default, Gobblin will run in Local mode, as opposed to running on Hadoop M/R. This means Gobblin will write Avro data to the local filesystem. In order to write to HDFS, set the writer.fs.uri property to the URI of the HDFS NameNode that data should be written to. Since the default version of Gobblin writes data in Avro format, the writer expects Avro records to be passed to it. Thus, any data pulled from an external source must be converted to Avro before it can be written out to the filesystem. The source.class property is one of the most important properties in Gobblin. It specifies what Source class to use. The Source class is responsible for determining what work needs to be done during each run of the job, and specifies what Extractor to use in order to read over each sub-unit of data. Examples of Source classes are WikipediaSource and SimpleJsonSource , which can be found in the GitHub repository. For more information on Sources and Extractors, check out the Architecture page. Typically, Gobblin jobs will be launched using the launch scripts in the bin folder. These scripts allow jobs to be launched on the local machine (e.g. SchedulerDaemon) or on Hadoop (e.g. CliMRJobLauncher). Check out the Job Launcher section below, to see the configuration difference between each launch mode. The Deployment page also has more information on the different ways a job can be launched. Job Launcher Properties Gobblin jobs can be launched and scheduled in a variety of ways. They can be scheduled via a Quartz scheduler or through Azkaban . Jobs can also be run without a scheduler via the Command Line. For more information on launching Gobblin jobs, check out the Deployment page. Common Job Launcher Properties These properties are common to both the Job Launcher and the Command Line. Name Description Required Default Value job.name The name of the job to run. This name must be unique within a single Gobblin instance. Yes None job.group A way to group logically similar jobs together. No None job.description A description of what the jobs does. No None job.lock.enabled If set to true job locks are enabled, if set to false they are disabled No True job.lock.type The fully qualified name of the JobLock class to run. The JobLock is responsible for ensuring that only a single instance of a job runs at a time. Allowed values: gobblin.runtime.locks.FileBasedJobLock , gobblin.runtime.locks.ZookeeperBasedJobLock No gobblin.runtime.locks.FileBasedJobLock job.runonce A boolean specifying whether the job will be only once, or multiple times. If set to true the job will only be run once even if a job.schedule is specified. If set to false and a job.schedule is specified then it will run according to the schedule. If set false and a job.schedule is not specified, it will run only once. No False job.disabled Whether the job is disabled or not. If set to true, then Gobblin will not run this job. No False SchedulerDaemon Properties This class is used to schedule Gobblin jobs on Quartz. The job can be launched via the command line, and takes in the location of a global configuration file as a parameter. This configuration file should have the property jobconf.dir in order to specify the location of all the .job or .pull files. Another core difference, is that the global configuration file for the SchedulerDaemon must specify the following properties: writer.staging.dir writer.output.dir data.publisher.final.dir state.store.dir They should not be set in individual job files, as they are system-level parameters. For more information on how to set the configuration parameters for jobs launched through the SchedulerDaemon, check out the Deployment page. Name Description Required Default Value job.schedule Cron-Based job schedule. This schedule only applies to jobs that run using Quartz. No None jobconf.dir When running in local mode, Gobblin will check this directory for any configuration files. Each configuration file should correspond to a separate Gobblin job, and each one should in a suffix specified by the jobconf.extensions parameter. No None jobconf.extensions Comma-separated list of supported job configuration file extensions. When running in local mode, Gobblin will only pick up job files ending in these suffixes. No pull,job jobconf.monitor.interval Controls how often Gobblin checks the jobconf.dir for new configuration files, or for configuration file updates. The parameter is measured in milliseconds. No 300000 CliMRJobLauncher Properties There are no configuration parameters specific to CliMRJobLauncher. This class is used to launch Gobblin jobs on Hadoop from the command line, the jobs are not scheduled. Common properties are set using the --sysconfig option when launching jobs via the command line. For more information on how to set the configuration parameters for jobs launched through the command line, check out the Deployment page. AzkabanJobLauncher Properties There are no configuration parameters specific to AzkabanJobLauncher. This class is used to schedule Gobblin jobs on Azkaban. Common properties can be set through Azkaban by creating a .properties file, check out the Azkaban Documentation for more information. For more information on how to set the configuration parameters for jobs scheduled through the Azkaban, check out the Deployment page. Job Type Properties Common Job Type Properties Name Description Required Default Value launcher.type Job launcher type; one of LOCAL, MAPREDUCE, YARN. LOCAL mode runs on a single machine (LocalJobLauncher), MAPREDUCE runs on a Hadoop cluster (MRJobLauncher), and YARN runs on a YARN cluster (not implemented yet). No LOCAL LocalJobLauncher Properties There are no configuration parameters specific to LocalJobLauncher. The LocalJobLauncher will launch a Hadoop job on a single machine. If launcher.type is set to LOCAL then this class will be used to launch the job. Properties required by the MRJobLauncher class. Name Description Required Default Value framework.jars Comma-separated list of jars the Gobblin framework depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches. No None job.jars Comma-separated list of jar files the job depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches. No None job.hdfs.jars Comma-separated list of jar files the job depends on located in HDFS. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches. No None job.local.files Comma-separated list of local files the job depends on. These files will be available to any map tasks that get launched via the DistributedCache. No None job.hdfs.files Comma-separated list of files on HDFS the job depends on. These files will be available to any map tasks that get launched via the DistributedCache. No None MRJobLauncher Properties Name Description Required Default Value mr.job.root.dir Working directory for a Gobblin Hadoop MR job. Gobblin uses this to write intermediate data, such as the workunit state files that are used by each map task. This has to be a path on HDFS. Yes None mr.job.max.mappers Maximum number of mappers to use in a Gobblin Hadoop MR job. If no explicit limit is set then a map task for each workunit will be launched. If the value of this properties is less than the number of workunits created, then each map task will run multiple tasks. No None mr.include.task.counters Whether to include task-level counters in the set of counters reported as Hadoop counters. Hadoop imposes a system-level limit (default to 120) on the number of counters, so a Gobblin MR job may easily go beyond that limit if the job has a large number of tasks and each task has a few counters. This property gives users an option to not include task-level counters to avoid going over that limit. Yes False Retry Properties Properties that control how tasks and jobs get retried on failure. Name Description Required Default Value workunit.retry.enabled Whether retries of failed work units across job runs are enabled or not. No True workunit.retry.policy Work unit retry policy, can be one of {always, never, onfull, onpartial}. No always task.maxretries Maximum number of task retries. A task will be re-tried this many times before it is considered a failure. No 5 task.retry.intervalinsec Interval in seconds between task retries. The interval increases linearly with each retry. For example, if the first interval is 300 seconds, then the second one is 600 seconds, etc. No 300 job.max.failures Maximum number of failures before an alert email is triggered. No 1 Task Execution Properties These properties control how tasks get executed for a job. Gobblin uses thread pools in order to executes the tasks for a specific job. In local mode there is a single thread pool per job that executes all the tasks for a job. In MR mode there is a thread pool for each map task (or container), and all Gobblin tasks assigned to that mapper are executed in that thread pool. Name Description Required Default Value taskexecutor.threadpool.size Size of the thread pool used by task executor for task execution. Each task executor will spawn this many threads to execute any Tasks that is has been allocated. No 10 tasktracker.threadpool.coresize Core size of the thread pool used by task tracker for task state tracking and reporting. No 10 tasktracker.threadpool.maxsize Maximum size of the thread pool used by task tracker for task state tracking and reporting. No 10 taskretry.threadpool.coresize Core size of the thread pool used by the task executor for task retries. No 2 taskretry.threadpool.maxsize Maximum size of the thread pool used by the task executor for task retries. No 2 task.status.reportintervalinms Task status reporting interval in milliseconds. No 30000 State Store Properties Name Description Required Default Value state.store.dir Root directory where job and task state files are stored. The state-store is used by Gobblin to track state between different executions of a job. All state-store files will be written to this directory. Yes None state.store.fs.uri File system URI for file-system-based state stores. No file:/// Metrics Properties Name Description Required Default Value metrics.enabled Whether metrics collecting and reporting are enabled or not. No True metrics.report.interval Metrics reporting interval in milliseconds. No 60000 metrics.log.dir The directory where metric files will be written to. No None metrics.reporting.file.enabled A boolean indicating whether or not metrics should be reported to a file. No True metrics.reporting.jmx.enabled A boolean indicating whether or not metrics should be exposed via JMX. No False Email Alert Properties Name Description Required Default Value email.alert.enabled Whether alert emails are enabled or not. Email alerts are only sent out when jobs fail consecutively job.max.failures number of times. No False email.notification.enabled Whether job completion notification emails are enabled or not. Notification emails are sent whenever the job completes, regardless of whether it failed or not. No False email.host Host name of the email server. Yes, if email notifications or alerts are enabled. None email.smtp.port SMTP port number. Yes, if email notifications or alerts are enabled. None email.user User name of the sender email account. No None email.password User password of the sender email account. No None email.from Sender email address. Yes, if email notifications or alerts are enabled. None email.tos Comma-separated list of recipient email addresses. Yes, if email notifications or alerts are enabled. None Source Properties Common Source Properties These properties are common properties that are used among different Source implementations. Depending on what source class is being used, these parameters may or may not be necessary. These parameters are not tied to a specific source, and thus can be used in new source classes. Name Description Required Default Value source.class Fully qualified name of the Source class. For example, org.apache.gobblin.example.wikipedia.WikipediaSource Yes None source.entity Name of the source entity that needs to be pulled from the source. The parameter represents a logical grouping of data that needs to be pulled from the source. Often this logical grouping comes in the form a database table, a source topic, etc. In many situations, such as when using the QueryBasedExtractor, it will be the name of the table that needs to pulled from the source. Required for QueryBasedExtractors, FileBasedExtractors. None source.timezone Timezone of the data being pulled in by the extractor. Examples include \"PST\" or \"UTC\". Required for QueryBasedExtractors None source.max.number.of.partitions Maximum number of partitions to split this current run across. Only used by the QueryBasedSource and FileBasedSource. No 20 source.skip.first.record True if you want to skip the first record of each data partition. Only used by the FileBasedExtractor. No False extract.namespace Namespace for the extract data. The namespace will be included in the default file name of the outputted data. No None source.conn.use.proxy.url The URL of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources. No None source.conn.use.proxy.port The port of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources. No None source.conn.username The username to authenticate with the source. This is parameter is only used for SFTP and JDBC sources. No None source.conn.password The password to use when authenticating with the source. This is parameter is only used for JDBC sources. No None source.conn.host The name of the host to connect to. Required for SftpExtractor, MySQLExtractor, OracleExtractor, SQLServerExtractor and TeradataExtractor. None source.conn.rest.url URL to connect to for REST requests. This parameter is only used for the Salesforce source. No None source.conn.version Version number of communication protocol. This parameter is only used for the Salesforce source. No None source.conn.timeout The timeout set for connecting to the source in milliseconds. No 500000 source.conn.port The value of the port to connect to. Required for SftpExtractor, MySQLExtractor, OracleExtractor, SQLServerExtractor and TeradataExtractor. None source.conn.sid The Oracle System ID (SID) that identifies the database to connect to. Required for OracleExtractor. None extract.table.name Table name in Hadoop which is different table name in source. No Source table name extract.is.full True if this pull should treat the data as a full dump of table from the source, false otherwise. No false extract.delta.fields List of columns that will be used as the delta field for the data. No None extract.primary.key.fields List of columns that will be used as the primary key for the data. No None extract.pull.limit This limits the number of records read by Gobblin. In Gobblin's extractor the readRecord() method is expected to return records until there are no more to pull, in which case it runs null. This parameter limits the number of times readRecord() is executed. This parameter is useful for pulling a limited sample of the source data for testing purposes. No Unbounded extract.full.run.time TODO TODO TODO Distcp CopySource Properties Name Description Required Default Value gobblin.copy.simulate Will perform copy file listing but doesn't execute actual copy. No False gobblin.copy.includeEmptyDirectories Whether to include empty directories from the source in the copy. No False RecursiveCopyableDataset Properties Name Description Required Default Value gobblin.copy.recursive.deleteEmptyDirectories Whether to delete newly empty directories found, up to the dataset root. No False gobblin.copy.recursive.delete Whether to delete files in the target that don't exist in the source. No False gobblin.copy.recursive.update Will update files that are different between the source and target, and skip files already in the target. No False DistcpFileSplitter Properties Name Description Required Default Value gobblin.copy.split.enabled Will split files into block level granularity work units, which can be copied independently, then merged back together before publishing. To actually achieve splitting, the max split size property also needs to be set. No False gobblin.copy.file.max.split.size If splitting is enabled, the split size (in bytes) for the block level work units is calculated based on rounding down the value of this property to the nearest integer multiple of the block size. If the value of this property is less than the block size, it gets adjusted up. No Long.MAX_VALUE WorkUnitBinPacker Properties Name Description Required Default Value gobblin.copy.binPacking.maxSizePerBin Limits the maximum weight that can be packed into a multi work unit produced from bin packing. A value of 0 means packing is not done. No 0 gobblin.copy.binPacking.maxWorkUnitsPerBin Limits the maximum number/amount of work units that can be packed into a multi work unit produced from bin packing. No 50 QueryBasedExtractor Properties The following table lists the query based extractor configuration properties. Name Description Required Default Value source.querybased.watermark.type The format of the watermark that is used when extracting data from the source. Possible types are timestamp, date, hour, simple. Yes timestamp source.querybased.start.value Value for the watermark to start pulling data from, also the default watermark if the previous watermark cannot be found in the old task states. Yes None source.querybased.partition.interval Number of hours to pull in each partition. No 1 source.querybased.hour.column Delta column with hour for hourly extracts (Ex: hour_sk) No None source.querybased.skip.high.watermark.calc If it is true, skips high watermark calculation in the source and it will use partition higher range as high watermark instead of getting it from source. No False source.querybased.query The query that the extractor should execute to pull data. No None source.querybased.hourly.extract True if hourly extract is required. No False source.querybased.extract.type \"snapshot\" for the incremental dimension pulls. \"append_daily\", \"append_hourly\" and \"append_batch\" for the append data append_batch for the data with sequence numbers as watermarks No None source.querybased.end.value The high watermark which this entire job should pull up to. If this is not specified, pull entire data from the table No None source.querybased.append.max.watermark.limit max limit of the high watermark for the append data. CURRENT_DATE - X CURRENT_HOUR - X where X =1 No CURRENT_DATE for daily extract CURRENT_HOUR for hourly extract source.querybased.is.watermark.override True if this pull should override previous watermark with start.value and end.value. False otherwise. No False source.querybased.low.watermark.backup.secs Number of seconds that needs to be backup from the previous high watermark. This is to cover late data. Ex: Set to 3600 to cover 1 hour late data. No 0 source.querybased.schema Database name No None source.querybased.is.specific.api.active True if this pull needs to use source specific apis instead of standard protocols. Ex: Use salesforce bulk api instead of rest api No False source.querybased.skip.count.calc A boolean, if true then the QueryBasedExtractor will skip the source count calculation. No False source.querybased.fetch.size This parameter is currently only used in JDBCExtractor. The JDBCExtractor will process this many number of records from the JDBC ResultSet at a time. It will then take these records and return them to the rest of the Gobblin flow so that they can get processed by the rest of the Gobblin components. No 1000 source.querybased.is.metadata.column.check.enabled When a query is specified in the configuration file, it is possible a user accidentally adds in a column name that does not exist on the source side. By default, this parameter is set to false, which means that if a column is specified in the query and it does not exist in the source data set, Gobblin will just skip over that column. If it is set to true, Gobblin will actually take the config specified column and check to see if it exists in the source data set. If it doesn't exist then the job will fail. No False source.querybased.is.compression.enabled A boolean specifying whether or not compression should be enabled when pulling data from the source. This parameter is only used for MySQL sources. If set to true, the MySQL will send compressed data back to the source. No False source.querybased.jdbc.resultset.fetch.size The number of rows to pull through JDBC at a time. This is useful when the JDBC ResultSet is too big to fit into memory, so only \"x\" number of records will be fetched at a time. No 1000 JdbcExtractor Properties The following table lists the jdbc based extractor configuration properties. Name Description Required Default Value source.conn.driver The fully qualified path of the JDBC driver used to connect to the external source. Yes None source.column.name.case A enum specifying whether or not to convert the column names to a specific case before performing a query. Possible values are TOUPPER or TOLOWER. No NOCHANGE FileBasedExtractor Properties The following table lists the file based extractor configuration properties. Name Description Required Default Value source.filebased.data.directory The data directory from which to pull data from. Yes None source.filebased.files.to.pull A list of files to pull - this should be set in the Source class and the extractor will pull the specified files. Yes None filebased.report.status.on.count The FileBasedExtractor will report it's status every time it processes the number of records specified by this parameter. The way it reports status is by logging out how many records it has seen. No 10000 source.filebased.fs.uri The URI of the filesystem to connect to. Required for HadoopExtractor. None source.filebased.preserve.file.name A boolean, if true then the original file names will be preserved when they are are written to the source. No False source.schema The schema of the data that will be pulled by the source. Yes None SftpExtractor Properties Name Description Required Default Value source.conn.private.key File location of the private key used for key based authentication. This parameter is only used for the SFTP source. Yes None source.conn.known.hosts File location of the known hosts file used for key based authentication. Yes None Converter Properties Properties for Gobblin converters. Name Description Required Default Value converter.classes Comma-separated list of fully qualified names of the Converter classes. The order is important as the converters will be applied in this order. No None CsvToJsonConverter Properties This converter takes in text data separated by a delimiter (converter.csv.to.json.delimiter), and splits the data into a JSON format recognized by JsonIntermediateToAvroConverter. Name Description Required Default Value converter.csv.to.json.delimiter The regex delimiter between CSV based files, only necessary when using the CsvToJsonConverter - e.g. \",\", \"/t\" or some other regex Yes None JsonIntermediateToAvroConverter Properties This converter takes in JSON data in a specific schema, and converts it to Avro data. Name Description Required Default Value converter.avro.date.format Source format of the date columns for Avro-related converters. No None converter.avro.timestamp.format Source format of the timestamp columns for Avro-related converters. No None converter.avro.time.format Source format of the time columns for Avro-related converters. No None converter.avro.binary.charset Source format of the time columns for Avro-related converters. No UTF-8 converter.is.epoch.time.in.seconds A boolean specifying whether or not a epoch time field in the JSON object is in seconds or not. Yes None converter.avro.max.conversion.failures This converter is will fail for this many number of records before throwing an exception. No 0 converter.avro.nullify.fields.enabled Generate new avro schema by nullifying fields that previously existed but not in the current schema. No false converter.avro.nullify.fields.original.schema.path Path of the original avro schema which will be used for merging and nullify fields. No None JsonStringToJsonIntermediateConverter Properties Name Description Required Default Value gobblin.converter.jsonStringToJsonIntermediate.unpackComplexSchemas Parse nested JSON record using source.schema. No True AvroFilterConverter Properties This converter takes in an Avro record, and filters out records by performing an equality operation on the value of the field specified by converter.filter.field and the value specified in converter.filter.value. It returns the record unmodified if the equality operation evaluates to true, false otherwise. Name Description Required Default Value converter.filter.field The name of the field in the Avro record, for which the converter will filter records on. Yes None converter.filter.value The value that will be used in the equality operation to filter out records. Yes None AvroFieldRetrieverConverter Properties This converter takes a specific field from an Avro record and returns its value. Name Description Required Default Value converter.avro.extractor.field.path The field in the Avro record to retrieve. If it is a nested field, then each level must be separated by a period. Yes None AvroFieldsPickConverter Properties Unlike AvroFieldRetriever, this converter takes multiple fields from Avro schema and convert schema and generic record. Name Description Required Default Value converter.avro.fields Comma-separted list of the fields in the Avro record. If it is a nested field, then each level must be separated by a period. Yes None AvroToJdbcEntryConverter Properties Converts Avro schema and generic record into Jdbc entry schema and data. Name Description Required Default Value converter.avro.jdbc.entry_fields_pairs Converts Avro field name(s) to fit for JDBC underlying data base. Input format is key value pairs of JSON array where key is avro field name and value is corresponding JDBC column name. No None Fork Properties Properties for Gobblin's fork operator. Name Description Required Default Value fork.operator.class Fully qualified name of the ForkOperator class. No org.apache.gobblin.fork.IdentityForkOperator fork.branches Number of fork branches. No 1 fork.branch.name.${branch index} Name of a fork branch with the given index, e.g., 0 and 1. No fork_${branch index}, e.g., fork_0 and fork_1. Quality Checker Properties Name Description Required Default Value qualitychecker.task.policies Comma-separted list of fully qualified names of the TaskLevelPolicy classes that will run at the end of each Task. No None qualitychecker.task.policy.types OPTIONAL implies the corresponding class in qualitychecker.task.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too. No OPTIONAL qualitychecker.row.policies Comma-separted list of fully qualified names of the RowLevelPolicy classes that will run on each record. No None qualitychecker.row.policy.types OPTIONAL implies the corresponding class in qualitychecker.row.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too, ERR_FILE implies that if the record does not pass the test then the record will be written to an error file. No OPTIONAL qualitychecker.row.err.file The quality checker will write the current record to the location specified by this parameter, if the current record fails to pass the quality checkers specified by qualitychecker.row.policies; this file will only be written to if the quality checker policy type is ERR_FILE. No None Writer Properties Name Description Required Default Value writer.destination.type Writer destination type. Can be HDFS, KAFKA, MYSQL or TERADATA No HDFS writer.output.format Writer output format; currently only Avro is supported. No AVRO writer.fs.uri File system URI for writer output. No file:/// writer.staging.dir Staging directory of writer output. All staging data that the writer produces will be placed in this directory, but all the data will be eventually moved to the writer.output.dir. Yes None writer.output.dir Output directory of writer output. All output data that the writer produces will be placed in this directory, but all the data will be eventually moved to the final directory by the publisher. Yes None writer.builder.class Fully qualified name of the writer builder class. No org.apache.gobblin.writer.AvroDataWriterBuilder writer.file.path The Path where the writer will write it's data. Data in this directory will be copied to it's final output directory by the DataPublisher. Yes None writer.file.name The name of the file the writer writes to. Yes part writer.partitioner.class Partitioner used for distributing records into multiple output files. writer.builder.class must be a subclass of PartitionAwareDataWriterBuilder , otherwise Gobblin will throw an error. No None (will not use partitioner) writer.buffer.size Writer buffer size in bytes. This parameter is only applicable for the AvroHdfsDataWriter. No 4096 writer.deflate.level Writer deflate level. Deflate is a type of compression for Avro data. No 9 writer.codec.type This is used to specify the type of compression used when writing data out. Possible values are NOCOMPRESSION, DEFLATE, SNAPPY. No DEFLATE writer.eager.initialization This is used to control the writer creation. If the value is set to true, writer is created before records are read. This means an empty file will be created even if no records were read. No False writer.parquet.page.size The page size threshold No 1048576 writer.parquet.dictionary.page.size The block size threshold. No 134217728 writer.parquet.dictionary To turn dictionary encoding on. No true writer.parquet.validate To turn on validation using the schema. No false writer.parquet.version Version of parquet writer to use. Available versions are v1 and v2. No v1 Data Publisher Properties Name Description Required Default Value data.publisher.type The fully qualified name of the DataPublisher class to run. The DataPublisher is responsible for publishing task data once all Tasks have been completed. Yes None data.publisher.final.dir The final output directory where the data should be published. Yes None data.publisher.replace.final.dir A boolean, if true and the the final output directory already exists, then the data will not be committed. If false and the final output directory already exists then it will be overwritten. Yes None data.publisher.final.name The final name of the file that is produced by Gobblin. By default, Gobblin already assigns a unique name to each file it produces. If that default name needs to be overridden then this parameter can be used. Typically, this parameter should be set on a per workunit basis so that file names don't collide. No None Generic Properties These properties are used throughout multiple Gobblin components. Name Description Required Default Value fs.uri Default file system URI for all file storage; over-writable by more specific configuration properties. No file:/// FileBasedJobLock Properties Name Description Required Default Value job.lock.dir Directory where job locks are stored. Job locks are used by the scheduler to ensure two executions of a job do not run at the same time. If a job is scheduled to run, Gobblin will first check this directory to see if there is a lock file for the job. If there is one, it will not run the job, if there isn't one then it will run the job. No None ZookeeperBasedJobLock Properties Name Description Required Default Value zookeeper.connection.string The connection string to the ZooKeeper cluster used to manage the lock. No localhost:2181 zookeeper.session.timeout.seconds The zookeeper session timeout. No 180 zookeeper.connection.timeout.seconds The zookeeper conection timeout. No 30 zookeeper.retry.backoff.seconds The amount of time in seconds to wait between retries. This will increase exponentially when retries occur. No 1 zookeeper.retry.count.max The maximum number of times to retry. No 10 zookeeper.locks.acquire.timeout.milliseconds The amount of time in milliseconds to wait while attempting to acquire the lock. No 5000 zookeeper.locks.reaper.threshold.seconds The threshold in seconds that determines when a lock path can be deleted. No 300 JDBC Writer properties Writer(and publisher) that writes to JDBC database. Please configure below two properties to use JDBC writer publisher. writer.builder.class=org.apache.gobblin.writer.JdbcWriterBuilder data.publisher.type=org.apache.gobblin.publisher.JdbcPublisher Name Description Required Default Value jdbc.publisher.database_name Destination database name Yes None jdbc.publisher.table_name Destination table name Yes None jdbc.publisher.replace_table Gobblin will replace the data in destination table. No false jdbc.publisher.username User name to connect to destination database Yes None jdbc.publisher.password Password to connect to destination database. Also, accepts encrypted password. Yes None jdbc.publisher.encrypt_key_loc Location of a key to decrypt an encrypted password No None jdbc.publisher.url Connection URL Yes None jdbc.publisher.driver JDBC driver class Yes None writer.staging.table User can pass staging table for Gobblin to use instead of Gobblin to create one. (e.g: For the user who does not have create table previlege can pass staging table for Gobblin to use). No None writer.truncate.staging.table Truncate staging table if user passed their own staging table via \"writer.staging.table\". No false writer.jdbc.batch_size Batch size for Insert operation No 30 writer.jdbc.insert_max_param_size Maximum number of parameters for JDBC insert operation (for MySQL Writer). No 100,000 (MySQL limitation)","title":"Configuration Glossary"},{"location":"user-guide/Configuration-Properties-Glossary/#table-of-contents","text":"Properties File Format Creating a Basic Properties File Job Launcher Properties Common Job Launcher Properties SchedulerDaemon Properties CliMRJobLauncher Properties AzkabanJobLauncher Properties Job Type Properties Common Job Type Properties LocalJobLauncher Properties MRJobLauncher Properties Retry Properties Task Execution Properties State Store Properties Metrics Properties Email Alert Properties Source Properties Common Source Properties Distcp CopySource Properties RecursiveCopyableDataset Properties DistcpFileSplitter Properties WorkUnitBinPacker Properties QueryBasedExtractor Properties JdbcExtractor Properties FileBasedExtractor Properties SftpExtractor Properties Converter Properties CsvToJsonConverter Properties JsonIntermediateToAvroConverter Properties JsonStringToJsonIntermediateConverter Properties AvroFilterConverter Properties AvroFieldRetrieverConverter Properties AvroFieldsPickConverter Properties AvroToJdbcEntryConverter Properties Fork Properties Quality Checker Properties Writer Properties Data Publisher Properties Generic Properties FileBasedJobLock Properties ZookeeperBasedJobLock Properties JDBC Writer Properties","title":"Table of Contents"},{"location":"user-guide/Configuration-Properties-Glossary/#properties-file-format","text":"Configuration properties files follow the Java Properties text file format . Further, file includes and variable expansion/interpolation as defined in Apache Commons Configuration are also supported. Example: common.properties writer.staging.dir=/path/to/staging/dir/ writer.output.dir=/path/to/output/dir/ my-job.properties include=common.properties job.name=MyFirstJob","title":"Properties File Format "},{"location":"user-guide/Configuration-Properties-Glossary/#creating-a-basic-properties-file","text":"In order to create a basic configuration property there is a small set of required properties that need to be set. The following properties are required to run any Gobblin job: job.name - Name of the job source.class - Fully qualified path to the Source class responsible for connecting to the data source writer.staging.dir - The directory each task will write staging data to writer.output.dir - The directory each task will commit data to data.publisher.final.dir - The final directory where all the data will be published state.store.dir - The directory where state-store files will be written For more information on each property, check out the comprehensive list below. If only these properties are set, then by default, Gobblin will run in Local mode, as opposed to running on Hadoop M/R. This means Gobblin will write Avro data to the local filesystem. In order to write to HDFS, set the writer.fs.uri property to the URI of the HDFS NameNode that data should be written to. Since the default version of Gobblin writes data in Avro format, the writer expects Avro records to be passed to it. Thus, any data pulled from an external source must be converted to Avro before it can be written out to the filesystem. The source.class property is one of the most important properties in Gobblin. It specifies what Source class to use. The Source class is responsible for determining what work needs to be done during each run of the job, and specifies what Extractor to use in order to read over each sub-unit of data. Examples of Source classes are WikipediaSource and SimpleJsonSource , which can be found in the GitHub repository. For more information on Sources and Extractors, check out the Architecture page. Typically, Gobblin jobs will be launched using the launch scripts in the bin folder. These scripts allow jobs to be launched on the local machine (e.g. SchedulerDaemon) or on Hadoop (e.g. CliMRJobLauncher). Check out the Job Launcher section below, to see the configuration difference between each launch mode. The Deployment page also has more information on the different ways a job can be launched.","title":"Creating a Basic Properties File "},{"location":"user-guide/Configuration-Properties-Glossary/#job-launcher-properties","text":"Gobblin jobs can be launched and scheduled in a variety of ways. They can be scheduled via a Quartz scheduler or through Azkaban . Jobs can also be run without a scheduler via the Command Line. For more information on launching Gobblin jobs, check out the Deployment page.","title":"Job Launcher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#common-job-launcher-properties","text":"These properties are common to both the Job Launcher and the Command Line. Name Description Required Default Value job.name The name of the job to run. This name must be unique within a single Gobblin instance. Yes None job.group A way to group logically similar jobs together. No None job.description A description of what the jobs does. No None job.lock.enabled If set to true job locks are enabled, if set to false they are disabled No True job.lock.type The fully qualified name of the JobLock class to run. The JobLock is responsible for ensuring that only a single instance of a job runs at a time. Allowed values: gobblin.runtime.locks.FileBasedJobLock , gobblin.runtime.locks.ZookeeperBasedJobLock No gobblin.runtime.locks.FileBasedJobLock job.runonce A boolean specifying whether the job will be only once, or multiple times. If set to true the job will only be run once even if a job.schedule is specified. If set to false and a job.schedule is specified then it will run according to the schedule. If set false and a job.schedule is not specified, it will run only once. No False job.disabled Whether the job is disabled or not. If set to true, then Gobblin will not run this job. No False","title":"Common Job Launcher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#schedulerdaemon-properties","text":"This class is used to schedule Gobblin jobs on Quartz. The job can be launched via the command line, and takes in the location of a global configuration file as a parameter. This configuration file should have the property jobconf.dir in order to specify the location of all the .job or .pull files. Another core difference, is that the global configuration file for the SchedulerDaemon must specify the following properties: writer.staging.dir writer.output.dir data.publisher.final.dir state.store.dir They should not be set in individual job files, as they are system-level parameters. For more information on how to set the configuration parameters for jobs launched through the SchedulerDaemon, check out the Deployment page. Name Description Required Default Value job.schedule Cron-Based job schedule. This schedule only applies to jobs that run using Quartz. No None jobconf.dir When running in local mode, Gobblin will check this directory for any configuration files. Each configuration file should correspond to a separate Gobblin job, and each one should in a suffix specified by the jobconf.extensions parameter. No None jobconf.extensions Comma-separated list of supported job configuration file extensions. When running in local mode, Gobblin will only pick up job files ending in these suffixes. No pull,job jobconf.monitor.interval Controls how often Gobblin checks the jobconf.dir for new configuration files, or for configuration file updates. The parameter is measured in milliseconds. No 300000","title":"SchedulerDaemon Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#climrjoblauncher-properties","text":"There are no configuration parameters specific to CliMRJobLauncher. This class is used to launch Gobblin jobs on Hadoop from the command line, the jobs are not scheduled. Common properties are set using the --sysconfig option when launching jobs via the command line. For more information on how to set the configuration parameters for jobs launched through the command line, check out the Deployment page.","title":"CliMRJobLauncher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#azkabanjoblauncher-properties","text":"There are no configuration parameters specific to AzkabanJobLauncher. This class is used to schedule Gobblin jobs on Azkaban. Common properties can be set through Azkaban by creating a .properties file, check out the Azkaban Documentation for more information. For more information on how to set the configuration parameters for jobs scheduled through the Azkaban, check out the Deployment page.","title":"AzkabanJobLauncher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#job-type-properties","text":"","title":"Job Type Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#common-job-type-properties","text":"Name Description Required Default Value launcher.type Job launcher type; one of LOCAL, MAPREDUCE, YARN. LOCAL mode runs on a single machine (LocalJobLauncher), MAPREDUCE runs on a Hadoop cluster (MRJobLauncher), and YARN runs on a YARN cluster (not implemented yet). No LOCAL","title":"Common Job Type Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#localjoblauncher-properties","text":"There are no configuration parameters specific to LocalJobLauncher. The LocalJobLauncher will launch a Hadoop job on a single machine. If launcher.type is set to LOCAL then this class will be used to launch the job. Properties required by the MRJobLauncher class. Name Description Required Default Value framework.jars Comma-separated list of jars the Gobblin framework depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches. No None job.jars Comma-separated list of jar files the job depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches. No None job.hdfs.jars Comma-separated list of jar files the job depends on located in HDFS. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches. No None job.local.files Comma-separated list of local files the job depends on. These files will be available to any map tasks that get launched via the DistributedCache. No None job.hdfs.files Comma-separated list of files on HDFS the job depends on. These files will be available to any map tasks that get launched via the DistributedCache. No None","title":"LocalJobLauncher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#mrjoblauncher-properties","text":"Name Description Required Default Value mr.job.root.dir Working directory for a Gobblin Hadoop MR job. Gobblin uses this to write intermediate data, such as the workunit state files that are used by each map task. This has to be a path on HDFS. Yes None mr.job.max.mappers Maximum number of mappers to use in a Gobblin Hadoop MR job. If no explicit limit is set then a map task for each workunit will be launched. If the value of this properties is less than the number of workunits created, then each map task will run multiple tasks. No None mr.include.task.counters Whether to include task-level counters in the set of counters reported as Hadoop counters. Hadoop imposes a system-level limit (default to 120) on the number of counters, so a Gobblin MR job may easily go beyond that limit if the job has a large number of tasks and each task has a few counters. This property gives users an option to not include task-level counters to avoid going over that limit. Yes False","title":"MRJobLauncher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#retry-properties","text":"Properties that control how tasks and jobs get retried on failure. Name Description Required Default Value workunit.retry.enabled Whether retries of failed work units across job runs are enabled or not. No True workunit.retry.policy Work unit retry policy, can be one of {always, never, onfull, onpartial}. No always task.maxretries Maximum number of task retries. A task will be re-tried this many times before it is considered a failure. No 5 task.retry.intervalinsec Interval in seconds between task retries. The interval increases linearly with each retry. For example, if the first interval is 300 seconds, then the second one is 600 seconds, etc. No 300 job.max.failures Maximum number of failures before an alert email is triggered. No 1","title":"Retry Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#task-execution-properties","text":"These properties control how tasks get executed for a job. Gobblin uses thread pools in order to executes the tasks for a specific job. In local mode there is a single thread pool per job that executes all the tasks for a job. In MR mode there is a thread pool for each map task (or container), and all Gobblin tasks assigned to that mapper are executed in that thread pool. Name Description Required Default Value taskexecutor.threadpool.size Size of the thread pool used by task executor for task execution. Each task executor will spawn this many threads to execute any Tasks that is has been allocated. No 10 tasktracker.threadpool.coresize Core size of the thread pool used by task tracker for task state tracking and reporting. No 10 tasktracker.threadpool.maxsize Maximum size of the thread pool used by task tracker for task state tracking and reporting. No 10 taskretry.threadpool.coresize Core size of the thread pool used by the task executor for task retries. No 2 taskretry.threadpool.maxsize Maximum size of the thread pool used by the task executor for task retries. No 2 task.status.reportintervalinms Task status reporting interval in milliseconds. No 30000","title":"Task Execution Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#state-store-properties","text":"Name Description Required Default Value state.store.dir Root directory where job and task state files are stored. The state-store is used by Gobblin to track state between different executions of a job. All state-store files will be written to this directory. Yes None state.store.fs.uri File system URI for file-system-based state stores. No file:///","title":"State Store Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#metrics-properties","text":"Name Description Required Default Value metrics.enabled Whether metrics collecting and reporting are enabled or not. No True metrics.report.interval Metrics reporting interval in milliseconds. No 60000 metrics.log.dir The directory where metric files will be written to. No None metrics.reporting.file.enabled A boolean indicating whether or not metrics should be reported to a file. No True metrics.reporting.jmx.enabled A boolean indicating whether or not metrics should be exposed via JMX. No False","title":"Metrics Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#email-alert-properties","text":"Name Description Required Default Value email.alert.enabled Whether alert emails are enabled or not. Email alerts are only sent out when jobs fail consecutively job.max.failures number of times. No False email.notification.enabled Whether job completion notification emails are enabled or not. Notification emails are sent whenever the job completes, regardless of whether it failed or not. No False email.host Host name of the email server. Yes, if email notifications or alerts are enabled. None email.smtp.port SMTP port number. Yes, if email notifications or alerts are enabled. None email.user User name of the sender email account. No None email.password User password of the sender email account. No None email.from Sender email address. Yes, if email notifications or alerts are enabled. None email.tos Comma-separated list of recipient email addresses. Yes, if email notifications or alerts are enabled. None","title":"Email Alert Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#source-properties","text":"","title":"Source Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#common-source-properties","text":"These properties are common properties that are used among different Source implementations. Depending on what source class is being used, these parameters may or may not be necessary. These parameters are not tied to a specific source, and thus can be used in new source classes. Name Description Required Default Value source.class Fully qualified name of the Source class. For example, org.apache.gobblin.example.wikipedia.WikipediaSource Yes None source.entity Name of the source entity that needs to be pulled from the source. The parameter represents a logical grouping of data that needs to be pulled from the source. Often this logical grouping comes in the form a database table, a source topic, etc. In many situations, such as when using the QueryBasedExtractor, it will be the name of the table that needs to pulled from the source. Required for QueryBasedExtractors, FileBasedExtractors. None source.timezone Timezone of the data being pulled in by the extractor. Examples include \"PST\" or \"UTC\". Required for QueryBasedExtractors None source.max.number.of.partitions Maximum number of partitions to split this current run across. Only used by the QueryBasedSource and FileBasedSource. No 20 source.skip.first.record True if you want to skip the first record of each data partition. Only used by the FileBasedExtractor. No False extract.namespace Namespace for the extract data. The namespace will be included in the default file name of the outputted data. No None source.conn.use.proxy.url The URL of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources. No None source.conn.use.proxy.port The port of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources. No None source.conn.username The username to authenticate with the source. This is parameter is only used for SFTP and JDBC sources. No None source.conn.password The password to use when authenticating with the source. This is parameter is only used for JDBC sources. No None source.conn.host The name of the host to connect to. Required for SftpExtractor, MySQLExtractor, OracleExtractor, SQLServerExtractor and TeradataExtractor. None source.conn.rest.url URL to connect to for REST requests. This parameter is only used for the Salesforce source. No None source.conn.version Version number of communication protocol. This parameter is only used for the Salesforce source. No None source.conn.timeout The timeout set for connecting to the source in milliseconds. No 500000 source.conn.port The value of the port to connect to. Required for SftpExtractor, MySQLExtractor, OracleExtractor, SQLServerExtractor and TeradataExtractor. None source.conn.sid The Oracle System ID (SID) that identifies the database to connect to. Required for OracleExtractor. None extract.table.name Table name in Hadoop which is different table name in source. No Source table name extract.is.full True if this pull should treat the data as a full dump of table from the source, false otherwise. No false extract.delta.fields List of columns that will be used as the delta field for the data. No None extract.primary.key.fields List of columns that will be used as the primary key for the data. No None extract.pull.limit This limits the number of records read by Gobblin. In Gobblin's extractor the readRecord() method is expected to return records until there are no more to pull, in which case it runs null. This parameter limits the number of times readRecord() is executed. This parameter is useful for pulling a limited sample of the source data for testing purposes. No Unbounded extract.full.run.time TODO TODO TODO","title":"Common Source Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#distcp-copysource-properties","text":"Name Description Required Default Value gobblin.copy.simulate Will perform copy file listing but doesn't execute actual copy. No False gobblin.copy.includeEmptyDirectories Whether to include empty directories from the source in the copy. No False","title":"Distcp CopySource Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#recursivecopyabledataset-properties","text":"Name Description Required Default Value gobblin.copy.recursive.deleteEmptyDirectories Whether to delete newly empty directories found, up to the dataset root. No False gobblin.copy.recursive.delete Whether to delete files in the target that don't exist in the source. No False gobblin.copy.recursive.update Will update files that are different between the source and target, and skip files already in the target. No False","title":"RecursiveCopyableDataset Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#distcpfilesplitter-properties","text":"Name Description Required Default Value gobblin.copy.split.enabled Will split files into block level granularity work units, which can be copied independently, then merged back together before publishing. To actually achieve splitting, the max split size property also needs to be set. No False gobblin.copy.file.max.split.size If splitting is enabled, the split size (in bytes) for the block level work units is calculated based on rounding down the value of this property to the nearest integer multiple of the block size. If the value of this property is less than the block size, it gets adjusted up. No Long.MAX_VALUE","title":"DistcpFileSplitter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#workunitbinpacker-properties","text":"Name Description Required Default Value gobblin.copy.binPacking.maxSizePerBin Limits the maximum weight that can be packed into a multi work unit produced from bin packing. A value of 0 means packing is not done. No 0 gobblin.copy.binPacking.maxWorkUnitsPerBin Limits the maximum number/amount of work units that can be packed into a multi work unit produced from bin packing. No 50","title":"WorkUnitBinPacker Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#querybasedextractor-properties","text":"The following table lists the query based extractor configuration properties. Name Description Required Default Value source.querybased.watermark.type The format of the watermark that is used when extracting data from the source. Possible types are timestamp, date, hour, simple. Yes timestamp source.querybased.start.value Value for the watermark to start pulling data from, also the default watermark if the previous watermark cannot be found in the old task states. Yes None source.querybased.partition.interval Number of hours to pull in each partition. No 1 source.querybased.hour.column Delta column with hour for hourly extracts (Ex: hour_sk) No None source.querybased.skip.high.watermark.calc If it is true, skips high watermark calculation in the source and it will use partition higher range as high watermark instead of getting it from source. No False source.querybased.query The query that the extractor should execute to pull data. No None source.querybased.hourly.extract True if hourly extract is required. No False source.querybased.extract.type \"snapshot\" for the incremental dimension pulls. \"append_daily\", \"append_hourly\" and \"append_batch\" for the append data append_batch for the data with sequence numbers as watermarks No None source.querybased.end.value The high watermark which this entire job should pull up to. If this is not specified, pull entire data from the table No None source.querybased.append.max.watermark.limit max limit of the high watermark for the append data. CURRENT_DATE - X CURRENT_HOUR - X where X =1 No CURRENT_DATE for daily extract CURRENT_HOUR for hourly extract source.querybased.is.watermark.override True if this pull should override previous watermark with start.value and end.value. False otherwise. No False source.querybased.low.watermark.backup.secs Number of seconds that needs to be backup from the previous high watermark. This is to cover late data. Ex: Set to 3600 to cover 1 hour late data. No 0 source.querybased.schema Database name No None source.querybased.is.specific.api.active True if this pull needs to use source specific apis instead of standard protocols. Ex: Use salesforce bulk api instead of rest api No False source.querybased.skip.count.calc A boolean, if true then the QueryBasedExtractor will skip the source count calculation. No False source.querybased.fetch.size This parameter is currently only used in JDBCExtractor. The JDBCExtractor will process this many number of records from the JDBC ResultSet at a time. It will then take these records and return them to the rest of the Gobblin flow so that they can get processed by the rest of the Gobblin components. No 1000 source.querybased.is.metadata.column.check.enabled When a query is specified in the configuration file, it is possible a user accidentally adds in a column name that does not exist on the source side. By default, this parameter is set to false, which means that if a column is specified in the query and it does not exist in the source data set, Gobblin will just skip over that column. If it is set to true, Gobblin will actually take the config specified column and check to see if it exists in the source data set. If it doesn't exist then the job will fail. No False source.querybased.is.compression.enabled A boolean specifying whether or not compression should be enabled when pulling data from the source. This parameter is only used for MySQL sources. If set to true, the MySQL will send compressed data back to the source. No False source.querybased.jdbc.resultset.fetch.size The number of rows to pull through JDBC at a time. This is useful when the JDBC ResultSet is too big to fit into memory, so only \"x\" number of records will be fetched at a time. No 1000","title":"QueryBasedExtractor Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#jdbcextractor-properties","text":"The following table lists the jdbc based extractor configuration properties. Name Description Required Default Value source.conn.driver The fully qualified path of the JDBC driver used to connect to the external source. Yes None source.column.name.case A enum specifying whether or not to convert the column names to a specific case before performing a query. Possible values are TOUPPER or TOLOWER. No NOCHANGE","title":"JdbcExtractor Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#filebasedextractor-properties","text":"The following table lists the file based extractor configuration properties. Name Description Required Default Value source.filebased.data.directory The data directory from which to pull data from. Yes None source.filebased.files.to.pull A list of files to pull - this should be set in the Source class and the extractor will pull the specified files. Yes None filebased.report.status.on.count The FileBasedExtractor will report it's status every time it processes the number of records specified by this parameter. The way it reports status is by logging out how many records it has seen. No 10000 source.filebased.fs.uri The URI of the filesystem to connect to. Required for HadoopExtractor. None source.filebased.preserve.file.name A boolean, if true then the original file names will be preserved when they are are written to the source. No False source.schema The schema of the data that will be pulled by the source. Yes None","title":"FileBasedExtractor Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#sftpextractor-properties","text":"Name Description Required Default Value source.conn.private.key File location of the private key used for key based authentication. This parameter is only used for the SFTP source. Yes None source.conn.known.hosts File location of the known hosts file used for key based authentication. Yes None","title":"SftpExtractor Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#converter-properties","text":"Properties for Gobblin converters. Name Description Required Default Value converter.classes Comma-separated list of fully qualified names of the Converter classes. The order is important as the converters will be applied in this order. No None","title":"Converter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#csvtojsonconverter-properties","text":"This converter takes in text data separated by a delimiter (converter.csv.to.json.delimiter), and splits the data into a JSON format recognized by JsonIntermediateToAvroConverter. Name Description Required Default Value converter.csv.to.json.delimiter The regex delimiter between CSV based files, only necessary when using the CsvToJsonConverter - e.g. \",\", \"/t\" or some other regex Yes None","title":"CsvToJsonConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#jsonintermediatetoavroconverter-properties","text":"This converter takes in JSON data in a specific schema, and converts it to Avro data. Name Description Required Default Value converter.avro.date.format Source format of the date columns for Avro-related converters. No None converter.avro.timestamp.format Source format of the timestamp columns for Avro-related converters. No None converter.avro.time.format Source format of the time columns for Avro-related converters. No None converter.avro.binary.charset Source format of the time columns for Avro-related converters. No UTF-8 converter.is.epoch.time.in.seconds A boolean specifying whether or not a epoch time field in the JSON object is in seconds or not. Yes None converter.avro.max.conversion.failures This converter is will fail for this many number of records before throwing an exception. No 0 converter.avro.nullify.fields.enabled Generate new avro schema by nullifying fields that previously existed but not in the current schema. No false converter.avro.nullify.fields.original.schema.path Path of the original avro schema which will be used for merging and nullify fields. No None","title":"JsonIntermediateToAvroConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#jsonstringtojsonintermediateconverter-properties","text":"Name Description Required Default Value gobblin.converter.jsonStringToJsonIntermediate.unpackComplexSchemas Parse nested JSON record using source.schema. No True","title":"JsonStringToJsonIntermediateConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#avrofilterconverter-properties","text":"This converter takes in an Avro record, and filters out records by performing an equality operation on the value of the field specified by converter.filter.field and the value specified in converter.filter.value. It returns the record unmodified if the equality operation evaluates to true, false otherwise. Name Description Required Default Value converter.filter.field The name of the field in the Avro record, for which the converter will filter records on. Yes None converter.filter.value The value that will be used in the equality operation to filter out records. Yes None","title":"AvroFilterConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#avrofieldretrieverconverter-properties","text":"This converter takes a specific field from an Avro record and returns its value. Name Description Required Default Value converter.avro.extractor.field.path The field in the Avro record to retrieve. If it is a nested field, then each level must be separated by a period. Yes None","title":"AvroFieldRetrieverConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#avrofieldspickconverter-properties","text":"Unlike AvroFieldRetriever, this converter takes multiple fields from Avro schema and convert schema and generic record. Name Description Required Default Value converter.avro.fields Comma-separted list of the fields in the Avro record. If it is a nested field, then each level must be separated by a period. Yes None","title":"AvroFieldsPickConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#avrotojdbcentryconverter-properties","text":"Converts Avro schema and generic record into Jdbc entry schema and data. Name Description Required Default Value converter.avro.jdbc.entry_fields_pairs Converts Avro field name(s) to fit for JDBC underlying data base. Input format is key value pairs of JSON array where key is avro field name and value is corresponding JDBC column name. No None","title":"AvroToJdbcEntryConverter Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#fork-properties","text":"Properties for Gobblin's fork operator. Name Description Required Default Value fork.operator.class Fully qualified name of the ForkOperator class. No org.apache.gobblin.fork.IdentityForkOperator fork.branches Number of fork branches. No 1 fork.branch.name.${branch index} Name of a fork branch with the given index, e.g., 0 and 1. No fork_${branch index}, e.g., fork_0 and fork_1.","title":"Fork Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#quality-checker-properties","text":"Name Description Required Default Value qualitychecker.task.policies Comma-separted list of fully qualified names of the TaskLevelPolicy classes that will run at the end of each Task. No None qualitychecker.task.policy.types OPTIONAL implies the corresponding class in qualitychecker.task.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too. No OPTIONAL qualitychecker.row.policies Comma-separted list of fully qualified names of the RowLevelPolicy classes that will run on each record. No None qualitychecker.row.policy.types OPTIONAL implies the corresponding class in qualitychecker.row.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too, ERR_FILE implies that if the record does not pass the test then the record will be written to an error file. No OPTIONAL qualitychecker.row.err.file The quality checker will write the current record to the location specified by this parameter, if the current record fails to pass the quality checkers specified by qualitychecker.row.policies; this file will only be written to if the quality checker policy type is ERR_FILE. No None","title":"Quality Checker Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#writer-properties","text":"Name Description Required Default Value writer.destination.type Writer destination type. Can be HDFS, KAFKA, MYSQL or TERADATA No HDFS writer.output.format Writer output format; currently only Avro is supported. No AVRO writer.fs.uri File system URI for writer output. No file:/// writer.staging.dir Staging directory of writer output. All staging data that the writer produces will be placed in this directory, but all the data will be eventually moved to the writer.output.dir. Yes None writer.output.dir Output directory of writer output. All output data that the writer produces will be placed in this directory, but all the data will be eventually moved to the final directory by the publisher. Yes None writer.builder.class Fully qualified name of the writer builder class. No org.apache.gobblin.writer.AvroDataWriterBuilder writer.file.path The Path where the writer will write it's data. Data in this directory will be copied to it's final output directory by the DataPublisher. Yes None writer.file.name The name of the file the writer writes to. Yes part writer.partitioner.class Partitioner used for distributing records into multiple output files. writer.builder.class must be a subclass of PartitionAwareDataWriterBuilder , otherwise Gobblin will throw an error. No None (will not use partitioner) writer.buffer.size Writer buffer size in bytes. This parameter is only applicable for the AvroHdfsDataWriter. No 4096 writer.deflate.level Writer deflate level. Deflate is a type of compression for Avro data. No 9 writer.codec.type This is used to specify the type of compression used when writing data out. Possible values are NOCOMPRESSION, DEFLATE, SNAPPY. No DEFLATE writer.eager.initialization This is used to control the writer creation. If the value is set to true, writer is created before records are read. This means an empty file will be created even if no records were read. No False writer.parquet.page.size The page size threshold No 1048576 writer.parquet.dictionary.page.size The block size threshold. No 134217728 writer.parquet.dictionary To turn dictionary encoding on. No true writer.parquet.validate To turn on validation using the schema. No false writer.parquet.version Version of parquet writer to use. Available versions are v1 and v2. No v1","title":"Writer Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#data-publisher-properties","text":"Name Description Required Default Value data.publisher.type The fully qualified name of the DataPublisher class to run. The DataPublisher is responsible for publishing task data once all Tasks have been completed. Yes None data.publisher.final.dir The final output directory where the data should be published. Yes None data.publisher.replace.final.dir A boolean, if true and the the final output directory already exists, then the data will not be committed. If false and the final output directory already exists then it will be overwritten. Yes None data.publisher.final.name The final name of the file that is produced by Gobblin. By default, Gobblin already assigns a unique name to each file it produces. If that default name needs to be overridden then this parameter can be used. Typically, this parameter should be set on a per workunit basis so that file names don't collide. No None","title":"Data Publisher Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#generic-properties","text":"These properties are used throughout multiple Gobblin components. Name Description Required Default Value fs.uri Default file system URI for all file storage; over-writable by more specific configuration properties. No file:///","title":"Generic Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#filebasedjoblock-properties","text":"Name Description Required Default Value job.lock.dir Directory where job locks are stored. Job locks are used by the scheduler to ensure two executions of a job do not run at the same time. If a job is scheduled to run, Gobblin will first check this directory to see if there is a lock file for the job. If there is one, it will not run the job, if there isn't one then it will run the job. No None","title":"FileBasedJobLock Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#zookeeperbasedjoblock-properties","text":"Name Description Required Default Value zookeeper.connection.string The connection string to the ZooKeeper cluster used to manage the lock. No localhost:2181 zookeeper.session.timeout.seconds The zookeeper session timeout. No 180 zookeeper.connection.timeout.seconds The zookeeper conection timeout. No 30 zookeeper.retry.backoff.seconds The amount of time in seconds to wait between retries. This will increase exponentially when retries occur. No 1 zookeeper.retry.count.max The maximum number of times to retry. No 10 zookeeper.locks.acquire.timeout.milliseconds The amount of time in milliseconds to wait while attempting to acquire the lock. No 5000 zookeeper.locks.reaper.threshold.seconds The threshold in seconds that determines when a lock path can be deleted. No 300","title":"ZookeeperBasedJobLock Properties "},{"location":"user-guide/Configuration-Properties-Glossary/#jdbc-writer-properties","text":"Writer(and publisher) that writes to JDBC database. Please configure below two properties to use JDBC writer publisher. writer.builder.class=org.apache.gobblin.writer.JdbcWriterBuilder data.publisher.type=org.apache.gobblin.publisher.JdbcPublisher Name Description Required Default Value jdbc.publisher.database_name Destination database name Yes None jdbc.publisher.table_name Destination table name Yes None jdbc.publisher.replace_table Gobblin will replace the data in destination table. No false jdbc.publisher.username User name to connect to destination database Yes None jdbc.publisher.password Password to connect to destination database. Also, accepts encrypted password. Yes None jdbc.publisher.encrypt_key_loc Location of a key to decrypt an encrypted password No None jdbc.publisher.url Connection URL Yes None jdbc.publisher.driver JDBC driver class Yes None writer.staging.table User can pass staging table for Gobblin to use instead of Gobblin to create one. (e.g: For the user who does not have create table previlege can pass staging table for Gobblin to use). No None writer.truncate.staging.table Truncate staging table if user passed their own staging table via \"writer.staging.table\". No false writer.jdbc.batch_size Batch size for Insert operation No 30 writer.jdbc.insert_max_param_size Maximum number of parameters for JDBC insert operation (for MySQL Writer). No 100,000 (MySQL limitation)","title":"JDBC Writer properties "},{"location":"user-guide/Docker-Integration/","text":"Table of Contents Table of Contents Introduction Docker Docker Repositories Gobblin-Wikipedia Repository Gobblin-Standalone Repository Future Work Introduction Gobblin integrates with Docker by running a Gobblin standalone service inside a Docker container. The Gobblin service inside the container can monitor the host filesystem for new job configuration files, run the jobs, and write the resulting data to the host filesystem. The Gobblin Docker images can be found on Docker Hub at: https://hub.docker.com/u/gobblin/ Docker For more information on Docker, including how to install it, check out the documentation at: https://docs.docker.com/ Docker Repositories Gobblin currently has four different repositories, and all are on Docker Hub here . The gobblin/gobblin-wikipedia repository contains images that run the Gobblin Wikipedia job found in the getting started guide . These images are useful for users new to Docker or Gobblin, they primarily act as a \"Hello World\" example for the Gobblin Docker integration. The gobblin/gobblin-standalone repository contains images that run a Gobblin standalone service inside a Docker container. These images provide an easy and simple way to setup a Gobblin standalone service on any Docker compatible machine. The gobblin/gobblin-base and gobblin/gobblin-distributions repositories are for internal use only, and are primarily useful for Gobblin developers. Gobblin-Wikipedia Repository The Docker images for this repository can be found on Docker Hub here . These images are mainly meant to act as a \"Hello World\" example for the Gobblin-Docker integration, and to provide a sanity check to see if the Gobblin-Docker integration is working on a given machine. The image contains the Gobblin configuration files to run the Gobblin Wikipedia job . When a container is launched using the gobblin-wikipedia image, Gobblin starts up, runs the Wikipedia example, and then exits. Running the gobblin-wikipedia image requires taking following steps (lets assume we want to an Ubuntu based image): Download the images from the gobblin/gobblin-wikipedia repository docker pull gobblin/gobblin-wikipedia:ubuntu-gobblin-latest Run the gobblin/gobblin-wikipedia:ubuntu-gobblin-latest image in a Docker container docker run gobblin/gobblin-wikipedia:ubuntu-gobblin-latest The logs are printed to the console, and no errors should pop up. This should provide a nice sanity check to ensure that everything is working as expected. The output of the job will be written to a directory inside the container. When the container exits that data will be lost. In order to preserve the output of the job, continue to the next step. Preserving the output of a Docker container requires using a data volume . To do this, run the below command: docker run -v /home/gobblin/work-dir:/home/gobblin/work-dir gobblin-wikipedia The output of the Gobblin-Wikipedia job should now be written to /home/gobblin/work-dir/job-output . The -v command in Docker uses a feature of Docker called data volumes . The -v option mounts a host directory into a container and is of the form [host-directory]:[container-directory] . Now any modifications to the host directory can be seen inside the container-directory, and any modifications to the container-directory can be seen inside the host-directory. This is a standard way to ensure data persists even after a Docker container finishes. It's important to note that the [host-directory] in the -v option can be changed to any directory (on OSX it must be under the /Users/ directory), but the [container-directory] must remain /home/gobblin/work-dir (at least for now). Gobblin-Standalone Repository The Docker images for this repository can be found on Docker Hub here . These images run a Gobblin standalone service inside a Docker container. The Gobblin standalone service is a long running process that can run Gobblin jobs defined in a .job or .pull file. The job / pull files are submitted to the standalone service by placing them in a directory on the local filesystem. The standalone service monitors this directory for any new job / pull files and runs them either immediately or on a scheduled basis (more information on how this works can be found here ). Running the Gobblin standalone service inside a Docker container allows Gobblin to pick up job / pull files from a directory on the host filesystem, run the job, and write the output back the host filesystem. All the heavy lifting is done inside a Docker container, the user just needs to worry about defining and submitting job / pull files. The goal is to provide a easy to setup environment for the Gobblin standalone service. Running the gobblin-standalone image requires taking the following steps: Download the images from the gobblin/gobblin-standalone repository docker pull gobblin/gobblin-standalone:ubuntu-gobblin-latest Run the gobblin/gobblin-standalone:ubuntu-gobblin-latest image in a Docker container docker run -v /home/gobblin/conf:/etc/opt/job-conf \\ -v /home/gobblin/work-dir:/home/gobblin/work-dir \\ -v /home/gobblin/logs:/var/log/gobblin \\ gobblin/gobblin-standalone:ubuntu-gobblin-latest A data volume needs to be created for the job configuration directory (contains all the job configuration files), the work directory (contains all the job output data), and the logs directory (contains all the Gobblin standalone logs). The -v /home/gobblin/conf:/etc/opt/job-conf option allows any new job / pull files added to the /home/gobblin/conf directory on the host filesystem will be seen by the Gobblin standalone service inside the container. So any job / pull added to the /home/gobblin/conf directory on the local filesystem will be run by the Gobblin standalone inside running inside the Docker container. Note the container directory ( /etc/opt/job-conf ) should not be modified, while the host directory ( /home/gobblin/conf ) directory can be any directory on the host filesystem that contains job / pull files. The -v /home/gobblin/work-dir:/home/gobblin/work-dir option allows the container to write data to the host filesystem, so that the data persists after the container is shutdown. Once again, the container directory ( /home/gobblin/work-dir ) should not be modified, while the host directory ( /home/gobblin/work-dir ) can be any directory on the host filesystem. The -v /home/gobblin/logs:/var/log/gobblin option allows the Gobblin standalone logs to be written to the host filesystem, so that they can be read on the host machine. This is useful for monitoring and debugging purposes. Once again, the container directory ( /var/log/gobblin ) directory should not be modified, while the container directory ( /home/gobblin/logs ) can be any directory on the host filesystem. Future Work Create gobblin-dev images that provide an development environment for Gobblin contributors Create gobblin-kafka images that provide an end-to-end service for writing to Kafka and ingesting the Kafka data through Gobblin Test and write a tutorial on using gobblin-standalone images to write to a HDFS cluster Create images based on Linux Alpine (lightweight Linux distro)","title":"Docker Integration"},{"location":"user-guide/Docker-Integration/#table-of-contents","text":"Table of Contents Introduction Docker Docker Repositories Gobblin-Wikipedia Repository Gobblin-Standalone Repository Future Work","title":"Table of Contents"},{"location":"user-guide/Docker-Integration/#introduction","text":"Gobblin integrates with Docker by running a Gobblin standalone service inside a Docker container. The Gobblin service inside the container can monitor the host filesystem for new job configuration files, run the jobs, and write the resulting data to the host filesystem. The Gobblin Docker images can be found on Docker Hub at: https://hub.docker.com/u/gobblin/","title":"Introduction"},{"location":"user-guide/Docker-Integration/#docker","text":"For more information on Docker, including how to install it, check out the documentation at: https://docs.docker.com/","title":"Docker"},{"location":"user-guide/Docker-Integration/#docker-repositories","text":"Gobblin currently has four different repositories, and all are on Docker Hub here . The gobblin/gobblin-wikipedia repository contains images that run the Gobblin Wikipedia job found in the getting started guide . These images are useful for users new to Docker or Gobblin, they primarily act as a \"Hello World\" example for the Gobblin Docker integration. The gobblin/gobblin-standalone repository contains images that run a Gobblin standalone service inside a Docker container. These images provide an easy and simple way to setup a Gobblin standalone service on any Docker compatible machine. The gobblin/gobblin-base and gobblin/gobblin-distributions repositories are for internal use only, and are primarily useful for Gobblin developers.","title":"Docker Repositories"},{"location":"user-guide/Docker-Integration/#gobblin-wikipedia-repository","text":"The Docker images for this repository can be found on Docker Hub here . These images are mainly meant to act as a \"Hello World\" example for the Gobblin-Docker integration, and to provide a sanity check to see if the Gobblin-Docker integration is working on a given machine. The image contains the Gobblin configuration files to run the Gobblin Wikipedia job . When a container is launched using the gobblin-wikipedia image, Gobblin starts up, runs the Wikipedia example, and then exits. Running the gobblin-wikipedia image requires taking following steps (lets assume we want to an Ubuntu based image): Download the images from the gobblin/gobblin-wikipedia repository docker pull gobblin/gobblin-wikipedia:ubuntu-gobblin-latest Run the gobblin/gobblin-wikipedia:ubuntu-gobblin-latest image in a Docker container docker run gobblin/gobblin-wikipedia:ubuntu-gobblin-latest The logs are printed to the console, and no errors should pop up. This should provide a nice sanity check to ensure that everything is working as expected. The output of the job will be written to a directory inside the container. When the container exits that data will be lost. In order to preserve the output of the job, continue to the next step. Preserving the output of a Docker container requires using a data volume . To do this, run the below command: docker run -v /home/gobblin/work-dir:/home/gobblin/work-dir gobblin-wikipedia The output of the Gobblin-Wikipedia job should now be written to /home/gobblin/work-dir/job-output . The -v command in Docker uses a feature of Docker called data volumes . The -v option mounts a host directory into a container and is of the form [host-directory]:[container-directory] . Now any modifications to the host directory can be seen inside the container-directory, and any modifications to the container-directory can be seen inside the host-directory. This is a standard way to ensure data persists even after a Docker container finishes. It's important to note that the [host-directory] in the -v option can be changed to any directory (on OSX it must be under the /Users/ directory), but the [container-directory] must remain /home/gobblin/work-dir (at least for now).","title":"Gobblin-Wikipedia Repository"},{"location":"user-guide/Docker-Integration/#gobblin-standalone-repository","text":"The Docker images for this repository can be found on Docker Hub here . These images run a Gobblin standalone service inside a Docker container. The Gobblin standalone service is a long running process that can run Gobblin jobs defined in a .job or .pull file. The job / pull files are submitted to the standalone service by placing them in a directory on the local filesystem. The standalone service monitors this directory for any new job / pull files and runs them either immediately or on a scheduled basis (more information on how this works can be found here ). Running the Gobblin standalone service inside a Docker container allows Gobblin to pick up job / pull files from a directory on the host filesystem, run the job, and write the output back the host filesystem. All the heavy lifting is done inside a Docker container, the user just needs to worry about defining and submitting job / pull files. The goal is to provide a easy to setup environment for the Gobblin standalone service. Running the gobblin-standalone image requires taking the following steps: Download the images from the gobblin/gobblin-standalone repository docker pull gobblin/gobblin-standalone:ubuntu-gobblin-latest Run the gobblin/gobblin-standalone:ubuntu-gobblin-latest image in a Docker container docker run -v /home/gobblin/conf:/etc/opt/job-conf \\ -v /home/gobblin/work-dir:/home/gobblin/work-dir \\ -v /home/gobblin/logs:/var/log/gobblin \\ gobblin/gobblin-standalone:ubuntu-gobblin-latest A data volume needs to be created for the job configuration directory (contains all the job configuration files), the work directory (contains all the job output data), and the logs directory (contains all the Gobblin standalone logs). The -v /home/gobblin/conf:/etc/opt/job-conf option allows any new job / pull files added to the /home/gobblin/conf directory on the host filesystem will be seen by the Gobblin standalone service inside the container. So any job / pull added to the /home/gobblin/conf directory on the local filesystem will be run by the Gobblin standalone inside running inside the Docker container. Note the container directory ( /etc/opt/job-conf ) should not be modified, while the host directory ( /home/gobblin/conf ) directory can be any directory on the host filesystem that contains job / pull files. The -v /home/gobblin/work-dir:/home/gobblin/work-dir option allows the container to write data to the host filesystem, so that the data persists after the container is shutdown. Once again, the container directory ( /home/gobblin/work-dir ) should not be modified, while the host directory ( /home/gobblin/work-dir ) can be any directory on the host filesystem. The -v /home/gobblin/logs:/var/log/gobblin option allows the Gobblin standalone logs to be written to the host filesystem, so that they can be read on the host machine. This is useful for monitoring and debugging purposes. Once again, the container directory ( /var/log/gobblin ) directory should not be modified, while the container directory ( /home/gobblin/logs ) can be any directory on the host filesystem.","title":"Gobblin-Standalone Repository"},{"location":"user-guide/Docker-Integration/#future-work","text":"Create gobblin-dev images that provide an development environment for Gobblin contributors Create gobblin-kafka images that provide an end-to-end service for writing to Kafka and ingesting the Kafka data through Gobblin Test and write a tutorial on using gobblin-standalone images to write to a HDFS cluster Create images based on Linux Alpine (lightweight Linux distro)","title":"Future Work"},{"location":"user-guide/FAQs/","text":"Table of Contents Table of Contents Gobblin General Questions What is Gobblin? What programming languages does Gobblin support? Does Gobblin require any external software to be installed? What Hadoop versions can Gobblin run on? How do I run and schedule a Gobblin job? How is Gobblin different from Sqoop? Technical Questions When running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen? Why does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job? How do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null? How do I compile Gobblin against CDH? Resolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully Gradle Build Fails With Cannot invoke method getURLs on null object Gradle Technical Questions How do I add a new external dependency? How do I add a new Maven Repository to pull artifacts from? Gobblin General Questions What is Gobblin? Gobblin is a universal ingestion framework. It's goal is to pull data from any source into an arbitrary data store. One major use case for Gobblin is pulling data into Hadoop. Gobblin can pull data from file systems, SQL stores, and data that is exposed by a REST API. See the Gobblin Home page for more information. What programming languages does Gobblin support? Gobblin currently only supports Java 7 and up. Does Gobblin require any external software to be installed? The machine that Gobblin is built on must have Java installed, and the $JAVA_HOME environment variable must be set. What Hadoop versions can Gobblin run on? Gobblin can only be run on Hadoop 2.x. By default, Gobblin compiles against Hadoop 2.3.0. How do I run and schedule a Gobblin job? Check out the Deployment page for information on how to run and schedule Gobblin jobs. Check out the Configuration page for information on how to set proper configuration properties for a job. How is Gobblin different from Sqoop? Sqoop main focus bulk import and export of data from relational databases to HDFS, it lacks the ETL functionality of data cleansing, data transformation, and data quality checks that Gobblin provides. Gobblin is also capable of pulling from any data source (e.g. file systems, RDMS, REST APIs). Technical Questions When running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen? Gobblin currently uses Hadoop map tasks as a container for running Gobblin tasks. Each map task runs 1 or more Gobblin workunits, and the progress of each workunit is not hooked into the progress of each map task. Even though the Hadoop job reports 100% completion, Gobblin is still doing work. See the Gobblin Deployment page for more information. Why does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job? Gobblin takes all WorkUnits created by the Source class and serializes each one into a file on Hadoop. These files are read by each map task, and are deserialized into Gobblin Tasks. These Tasks are then run by the map-task. The reason the job stalls is that Gobblin is writing all these files to HDFS, which can take a while especially if there are a lot of tasks to run. See the Gobblin Deployment page for more information. How do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null ? This error typically occurs due to Hadoop version conflict issues. If Gobblin is compiled against a specific Hadoop version, but then deployed on a different Hadoop version or installation, this error may be thrown. For example, if you simply compile Gobblin using ./gradlew clean build , but deploy Gobblin to a cluster with CDH installed, you may hit this error. It is important to realize that the the gobblin-dist.tar.gz file produced by ./gradlew clean build will include all the Hadoop jar dependencies; and if one follows the MR deployment guide , Gobblin will be launched with these dependencies on the classpath. To fix this take the following steps: Delete all the Hadoop jars from the Gobblin lib folder Ensure that the environment variable HADOOP_CLASSPATH is set and points to a directory containing the Hadoop libraries for the cluster How do I compile Gobblin against CDH? Cloudera Distributed Hadoop (often abbreviated as CDH) is a popular Hadoop distribution. Typically, when running Gobblin on a CDH cluster it is recommended that one also compile Gobblin against the same CDH version. Not doing so may cause unexpected runtime behavior. To compile against a specific CDH version simply use the hadoopVersion parameter. For example, to compile against version 2.5.0-cdh5.3.0 run ./gradlew clean build -PhadoopVersion=2.5.0-cdh5.3.0 . Resolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully This exception usually just means that a Hadoop Map Task running Gobblin Tasks threw some exception. Unfortunately, the exception isn't truly indicative of the underlying problem, all it is really saying is that something went wrong in the Gobblin Task. Each Hadoop Map Task has its own log file and it is often easiest to look at the logs of the Map Task when debugging this problem. There are multiple ways to do this, but one of the easiest ways is to execute yarn logs -applicationId application ID [OPTIONS] Gradle Build Fails With Cannot invoke method getURLs on null object Add -x test to build the project without running the tests; this will make the exception go away. If one needs to run the tests then make sure Java Cryptography Extension is installed. Gradle Technical Questions How do I add a new external dependency? Say I want to add oozie-core-4.2.0.jar as a dependency to the gobblin-scheduler subproject. I would first open the file build.gradle and add the following entry to the ext.externalDependency array: \"oozieCore\": \"org.apache.oozie:oozie-core:4.2.0\" . Then in the gobblin-scheduler/build.gradle file I would add the following line to the dependency block: compile externalDependency.oozieCore . How do I add a new Maven Repository to pull artifacts from? Often times, one may have important artifacts stored in a local or private Maven repository. As of 01/21/2016 Gobblin only pulls artifacts from the following Maven Repositories: Maven Central , Conjars , and Cloudera . In order to add another Maven Repository modify the defaultEnvironment.gradle file and the new repository using the same pattern as the existing ones.","title":"FAQs"},{"location":"user-guide/FAQs/#table-of-contents","text":"Table of Contents Gobblin General Questions What is Gobblin? What programming languages does Gobblin support? Does Gobblin require any external software to be installed? What Hadoop versions can Gobblin run on? How do I run and schedule a Gobblin job? How is Gobblin different from Sqoop? Technical Questions When running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen? Why does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job? How do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null? How do I compile Gobblin against CDH? Resolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully Gradle Build Fails With Cannot invoke method getURLs on null object Gradle Technical Questions How do I add a new external dependency? How do I add a new Maven Repository to pull artifacts from?","title":"Table of Contents"},{"location":"user-guide/FAQs/#gobblin","text":"","title":"Gobblin"},{"location":"user-guide/FAQs/#general-questions","text":"","title":"General Questions "},{"location":"user-guide/FAQs/#what-is-gobblin","text":"Gobblin is a universal ingestion framework. It's goal is to pull data from any source into an arbitrary data store. One major use case for Gobblin is pulling data into Hadoop. Gobblin can pull data from file systems, SQL stores, and data that is exposed by a REST API. See the Gobblin Home page for more information.","title":"What is Gobblin?"},{"location":"user-guide/FAQs/#what-programming-languages-does-gobblin-support","text":"Gobblin currently only supports Java 7 and up.","title":"What programming languages does Gobblin support?"},{"location":"user-guide/FAQs/#does-gobblin-require-any-external-software-to-be-installed","text":"The machine that Gobblin is built on must have Java installed, and the $JAVA_HOME environment variable must be set.","title":"Does Gobblin require any external software to be installed?"},{"location":"user-guide/FAQs/#what-hadoop-versions-can-gobblin-run-on","text":"Gobblin can only be run on Hadoop 2.x. By default, Gobblin compiles against Hadoop 2.3.0.","title":"What Hadoop versions can Gobblin run on?"},{"location":"user-guide/FAQs/#how-do-i-run-and-schedule-a-gobblin-job","text":"Check out the Deployment page for information on how to run and schedule Gobblin jobs. Check out the Configuration page for information on how to set proper configuration properties for a job.","title":"How do I run and schedule a Gobblin job?"},{"location":"user-guide/FAQs/#how-is-gobblin-different-from-sqoop","text":"Sqoop main focus bulk import and export of data from relational databases to HDFS, it lacks the ETL functionality of data cleansing, data transformation, and data quality checks that Gobblin provides. Gobblin is also capable of pulling from any data source (e.g. file systems, RDMS, REST APIs).","title":"How is Gobblin different from Sqoop?"},{"location":"user-guide/FAQs/#technical-questions","text":"","title":"Technical Questions "},{"location":"user-guide/FAQs/#when-running-on-hadoop-each-map-task-quickly-reaches-100-percent-completion-but-then-stalls-for-a-long-time-why-does-this-happen","text":"Gobblin currently uses Hadoop map tasks as a container for running Gobblin tasks. Each map task runs 1 or more Gobblin workunits, and the progress of each workunit is not hooked into the progress of each map task. Even though the Hadoop job reports 100% completion, Gobblin is still doing work. See the Gobblin Deployment page for more information.","title":"When running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen?"},{"location":"user-guide/FAQs/#why-does-gobblin-on-hadoop-stall-for-a-long-time-between-adding-files-to-the-distrbutedcache-and-launching-the-actual-job","text":"Gobblin takes all WorkUnits created by the Source class and serializes each one into a file on Hadoop. These files are read by each map task, and are deserialized into Gobblin Tasks. These Tasks are then run by the map-task. The reason the job stalls is that Gobblin is writing all these files to HDFS, which can take a while especially if there are a lot of tasks to run. See the Gobblin Deployment page for more information.","title":"Why does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job?"},{"location":"user-guide/FAQs/#how-do-i-fix-unsupportedfilesystemexception-no-abstractfilesystem-for-scheme-null","text":"This error typically occurs due to Hadoop version conflict issues. If Gobblin is compiled against a specific Hadoop version, but then deployed on a different Hadoop version or installation, this error may be thrown. For example, if you simply compile Gobblin using ./gradlew clean build , but deploy Gobblin to a cluster with CDH installed, you may hit this error. It is important to realize that the the gobblin-dist.tar.gz file produced by ./gradlew clean build will include all the Hadoop jar dependencies; and if one follows the MR deployment guide , Gobblin will be launched with these dependencies on the classpath. To fix this take the following steps: Delete all the Hadoop jars from the Gobblin lib folder Ensure that the environment variable HADOOP_CLASSPATH is set and points to a directory containing the Hadoop libraries for the cluster","title":"How do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null?"},{"location":"user-guide/FAQs/#how-do-i-compile-gobblin-against-cdh","text":"Cloudera Distributed Hadoop (often abbreviated as CDH) is a popular Hadoop distribution. Typically, when running Gobblin on a CDH cluster it is recommended that one also compile Gobblin against the same CDH version. Not doing so may cause unexpected runtime behavior. To compile against a specific CDH version simply use the hadoopVersion parameter. For example, to compile against version 2.5.0-cdh5.3.0 run ./gradlew clean build -PhadoopVersion=2.5.0-cdh5.3.0 .","title":"How do I compile Gobblin against CDH?"},{"location":"user-guide/FAQs/#resolve-gobblin-on-mr-exception-ioexception-not-all-tasks-running-in-mapper-attempt_id-completed-successfully","text":"This exception usually just means that a Hadoop Map Task running Gobblin Tasks threw some exception. Unfortunately, the exception isn't truly indicative of the underlying problem, all it is really saying is that something went wrong in the Gobblin Task. Each Hadoop Map Task has its own log file and it is often easiest to look at the logs of the Map Task when debugging this problem. There are multiple ways to do this, but one of the easiest ways is to execute yarn logs -applicationId application ID [OPTIONS]","title":"Resolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully"},{"location":"user-guide/FAQs/#gradle-build-fails-with-cannot-invoke-method-geturls-on-null-object","text":"Add -x test to build the project without running the tests; this will make the exception go away. If one needs to run the tests then make sure Java Cryptography Extension is installed.","title":"Gradle Build Fails With Cannot invoke method getURLs on null object"},{"location":"user-guide/FAQs/#gradle","text":"","title":"Gradle"},{"location":"user-guide/FAQs/#technical-questions_1","text":"","title":"Technical Questions"},{"location":"user-guide/FAQs/#how-do-i-add-a-new-external-dependency","text":"Say I want to add oozie-core-4.2.0.jar as a dependency to the gobblin-scheduler subproject. I would first open the file build.gradle and add the following entry to the ext.externalDependency array: \"oozieCore\": \"org.apache.oozie:oozie-core:4.2.0\" . Then in the gobblin-scheduler/build.gradle file I would add the following line to the dependency block: compile externalDependency.oozieCore .","title":"How do I add a new external dependency?"},{"location":"user-guide/FAQs/#how-do-i-add-a-new-maven-repository-to-pull-artifacts-from","text":"Often times, one may have important artifacts stored in a local or private Maven repository. As of 01/21/2016 Gobblin only pulls artifacts from the following Maven Repositories: Maven Central , Conjars , and Cloudera . In order to add another Maven Repository modify the defaultEnvironment.gradle file and the new repository using the same pattern as the existing ones.","title":"How do I add a new Maven Repository to pull artifacts from?"},{"location":"user-guide/Gobblin-CLI/","text":"Table of Contents Table of Contents Gobblin Commands Execution Modes Gobblin Commands The Distcp Quick App The OneShot Quick App Developing quick apps for the CLI Implementing new Gobblin commands Gobblin Service Execution Modes ( as Daemon ) Gobblin System Configurations Gobblin Commands Execution Modes The Gobblin distribution comes with a script ./bin/gobblin for all commands and services. Here is the usage: Usage: gobblin.sh cli cli-command params gobblin.sh service execution-mode start|stop|status Use gobblin cli|service --help for more information. (Gobblin Version: 0.15.0) For Gobblin CLI commands, run following: Usage: gobblin.sh cli cli-command params options: cli-commands: passwordManager Encrypt or decrypt strings for the password manager. decrypt Decryption utilities run Run a Gobblin application. config Query the config library jobs Command line job info and operations stateMigration Command line tools for migrating state store job-state-to-json To convert Job state to JSON cleaner Data retention utility keystore Examine JCE Keystore files watermarks Inspect streaming watermarks job-store-schema-manager Database job history store schema manager --conf-dir gobblin-conf-dir-path Gobblon config path. default is '$GOBBLIN_HOME/conf/ exe-mode-name '. --log4j-conf path-of-log4j-file default is ' gobblin-conf-dir-path / execution-mode /log4j.properties'. --jvmopts jvm or gc options String containing JVM flags to include, in addition to -Xmx1g -Xms512m . --jars csv list of extra jars Column-separated list of extra jars to put on the CLASSPATH. --enable-gc-logs enables gc logs dumps. --show-classpath prints gobblin runtime classpath. --help Display this help. --verbose Display full command used to start the process. Gobblin Version: 0.15.0 Argument details: --conf-dir : specifies the path to directory containing gobblin system configuration files, like application.conf or reference.conf , log4j.properties and quartz.properties . --log4j-conf : specify the path of log4j config file to override the one in config directory (default is conf / gobblin-mode /log4j.properties . Gobblin uses SLF4J and the slf4j-log4j12 binding for logging. --jvmopts : to specify any JVM parameters, default is -Xmx1g -Xms512m . --enable-gc-logs : adds GC options to JVM parameters: -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCompressedOops -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$GOBBLIN_LOGS/ -Xloggc:$GOBBLIN_LOGS/gobblin-$GOBBLIN_MODE-gc.log --show-classpath : It prints the full value of the classpath that gobblin uses. all other arguments are self-explanatory. Gobblin Commands Gobblin provides following CLI commands: Available commands: job-state-to-json To convert Job state to JSON jobs Command line job info and operations passwordManager Encrypt or decrypt strings for the password manager. run Run a Gobblin application. decrypt Decryption utilities job-store-schema-manager Database job history store schema manager stateMigration Command line tools for migrating state store keystore Examine JCE Keystore files config Query the config library watermarks Inspect streaming watermarks cleaner Data retention utility Details on how to use run command: Gobblin ingestion applications can be accessed through the following command: gobblin cli run [listQuickApps] [ quick-app ] -jobName jobName [OPTIONS] For usage run ./bin/gobblin cli run . gobblin cli run uses Embedded Gobblin and subclasses to run Gobblin ingestion jobs, giving CLI access to most functionality that could be achieved using EmbeddedGobblin . For example, the following command will run a Hello World job (it will print \"Hello World 1 !\" somewhere in the logs). gobblin cli run -jobName helloWorld -setTemplate resource:///templates/hello-world.template Obviously, it is daunting to have to know the path to templates and exactly which configurations to set. The alternative is to use a quick app. Running: gobblin cli run listQuickApps will provide with a list of available quick apps. To run a quick app: gobblin cli run quick-app-name Quick apps may require additional arguments. For the usage of a particular app, run bin/gobblin cli run quick-app-name -h . The Distcp Quick App For example, consider the quick app distcp: $ gobblin cli run distcp -h usage: gobblin cli run distcp [OPTIONS] source target -delete Delete files in target that don't exist on source. -deleteEmptyParentDirectories If deleting files on target, also delete newly empty parent directories. -distributeJar arg -h,--help -l Uses log to print out erros in the base CLI code. -mrMode -setConfiguration arg -setJobTimeout arg -setLaunchTimeout arg -setShutdownTimeout arg -simulate -update Specifies files should be updated if they're different in the source. -useStateStore arg This provides usage for the app distcp, as well as listing all available options. Distcp could then be run: gobblin cli run distcp file:///source/path file:///target/path The OneShot Quick App The Gobblin cli also ships with a generic job runner, the oneShot quick app. You can use it to run a single job using a standard config file. This is very useful during development, testing and also makes it easy to integrate with schedulers that just need to fire off a command line job. The oneShot app allows you to run a job in standalone mode or in map-reduce mode. $ gobblin cli run oneShot -baseConf base-config-file -appConf path-to-job-conf-file # The Base Config file is an optional parameter and contains defaults for your mode of # execution (e.g. standalone modes would typically use # gobblin-dist/conf/standalone/application.conf and # mapreduce mode would typically use gobblin-dist/conf/mapreduce/application.conf) # # The Job Config file is your regular .pull or .conf file and is a required parameter. # You should use a fully qualified URI to your pull file. Otherwise Gobblin will pick the # default FS configured in the environment, which may not be what you want. # e.g file:///gobblin-conf/my-job/wikipedia.pull or hdfs:///gobblin-conf/my-job/kafka-hdfs.pull The oneShot app comes with certain hardcoded defaults (that it inherits from EmbeddedGobblin here ), that you may not be expecting. Make sure you understand what they do and override them in your baseConf or appConf files if needed. Notable differences at the time of this writing include: state.store.enabled = false (set this to true in your appConfig or baseConfig if you want state storage for repeated oneshot runs) data.publisher.appendExtractToFinalDir = false (set this to true in your appConfig or baseConfig if you want to see the extract name appended to the job output directory) The oneShot app allows for specifying the log4j file of your job execution which can be very helpful while debugging pesky failures. You can launch the job in MR-Mode by using the -mrMode switch. oneShot execution of standalone with a log4j file. $ gobblin cli run oneShot -baseConf /app/gobblin-dist/conf/standalone/application.conf -appConf file:///app/kafkaConfDir/kafka-simple-hdfs.pull --log4j-conf /app/gobblin-dist/conf/standalone/log4j.properties oneShot execution of map-reduce job with a log4j file $ gobblin cli run oneShot -mrMode -baseConf /app/gobblin-dist/conf/standalone/application.conf -appConf file:///app/kafkaConfDir/kafka-simple-hdfs.pull --log4j-conf /app/gobblin-dist/conf/standalone/log4j.properties Developing quick apps for the CLI It is very easy to convert a subclass of EmbeddedGobblin into a quick application for Gobblin CLI. All that is needed is to implement a EmbeddedGobblinCliFactory which knows how instantiate the EmbeddedGobblin from a CommandLine object and annotate it with the Alias annotation. There are two utility classes that make this very easy: PublicMethodsGobblinCliFactory : this class will automatically infer CLI options from the public methods of a subclass of EmbeddedGobblin . All the developer has to do is implement the method constructEmbeddedGobblin(CommandLine) that calls the appropriate constructor of the desired EmbeddedGobblin subclass with parameters extracted from the CLI. Additionally, it is a good idea to override getUsageString() with the appropriate usage string. For an example, see gobblin.runtime.embedded.EmbeddedGobblinDistcp.CliFactory . ConstructorAndPublicMethodsGobblinCliFactory : this class does everything PublicMethodsGobblinCliFactory does, but it additionally automatically infers how to construct the EmbeddedGobblin object from a constructor annotated with EmbeddedGobblinCliSupport . For an example, see gobblin.runtime.embedded.EmbeddedGobblin.CliFactory . Implementing new Gobblin commands To implement a new Gobblin command to list and execute using ./bin/gobblin , implement the class gobblin.runtime.cli.CliApplication , and annotate it with the Alias annotation. The Gobblin CLI will automatically find the command, and users can invoke it by the Alias value. Gobblin Service Execution Modes ( as Daemon ) For more info on Gobblin service execution modes, run bin/gobblin service --help : Usage: gobblin.sh service execution-mode start|stop|status Argument Options: execution-mode standalone, cluster-master, cluster-worker, aws, yarn, mapreduce, service-manager. --conf-dir gobblin-conf-dir-path Gobblon config path. default is '$GOBBLIN_HOME/conf/ exe-mode-name '. --log4j-conf path-of-log4j-file default is ' gobblin-conf-dir-path / execution-mode /log4j.properties'. --jvmopts jvm or gc options String containing JVM flags to include, in addition to -Xmx1g -Xms512m . --jars csv list of extra jars Column-separated list of extra jars to put on the CLASSPATH. --enable-gc-logs enables gc logs dumps. --show-classpath prints gobblin runtime classpath. --cluster-name Name of the cluster to be used by helix other services. ( default: gobblin_cluster). --jt resource manager URL Only for mapreduce mode: Job submission URL, if not set, taken from ${HADOOP_HOME}/conf. --fs file system URL Only for mapreduce mode: Target file system, if not set, taken from ${HADOOP_HOME}/conf. --help Display this help. --verbose Display full command used to start the process. Gobblin Version: 0.15.0 Standalone: This mode starts all Gobblin services in single JVM on a single node. This mode is useful for development and light weight usage: gobblin service standalone start For more details and architecture on each execution mode, refer Standalone-Deployment Mapreduce: This mode is dependent on Hadoop (both MapReduce and HDFS) running locally or remote cluster. Before launching any Gobblin jobs on Hadoop MapReduce, check the Gobblin system configuration file located at conf/mapreduce/application.properties for property fs.uri , which defines the file system URI used. The default value is hdfs://localhost:8020 , which points to the local HDFS on the default port 8020. Change it to the right value depending on your Hadoop/HDFS setup. For example, if you have HDFS setup somwhere on port 9000, then set the property as follows: fs.uri=hdfs:// namenode host name :9000/ --jt : resource manager URL --fs : file system type value for fs.uri This mode will have the minimum set of Gobblin jars, selected using libs/gobblin- module_name -$GOBBLIN_VERSION.jar , which is passed as -libjar to hadoop command while running the job. These same set of jars also gets added to the Hadoop DistributedCache for use in the mappers. If a job has additional jars needed for task executions (in the mappers), those jars can also be included by using the --jars option or the following job configuration property in the job configuration file: job.jars= comma-separated list of jars the job depends on if HADOOP_HOME is set in the environment, Gobblin will add result of hadoop classpath prior to default GOBBLIN_CLASSPATH to give them precedence while running bin/gobblin . All job data and persisted job/task states will be written to the specified file system. Before launching any jobs, make sure the environment variable HADOOP_HOME is set so that it can access hadoop binaries under {HADOOP_HOME}/bin and also working directory should be set with configuration {gobblin.cluster.work.dir} . Note that the Gobblin working directory will be created on the file system specified above. An important side effect of this is that (depending on the application) non-fully-qualified paths (like /my/path ) will default to local file system if HADOOP_HOME is not set, while they will default to HDFS if the variable is set. When referring to local paths, it is always a good idea to use the fully qualified path (e.g. file:///my/path ). Cluster Mode (master worker) This is a cluster mode consist of master and worker process. gobblin service cluster-master start gobblin service cluster-worker start AWS This mode starts Gobblin on AWS cloud cluster. gobblin service aws start YARN This mode starts Gobblin on YARN cluster. gobblin service yarn start Gobblin System Configurations Following values can be override by setting it in gobblin-env.sh GOBBLIN_LOGS : by default the logs are written to $GOBBLIN_HOME/logs , it can be override by setting GOBBLIN_LOGS \\ GOBBLIN_VERSION : by default gobblin version is set by the build process, it can be override by setting GOBBLIN_VERSION \\ All Gobblin system configurations details can be found here: Configuration Properties Glossary .","title":"Gobblin CLI"},{"location":"user-guide/Gobblin-CLI/#table-of-contents","text":"Table of Contents Gobblin Commands Execution Modes Gobblin Commands The Distcp Quick App The OneShot Quick App Developing quick apps for the CLI Implementing new Gobblin commands Gobblin Service Execution Modes ( as Daemon ) Gobblin System Configurations","title":"Table of Contents"},{"location":"user-guide/Gobblin-CLI/#gobblin-commands-execution-modes","text":"The Gobblin distribution comes with a script ./bin/gobblin for all commands and services. Here is the usage: Usage: gobblin.sh cli cli-command params gobblin.sh service execution-mode start|stop|status Use gobblin cli|service --help for more information. (Gobblin Version: 0.15.0) For Gobblin CLI commands, run following: Usage: gobblin.sh cli cli-command params options: cli-commands: passwordManager Encrypt or decrypt strings for the password manager. decrypt Decryption utilities run Run a Gobblin application. config Query the config library jobs Command line job info and operations stateMigration Command line tools for migrating state store job-state-to-json To convert Job state to JSON cleaner Data retention utility keystore Examine JCE Keystore files watermarks Inspect streaming watermarks job-store-schema-manager Database job history store schema manager --conf-dir gobblin-conf-dir-path Gobblon config path. default is '$GOBBLIN_HOME/conf/ exe-mode-name '. --log4j-conf path-of-log4j-file default is ' gobblin-conf-dir-path / execution-mode /log4j.properties'. --jvmopts jvm or gc options String containing JVM flags to include, in addition to -Xmx1g -Xms512m . --jars csv list of extra jars Column-separated list of extra jars to put on the CLASSPATH. --enable-gc-logs enables gc logs dumps. --show-classpath prints gobblin runtime classpath. --help Display this help. --verbose Display full command used to start the process. Gobblin Version: 0.15.0 Argument details: --conf-dir : specifies the path to directory containing gobblin system configuration files, like application.conf or reference.conf , log4j.properties and quartz.properties . --log4j-conf : specify the path of log4j config file to override the one in config directory (default is conf / gobblin-mode /log4j.properties . Gobblin uses SLF4J and the slf4j-log4j12 binding for logging. --jvmopts : to specify any JVM parameters, default is -Xmx1g -Xms512m . --enable-gc-logs : adds GC options to JVM parameters: -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCompressedOops -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$GOBBLIN_LOGS/ -Xloggc:$GOBBLIN_LOGS/gobblin-$GOBBLIN_MODE-gc.log --show-classpath : It prints the full value of the classpath that gobblin uses. all other arguments are self-explanatory.","title":"Gobblin Commands &amp; Execution Modes"},{"location":"user-guide/Gobblin-CLI/#gobblin-commands","text":"Gobblin provides following CLI commands: Available commands: job-state-to-json To convert Job state to JSON jobs Command line job info and operations passwordManager Encrypt or decrypt strings for the password manager. run Run a Gobblin application. decrypt Decryption utilities job-store-schema-manager Database job history store schema manager stateMigration Command line tools for migrating state store keystore Examine JCE Keystore files config Query the config library watermarks Inspect streaming watermarks cleaner Data retention utility Details on how to use run command: Gobblin ingestion applications can be accessed through the following command: gobblin cli run [listQuickApps] [ quick-app ] -jobName jobName [OPTIONS] For usage run ./bin/gobblin cli run . gobblin cli run uses Embedded Gobblin and subclasses to run Gobblin ingestion jobs, giving CLI access to most functionality that could be achieved using EmbeddedGobblin . For example, the following command will run a Hello World job (it will print \"Hello World 1 !\" somewhere in the logs). gobblin cli run -jobName helloWorld -setTemplate resource:///templates/hello-world.template Obviously, it is daunting to have to know the path to templates and exactly which configurations to set. The alternative is to use a quick app. Running: gobblin cli run listQuickApps will provide with a list of available quick apps. To run a quick app: gobblin cli run quick-app-name Quick apps may require additional arguments. For the usage of a particular app, run bin/gobblin cli run quick-app-name -h .","title":"Gobblin Commands"},{"location":"user-guide/Gobblin-CLI/#the-distcp-quick-app","text":"For example, consider the quick app distcp: $ gobblin cli run distcp -h usage: gobblin cli run distcp [OPTIONS] source target -delete Delete files in target that don't exist on source. -deleteEmptyParentDirectories If deleting files on target, also delete newly empty parent directories. -distributeJar arg -h,--help -l Uses log to print out erros in the base CLI code. -mrMode -setConfiguration arg -setJobTimeout arg -setLaunchTimeout arg -setShutdownTimeout arg -simulate -update Specifies files should be updated if they're different in the source. -useStateStore arg This provides usage for the app distcp, as well as listing all available options. Distcp could then be run: gobblin cli run distcp file:///source/path file:///target/path","title":"The Distcp Quick App"},{"location":"user-guide/Gobblin-CLI/#the-oneshot-quick-app","text":"The Gobblin cli also ships with a generic job runner, the oneShot quick app. You can use it to run a single job using a standard config file. This is very useful during development, testing and also makes it easy to integrate with schedulers that just need to fire off a command line job. The oneShot app allows you to run a job in standalone mode or in map-reduce mode. $ gobblin cli run oneShot -baseConf base-config-file -appConf path-to-job-conf-file # The Base Config file is an optional parameter and contains defaults for your mode of # execution (e.g. standalone modes would typically use # gobblin-dist/conf/standalone/application.conf and # mapreduce mode would typically use gobblin-dist/conf/mapreduce/application.conf) # # The Job Config file is your regular .pull or .conf file and is a required parameter. # You should use a fully qualified URI to your pull file. Otherwise Gobblin will pick the # default FS configured in the environment, which may not be what you want. # e.g file:///gobblin-conf/my-job/wikipedia.pull or hdfs:///gobblin-conf/my-job/kafka-hdfs.pull The oneShot app comes with certain hardcoded defaults (that it inherits from EmbeddedGobblin here ), that you may not be expecting. Make sure you understand what they do and override them in your baseConf or appConf files if needed. Notable differences at the time of this writing include: state.store.enabled = false (set this to true in your appConfig or baseConfig if you want state storage for repeated oneshot runs) data.publisher.appendExtractToFinalDir = false (set this to true in your appConfig or baseConfig if you want to see the extract name appended to the job output directory) The oneShot app allows for specifying the log4j file of your job execution which can be very helpful while debugging pesky failures. You can launch the job in MR-Mode by using the -mrMode switch. oneShot execution of standalone with a log4j file. $ gobblin cli run oneShot -baseConf /app/gobblin-dist/conf/standalone/application.conf -appConf file:///app/kafkaConfDir/kafka-simple-hdfs.pull --log4j-conf /app/gobblin-dist/conf/standalone/log4j.properties oneShot execution of map-reduce job with a log4j file $ gobblin cli run oneShot -mrMode -baseConf /app/gobblin-dist/conf/standalone/application.conf -appConf file:///app/kafkaConfDir/kafka-simple-hdfs.pull --log4j-conf /app/gobblin-dist/conf/standalone/log4j.properties","title":"The OneShot Quick App"},{"location":"user-guide/Gobblin-CLI/#developing-quick-apps-for-the-cli","text":"It is very easy to convert a subclass of EmbeddedGobblin into a quick application for Gobblin CLI. All that is needed is to implement a EmbeddedGobblinCliFactory which knows how instantiate the EmbeddedGobblin from a CommandLine object and annotate it with the Alias annotation. There are two utility classes that make this very easy: PublicMethodsGobblinCliFactory : this class will automatically infer CLI options from the public methods of a subclass of EmbeddedGobblin . All the developer has to do is implement the method constructEmbeddedGobblin(CommandLine) that calls the appropriate constructor of the desired EmbeddedGobblin subclass with parameters extracted from the CLI. Additionally, it is a good idea to override getUsageString() with the appropriate usage string. For an example, see gobblin.runtime.embedded.EmbeddedGobblinDistcp.CliFactory . ConstructorAndPublicMethodsGobblinCliFactory : this class does everything PublicMethodsGobblinCliFactory does, but it additionally automatically infers how to construct the EmbeddedGobblin object from a constructor annotated with EmbeddedGobblinCliSupport . For an example, see gobblin.runtime.embedded.EmbeddedGobblin.CliFactory .","title":"Developing quick apps for the CLI"},{"location":"user-guide/Gobblin-CLI/#implementing-new-gobblin-commands","text":"To implement a new Gobblin command to list and execute using ./bin/gobblin , implement the class gobblin.runtime.cli.CliApplication , and annotate it with the Alias annotation. The Gobblin CLI will automatically find the command, and users can invoke it by the Alias value.","title":"Implementing new Gobblin commands"},{"location":"user-guide/Gobblin-CLI/#gobblin-service-execution-modes-as-daemon","text":"For more info on Gobblin service execution modes, run bin/gobblin service --help : Usage: gobblin.sh service execution-mode start|stop|status Argument Options: execution-mode standalone, cluster-master, cluster-worker, aws, yarn, mapreduce, service-manager. --conf-dir gobblin-conf-dir-path Gobblon config path. default is '$GOBBLIN_HOME/conf/ exe-mode-name '. --log4j-conf path-of-log4j-file default is ' gobblin-conf-dir-path / execution-mode /log4j.properties'. --jvmopts jvm or gc options String containing JVM flags to include, in addition to -Xmx1g -Xms512m . --jars csv list of extra jars Column-separated list of extra jars to put on the CLASSPATH. --enable-gc-logs enables gc logs dumps. --show-classpath prints gobblin runtime classpath. --cluster-name Name of the cluster to be used by helix other services. ( default: gobblin_cluster). --jt resource manager URL Only for mapreduce mode: Job submission URL, if not set, taken from ${HADOOP_HOME}/conf. --fs file system URL Only for mapreduce mode: Target file system, if not set, taken from ${HADOOP_HOME}/conf. --help Display this help. --verbose Display full command used to start the process. Gobblin Version: 0.15.0 Standalone: This mode starts all Gobblin services in single JVM on a single node. This mode is useful for development and light weight usage: gobblin service standalone start For more details and architecture on each execution mode, refer Standalone-Deployment Mapreduce: This mode is dependent on Hadoop (both MapReduce and HDFS) running locally or remote cluster. Before launching any Gobblin jobs on Hadoop MapReduce, check the Gobblin system configuration file located at conf/mapreduce/application.properties for property fs.uri , which defines the file system URI used. The default value is hdfs://localhost:8020 , which points to the local HDFS on the default port 8020. Change it to the right value depending on your Hadoop/HDFS setup. For example, if you have HDFS setup somwhere on port 9000, then set the property as follows: fs.uri=hdfs:// namenode host name :9000/ --jt : resource manager URL --fs : file system type value for fs.uri This mode will have the minimum set of Gobblin jars, selected using libs/gobblin- module_name -$GOBBLIN_VERSION.jar , which is passed as -libjar to hadoop command while running the job. These same set of jars also gets added to the Hadoop DistributedCache for use in the mappers. If a job has additional jars needed for task executions (in the mappers), those jars can also be included by using the --jars option or the following job configuration property in the job configuration file: job.jars= comma-separated list of jars the job depends on if HADOOP_HOME is set in the environment, Gobblin will add result of hadoop classpath prior to default GOBBLIN_CLASSPATH to give them precedence while running bin/gobblin . All job data and persisted job/task states will be written to the specified file system. Before launching any jobs, make sure the environment variable HADOOP_HOME is set so that it can access hadoop binaries under {HADOOP_HOME}/bin and also working directory should be set with configuration {gobblin.cluster.work.dir} . Note that the Gobblin working directory will be created on the file system specified above. An important side effect of this is that (depending on the application) non-fully-qualified paths (like /my/path ) will default to local file system if HADOOP_HOME is not set, while they will default to HDFS if the variable is set. When referring to local paths, it is always a good idea to use the fully qualified path (e.g. file:///my/path ). Cluster Mode (master worker) This is a cluster mode consist of master and worker process. gobblin service cluster-master start gobblin service cluster-worker start AWS This mode starts Gobblin on AWS cloud cluster. gobblin service aws start YARN This mode starts Gobblin on YARN cluster. gobblin service yarn start","title":"Gobblin Service Execution Modes ( as Daemon )"},{"location":"user-guide/Gobblin-CLI/#gobblin-system-configurations","text":"Following values can be override by setting it in gobblin-env.sh GOBBLIN_LOGS : by default the logs are written to $GOBBLIN_HOME/logs , it can be override by setting GOBBLIN_LOGS \\ GOBBLIN_VERSION : by default gobblin version is set by the build process, it can be override by setting GOBBLIN_VERSION \\ All Gobblin system configurations details can be found here: Configuration Properties Glossary .","title":"Gobblin System Configurations"},{"location":"user-guide/Gobblin-Compliance/","text":"Introduction Usage Configuration Developer Guide Introduction The Gobblin Compliance module allows for data purging to meet regulatory compliance requirements. The module includes functionality for purging datasets and the associated operational support in production. The purging is performed using Hive meaning that purging of datasets is supported in any format that Hive can read from and write to, including for example ORC and Parquet. Further the purger is built on top of the Gobblin framework which means that the fault-tolerance, scalability and flexibility provided by Gobblin is taken full advantage of. Usage As an example, let us assume that regulation requires that once a guest has checked out of a hotel, certain guest data needs to be purged within a certain number of days after the guest leaves. Hence, the goal is to purge the hotel's datasets of data associated with their guests after they have left in order to meet regulatory compliance requirements. Hive databases and tables are setup to be purged by following these steps: Whitelisting of the database, or table for purging Specifying the dataset descriptor for the tables to be purged Specifying the JSON path of the compliance field in the dataset descriptor The table which contains the list of ids whose associated data is to be purged The name of the id column in the table to match against For example, in order to purge a Hive table named tracking.event , these properties are specified: Specify the purger whitelist to include tracking.event gobblin.compliance.dataset.whitelist=tracking.event Add a TBLPROPERTY named dataset.descriptor to the tracking.event Hive table to specify the compliance field to match in the table as an escaped JSON (since it has to be a valid string): {\\\"complianceSpec\\\" : {\\\"identifierField\\\" : \\\"metadata.guestid\\\" }} Specify the JSON field path for the compliance field (that evaluates to metadata.guestId) dataset.descriptor.fieldPath=complianceSpec.identifierField Specify the table containing the list of ids to purge gobblin.compliance.complianceIdTable=u_purger.guestIds Specify the name of the id in the table to match against gobblin.compliance.complianceId=guestId With these properties in place, a Gobblin job can be setup to purge the table. The work unit for the purger is an individual table partition. Hence the purger will iterate over all the partitions in the table, and purge each partition individually, processing as many partitions in parallel as specified (by the property 'gobblin.compliance.purger.maxWorkunits', which defaults to 5). Configuration Configuration options for the Hive Purger include Property Description gobblin.compliance.dataset.whitelist The list of databases/tables to purge, comma-separated dataset.descriptor.fieldPath The JSON field path specifying the compliance field gobblin.compliance.complianceIdTable The table containing the list of ids whose data needs to be purged gobblin.compliance.complianceId The name of the id column in the complianceIdTable to match against gobblin.compliance.purger.maxWorkunits The number of partitions to purge in parallel gobblin.compliance.purger.policy.class The policy class that specifies the criteria for purging, defaults to HivePurgerPolicy gobblin.compliance.purger.commit.policy.class The policy class that specifies the criteria for committing a purged dataset, defaults to HivePurgerCommitPolicy Developer Guide The Developer Guide further describes the design of the module.","title":"Gobblin Compliance"},{"location":"user-guide/Gobblin-Compliance/#introduction","text":"The Gobblin Compliance module allows for data purging to meet regulatory compliance requirements. The module includes functionality for purging datasets and the associated operational support in production. The purging is performed using Hive meaning that purging of datasets is supported in any format that Hive can read from and write to, including for example ORC and Parquet. Further the purger is built on top of the Gobblin framework which means that the fault-tolerance, scalability and flexibility provided by Gobblin is taken full advantage of.","title":"Introduction"},{"location":"user-guide/Gobblin-Compliance/#usage","text":"As an example, let us assume that regulation requires that once a guest has checked out of a hotel, certain guest data needs to be purged within a certain number of days after the guest leaves. Hence, the goal is to purge the hotel's datasets of data associated with their guests after they have left in order to meet regulatory compliance requirements. Hive databases and tables are setup to be purged by following these steps: Whitelisting of the database, or table for purging Specifying the dataset descriptor for the tables to be purged Specifying the JSON path of the compliance field in the dataset descriptor The table which contains the list of ids whose associated data is to be purged The name of the id column in the table to match against For example, in order to purge a Hive table named tracking.event , these properties are specified: Specify the purger whitelist to include tracking.event gobblin.compliance.dataset.whitelist=tracking.event Add a TBLPROPERTY named dataset.descriptor to the tracking.event Hive table to specify the compliance field to match in the table as an escaped JSON (since it has to be a valid string): {\\\"complianceSpec\\\" : {\\\"identifierField\\\" : \\\"metadata.guestid\\\" }} Specify the JSON field path for the compliance field (that evaluates to metadata.guestId) dataset.descriptor.fieldPath=complianceSpec.identifierField Specify the table containing the list of ids to purge gobblin.compliance.complianceIdTable=u_purger.guestIds Specify the name of the id in the table to match against gobblin.compliance.complianceId=guestId With these properties in place, a Gobblin job can be setup to purge the table. The work unit for the purger is an individual table partition. Hence the purger will iterate over all the partitions in the table, and purge each partition individually, processing as many partitions in parallel as specified (by the property 'gobblin.compliance.purger.maxWorkunits', which defaults to 5).","title":"Usage"},{"location":"user-guide/Gobblin-Compliance/#configuration","text":"Configuration options for the Hive Purger include Property Description gobblin.compliance.dataset.whitelist The list of databases/tables to purge, comma-separated dataset.descriptor.fieldPath The JSON field path specifying the compliance field gobblin.compliance.complianceIdTable The table containing the list of ids whose data needs to be purged gobblin.compliance.complianceId The name of the id column in the complianceIdTable to match against gobblin.compliance.purger.maxWorkunits The number of partitions to purge in parallel gobblin.compliance.purger.policy.class The policy class that specifies the criteria for purging, defaults to HivePurgerPolicy gobblin.compliance.purger.commit.policy.class The policy class that specifies the criteria for committing a purged dataset, defaults to HivePurgerCommitPolicy","title":"Configuration"},{"location":"user-guide/Gobblin-Compliance/#developer-guide","text":"The Developer Guide further describes the design of the module.","title":"Developer Guide"},{"location":"user-guide/Gobblin-Deployment/","text":"Table of Contents Table of Contents Gobblin Execution Modes Overview Standalone Architecture MapReduce architecture Master-Worker architecture AWS architecture YARN architecture Gobblin-As-A-Service architecture Gobblin Execution Modes Overview One important feature of Gobblin is that it can be run on different platforms. Currently, Gobblin can run in standalone mode (which runs on a single machine), and on Hadoop MapReduce mode (which runs on a Hadoop cluster). This page summarizes the different deployment modes of Gobblin. It is important to understand the architecture of Gobblin in a specific deployment mode, so this page also describes the architecture of each deployment mode. Gobblin supports Java 7 and up, but can only run on Hadoop 2.x. By default, Gobblin will build against Hadoop 2.x, run ./gradlew clean build . More information on how to build Gobblin can be found here . All directories/paths referred below are relative to gobblin-dist . To run gobblin in any of the following executuon mode using gobblin.sh , refer Gobblin-CLI for the usage. Standalone Architecture The following diagram illustrates the Gobblin standalone architecture. In the standalone mode, a Gobblin instance runs in a single JVM and tasks run in a thread pool, the size of which is configurable. The standalone mode is good for light-weight data sources such as small databases. The standalone mode is also the default mode for trying and testing Gobblin. In the standalone deployment, the JobScheduler runs as a daemon process that schedules and runs jobs using the so-called JobLauncher s. The JobScheduler maintains a thread pool in which a new JobLauncher is started for each job run. Gobblin ships with two types of JobLauncher s, namely, the LocalJobLauncher and MRJobLauncher for launching and running Gobblin jobs on a single machine and on Hadoop MapReduce, respectively. Which JobLauncher to use can be configured on a per-job basis, which means the JobScheduler can schedule and run jobs in different deployment modes. This section will focus on the LocalJobLauncher for launching and running Gobblin jobs on a single machine. The MRJobLauncher will be covered in a later section on the architecture of Gobblin on Hadoop MapReduce. Each LocalJobLauncher starts and manages a few components for executing tasks of a Gobblin job. Specifically, a TaskExecutor is responsible for executing tasks in a thread pool, whose size is configurable on a per-job basis. A LocalTaskStateTracker is responsible for keep tracking of the state of running tasks, and particularly updating the task metrics. The LocalJobLauncher follows the steps below to launch and run a Gobblin job: Starting the TaskExecutor and LocalTaskStateTracker . Creating an instance of the Source class specified in the job configuration and getting the list of WorkUnit s to do. Creating a task for each WorkUnit in the list, registering the task with the LocalTaskStateTracker , and submitting the task to the TaskExecutor to run. Waiting for all the submitted tasks to finish. Upon completion of all the submitted tasks, collecting tasks states and persisting them to the state store, and publishing the extracted data. MapReduce architecture The digram below shows the architecture of Gobblin on Hadoop MapReduce. As the diagram shows, a Gobblin job runs as a mapper-only MapReduce job that runs tasks of the Gobblin job in the mappers. The basic idea here is to use the mappers purely as containers to run Gobblin tasks. This design also makes it easier to integrate with Yarn. Unlike in the standalone mode, task retries are not handled by Gobblin itself in the Hadoop MapReduce mode. Instead, Gobblin relies on the task retry mechanism of Hadoop MapReduce. In this mode, a MRJobLauncher is used to launch and run a Gobblin job on Hadoop MapReduce, following the steps below: Creating an instance of the Source class specified in the job configuration and getting the list of WorkUnit s to do. Serializing each WorkUnit into a file on HDFS that will be read later by a mapper. Creating a file that lists the paths of the files storing serialized WorkUnit s. Creating and configuring a mapper-only Hadoop MapReduce job that takes the file created in step 3 as input. Starting the MapReduce job to run on the cluster of choice and waiting for it to finish. Upon completion of the MapReduce job, collecting tasks states and persisting them to the state store, and publishing the extracted data. A mapper in a Gobblin MapReduce job runs one or more tasks, depending on the number of WorkUnit s to do and the (optional) maximum number of mappers specified in the job configuration. If there is no maximum number of mappers specified in the job configuration, each WorkUnit corresponds to one task that is executed by one mapper and each mapper only runs one task. Otherwise, if a maximum number of mappers is specified and there are more WorkUnit s than the maximum number of mappers allowed, each mapper may handle more than one WorkUnit . There is also a special type of WorkUnit s named MultiWorkUnit that group multiple WorkUnit s to be executed together in one batch in a single mapper. A mapper in a Gobblin MapReduce job follows the step below to run tasks assigned to it: Starting the TaskExecutor that is responsible for executing tasks in a configurable-size thread pool and the MRTaskStateTracker that is responsible for keep tracking of the state of running tasks in the mapper. Reading the next input record that is the path to the file storing a serialized WorkUnit . Deserializing the WorkUnit and adding it to the list of WorkUnit s to do. If the input is a MultiWorkUnit , the WorkUnit s it wraps are all added to the list. Steps 2 and 3 are repeated until all assigned WorkUnit s are deserialized and added to the list. For each WorkUnit on the list of WorkUnit s to do, creating a task for the WorkUnit , registering the task with the MRTaskStateTracker , and submitting the task to the TaskExecutor to run. Note that the tasks may run in parallel if the TaskExecutor is configured to have more than one thread in its thread pool. Waiting for all the submitted tasks to finish. Upon completion of all the submitted tasks, writing out the state of each task into a file that will be read by the MRJobLauncher when collecting task states. Going back to step 2 and reading the next input record if available. Master-Worker architecture AWS architecture YARN architecture Gobblin-As-A-Service architecture","title":"Deployment"},{"location":"user-guide/Gobblin-Deployment/#table-of-contents","text":"Table of Contents Gobblin Execution Modes Overview Standalone Architecture MapReduce architecture Master-Worker architecture AWS architecture YARN architecture Gobblin-As-A-Service architecture","title":"Table of Contents"},{"location":"user-guide/Gobblin-Deployment/#gobblin-execution-modes-overview","text":"One important feature of Gobblin is that it can be run on different platforms. Currently, Gobblin can run in standalone mode (which runs on a single machine), and on Hadoop MapReduce mode (which runs on a Hadoop cluster). This page summarizes the different deployment modes of Gobblin. It is important to understand the architecture of Gobblin in a specific deployment mode, so this page also describes the architecture of each deployment mode. Gobblin supports Java 7 and up, but can only run on Hadoop 2.x. By default, Gobblin will build against Hadoop 2.x, run ./gradlew clean build . More information on how to build Gobblin can be found here . All directories/paths referred below are relative to gobblin-dist . To run gobblin in any of the following executuon mode using gobblin.sh , refer Gobblin-CLI for the usage.","title":"Gobblin Execution Modes Overview "},{"location":"user-guide/Gobblin-Deployment/#standalone-architecture","text":"The following diagram illustrates the Gobblin standalone architecture. In the standalone mode, a Gobblin instance runs in a single JVM and tasks run in a thread pool, the size of which is configurable. The standalone mode is good for light-weight data sources such as small databases. The standalone mode is also the default mode for trying and testing Gobblin. In the standalone deployment, the JobScheduler runs as a daemon process that schedules and runs jobs using the so-called JobLauncher s. The JobScheduler maintains a thread pool in which a new JobLauncher is started for each job run. Gobblin ships with two types of JobLauncher s, namely, the LocalJobLauncher and MRJobLauncher for launching and running Gobblin jobs on a single machine and on Hadoop MapReduce, respectively. Which JobLauncher to use can be configured on a per-job basis, which means the JobScheduler can schedule and run jobs in different deployment modes. This section will focus on the LocalJobLauncher for launching and running Gobblin jobs on a single machine. The MRJobLauncher will be covered in a later section on the architecture of Gobblin on Hadoop MapReduce. Each LocalJobLauncher starts and manages a few components for executing tasks of a Gobblin job. Specifically, a TaskExecutor is responsible for executing tasks in a thread pool, whose size is configurable on a per-job basis. A LocalTaskStateTracker is responsible for keep tracking of the state of running tasks, and particularly updating the task metrics. The LocalJobLauncher follows the steps below to launch and run a Gobblin job: Starting the TaskExecutor and LocalTaskStateTracker . Creating an instance of the Source class specified in the job configuration and getting the list of WorkUnit s to do. Creating a task for each WorkUnit in the list, registering the task with the LocalTaskStateTracker , and submitting the task to the TaskExecutor to run. Waiting for all the submitted tasks to finish. Upon completion of all the submitted tasks, collecting tasks states and persisting them to the state store, and publishing the extracted data.","title":"Standalone Architecture "},{"location":"user-guide/Gobblin-Deployment/#mapreduce-architecture","text":"The digram below shows the architecture of Gobblin on Hadoop MapReduce. As the diagram shows, a Gobblin job runs as a mapper-only MapReduce job that runs tasks of the Gobblin job in the mappers. The basic idea here is to use the mappers purely as containers to run Gobblin tasks. This design also makes it easier to integrate with Yarn. Unlike in the standalone mode, task retries are not handled by Gobblin itself in the Hadoop MapReduce mode. Instead, Gobblin relies on the task retry mechanism of Hadoop MapReduce. In this mode, a MRJobLauncher is used to launch and run a Gobblin job on Hadoop MapReduce, following the steps below: Creating an instance of the Source class specified in the job configuration and getting the list of WorkUnit s to do. Serializing each WorkUnit into a file on HDFS that will be read later by a mapper. Creating a file that lists the paths of the files storing serialized WorkUnit s. Creating and configuring a mapper-only Hadoop MapReduce job that takes the file created in step 3 as input. Starting the MapReduce job to run on the cluster of choice and waiting for it to finish. Upon completion of the MapReduce job, collecting tasks states and persisting them to the state store, and publishing the extracted data. A mapper in a Gobblin MapReduce job runs one or more tasks, depending on the number of WorkUnit s to do and the (optional) maximum number of mappers specified in the job configuration. If there is no maximum number of mappers specified in the job configuration, each WorkUnit corresponds to one task that is executed by one mapper and each mapper only runs one task. Otherwise, if a maximum number of mappers is specified and there are more WorkUnit s than the maximum number of mappers allowed, each mapper may handle more than one WorkUnit . There is also a special type of WorkUnit s named MultiWorkUnit that group multiple WorkUnit s to be executed together in one batch in a single mapper. A mapper in a Gobblin MapReduce job follows the step below to run tasks assigned to it: Starting the TaskExecutor that is responsible for executing tasks in a configurable-size thread pool and the MRTaskStateTracker that is responsible for keep tracking of the state of running tasks in the mapper. Reading the next input record that is the path to the file storing a serialized WorkUnit . Deserializing the WorkUnit and adding it to the list of WorkUnit s to do. If the input is a MultiWorkUnit , the WorkUnit s it wraps are all added to the list. Steps 2 and 3 are repeated until all assigned WorkUnit s are deserialized and added to the list. For each WorkUnit on the list of WorkUnit s to do, creating a task for the WorkUnit , registering the task with the MRTaskStateTracker , and submitting the task to the TaskExecutor to run. Note that the tasks may run in parallel if the TaskExecutor is configured to have more than one thread in its thread pool. Waiting for all the submitted tasks to finish. Upon completion of all the submitted tasks, writing out the state of each task into a file that will be read by the MRJobLauncher when collecting task states. Going back to step 2 and reading the next input record if available.","title":"MapReduce architecture "},{"location":"user-guide/Gobblin-Deployment/#master-worker-architecture","text":"","title":"Master-Worker architecture"},{"location":"user-guide/Gobblin-Deployment/#aws-architecture","text":"","title":"AWS architecture"},{"location":"user-guide/Gobblin-Deployment/#yarn-architecture","text":"","title":"YARN architecture"},{"location":"user-guide/Gobblin-Deployment/#gobblin-as-a-service-architecture","text":"","title":"Gobblin-As-A-Service  architecture"},{"location":"user-guide/Gobblin-Schedulers/","text":"Table of Contents Table of Contents Introduction Quartz Azkaban Oozie Launching Gobblin in Local Mode Example Config Files Uploading Files to HDFS Adding Gobblin jar Dependencies Launching the Job Launching Gobblin in MapReduce Mode Example Config Files Further steps Debugging Tips Introduction Gobblin jobs can be scheduled on a recurring basis using a few different tools. Gobblin ships with a built in Quartz Scheduler . Gobblin also integrates with a few other third party tools. Quartz Gobblin has a built in Quartz scheduler as part of the JobScheduler class. This class integrates with the Gobblin SchedulerDaemon , which can be run using the Gobblin `bin/gobblin-standalone.sh script. So in order to take advantage of the Quartz scheduler two steps need to be taken: Use the bin/gobblin-standalone.sh script Add the property job.schedule to the .pull file The value for this property should be a CRONTrigger Azkaban Gobblin can be launched via Azkaban , and open-source Workflow Manager for scheduling and launching Hadoop jobs. Gobblin's AzkabanJobLauncher can be used to launch a Gobblin job through Azkaban. One has to follow the typical setup to create a zip file that can be uploaded to Azkaban (it should include all dependent jars, which can be found in gobblin-dist.tar.gz ). The .job file for the Azkaban Job should contain all configuration properties that would be put in a .pull file (for example, the Wikipedia Example .pull file). All Gobblin system dependent properties (e.g. conf/gobblin-mapreduce.properties or conf/gobblin-standalone.properties ) should also be in the zip file. In the Azkaban .job file, the type parameter should be set to hadoopJava (see here for more information about the hadoopJava Job Type). The job.class parameter should be set to gobblin.azkaban.AzkabanJobLauncher . Oozie Oozie is a very popular scheduler for the Hadoop environment. It allows users to define complex workflows using XML files. A workflow can be composed of a series of actions, such as Java Jobs, Pig Jobs, Spark Jobs, etc. Gobblin has two integration points with Oozie. It can be run as a stand-alone Java process via Oozie's java tag, or it can be run as an Map Reduce job via Oozie. The following guides assume Oozie is already setup and running on some machine, if this is not the case consult the Oozie documentation for getting everything setup. These tutorial only outline how to launch a basic Oozie job that simply runs a Gobblin java a single time. For information on how to build more complex flows, and how to run jobs on a schedule, check out the Oozie documentation online. Launching Gobblin in Local Mode This guide focuses on getting Gobblin to run in as a stand alone Java Process. This means it will not launch a separate MR job to distribute its workload. It is important to understand how the current version of Oozie will launch a Java process. It will first start an MapReduce job and will run the Gobblin as a Java process inside a single map task. The Gobblin job will then ingest all data it is configured to pull and then it will shutdown. Example Config Files gobblin-oozie/src/main/resources/local contains sample configuration files for launching Gobblin Oozie. There are a number of important files in this directory: gobblin-oozie-example-system.properties contains default system level properties for Gobblin. When launched with Oozie, Gobblin will run inside a map task; it is thus recommended to configure Gobblin to write directly to HDFS rather than the local file system. The property fs.uri in this file should be changed to point to the NameNode of the Hadoop File System the job should write to. By default, all data is written under a folder called gobblin-out ; to change this modify the gobblin.work.dir parameter in this file. gobblin-oozie-example-workflow.properties contains default Oozie properties for any job launched. It is also the entry point for launching an Oozie job (e.g. to launch an Oozie job from the command line you execute oozie job -config gobblin-oozie-example-workflow.properties -run ). In this file one needs to update the name.node and resource.manager to the values specific to their environment. Another important property in this file is oozie.wf.application.path ; it points to a folder on HDFS that contains any workflows to be run. It is important to note, that the workflow.xml files must be on HDFS in order for Oozie to pick them up (this is because Oozie typically runs on a separate machine as any client process). gobblin-oozie-example-workflow.xml contains an example Oozie workflow. This example simply launches a Java process that invokes the main method of the CliLocalJobLauncher . The main method of this class expects two file paths to be passed to it (once again these files need to be on HDFS). The jobconfig arg should point to a file on HDFS containing all job configuration parameters. An example jobconfig file can be found here . The sysconfig arg should point to a file on HDFS containing all system configuration parameters. An example sysconfig file for Oozie can be found here . Uploading Files to HDFS Oozie only reads a job properties file from the local file system (e.g. gobblin-oozie-example-workflow.properties ), it expects all other configuration and dependent files to be uploaded to HDFS. Specifically, it looks for these files under the directory specified by oozie.wf.application.path Make sure this is the case before trying to launch an Oozie job. Adding Gobblin jar Dependencies Gobblin has a number of jar dependencies that need to be used when launching a Gobblin job. These dependencies can be taken from the gobblin-dist.tar.gz file that is created after building Gobblin. The tarball should contain a lib folder will the necessary dependencies. This folder should be placed into a lib folder under the same same directory specified by oozie.wf.application.path in the gobblin-oozie-example-workflow.properties file. Launching the Job Assuming one has the Oozie CLI installed, the job can be launched using the following command: oozie job -config gobblin-oozie-example-workflow.properties -run . Launching Gobblin in MapReduce Mode Launching Gobblin in mapreduce Mode works quite similar to the local mode. In this mode, the oozie launcher action will spawn a second mapreduce process where gobblin will process its tasks in distributed mode across the cluster. Since each of the Mappers needs access to the gobblin libraries, we need to provide the jars via the job.hdfs.jars variable Example Config Files gobblin-oozie/src/main/resources/mapreduce contains sample configuration files for launching Gobblin Oozie in Mapreduce mode. The main difference to launching Gobblin Oozie in Local mode are a view extra MapReduce related configuration variables in the sysconfig.properties file and launching CliMRJobLauncher instead CliLocalJobLauncher. Further steps Everything else should be working the same way as in Local mode (see above) Debugging Tips Once the job has been launched, its status can be queried via the following command: oozie job -info oozie-job-id and the logs can be shown via the following command oozie job -log oozie-job-id . In order to get see the standard output of Gobblin, one needs to check the logs the Map task running the Gobblin process. oozie job -info oozie-job-id should show the Hadoop job_id of the Hadoop Job launched to run the Gobblin process. Using this id one should be able to find the logs of the Map tasks through the UI or other command line tools (e.g. yarn logs ).","title":"Schedulers"},{"location":"user-guide/Gobblin-Schedulers/#table-of-contents","text":"Table of Contents Introduction Quartz Azkaban Oozie Launching Gobblin in Local Mode Example Config Files Uploading Files to HDFS Adding Gobblin jar Dependencies Launching the Job Launching Gobblin in MapReduce Mode Example Config Files Further steps Debugging Tips","title":"Table of Contents"},{"location":"user-guide/Gobblin-Schedulers/#introduction","text":"Gobblin jobs can be scheduled on a recurring basis using a few different tools. Gobblin ships with a built in Quartz Scheduler . Gobblin also integrates with a few other third party tools.","title":"Introduction"},{"location":"user-guide/Gobblin-Schedulers/#quartz","text":"Gobblin has a built in Quartz scheduler as part of the JobScheduler class. This class integrates with the Gobblin SchedulerDaemon , which can be run using the Gobblin `bin/gobblin-standalone.sh script. So in order to take advantage of the Quartz scheduler two steps need to be taken: Use the bin/gobblin-standalone.sh script Add the property job.schedule to the .pull file The value for this property should be a CRONTrigger","title":"Quartz"},{"location":"user-guide/Gobblin-Schedulers/#azkaban","text":"Gobblin can be launched via Azkaban , and open-source Workflow Manager for scheduling and launching Hadoop jobs. Gobblin's AzkabanJobLauncher can be used to launch a Gobblin job through Azkaban. One has to follow the typical setup to create a zip file that can be uploaded to Azkaban (it should include all dependent jars, which can be found in gobblin-dist.tar.gz ). The .job file for the Azkaban Job should contain all configuration properties that would be put in a .pull file (for example, the Wikipedia Example .pull file). All Gobblin system dependent properties (e.g. conf/gobblin-mapreduce.properties or conf/gobblin-standalone.properties ) should also be in the zip file. In the Azkaban .job file, the type parameter should be set to hadoopJava (see here for more information about the hadoopJava Job Type). The job.class parameter should be set to gobblin.azkaban.AzkabanJobLauncher .","title":"Azkaban"},{"location":"user-guide/Gobblin-Schedulers/#oozie","text":"Oozie is a very popular scheduler for the Hadoop environment. It allows users to define complex workflows using XML files. A workflow can be composed of a series of actions, such as Java Jobs, Pig Jobs, Spark Jobs, etc. Gobblin has two integration points with Oozie. It can be run as a stand-alone Java process via Oozie's java tag, or it can be run as an Map Reduce job via Oozie. The following guides assume Oozie is already setup and running on some machine, if this is not the case consult the Oozie documentation for getting everything setup. These tutorial only outline how to launch a basic Oozie job that simply runs a Gobblin java a single time. For information on how to build more complex flows, and how to run jobs on a schedule, check out the Oozie documentation online.","title":"Oozie"},{"location":"user-guide/Gobblin-Schedulers/#launching-gobblin-in-local-mode","text":"This guide focuses on getting Gobblin to run in as a stand alone Java Process. This means it will not launch a separate MR job to distribute its workload. It is important to understand how the current version of Oozie will launch a Java process. It will first start an MapReduce job and will run the Gobblin as a Java process inside a single map task. The Gobblin job will then ingest all data it is configured to pull and then it will shutdown.","title":"Launching Gobblin in Local Mode"},{"location":"user-guide/Gobblin-Schedulers/#example-config-files","text":"gobblin-oozie/src/main/resources/local contains sample configuration files for launching Gobblin Oozie. There are a number of important files in this directory: gobblin-oozie-example-system.properties contains default system level properties for Gobblin. When launched with Oozie, Gobblin will run inside a map task; it is thus recommended to configure Gobblin to write directly to HDFS rather than the local file system. The property fs.uri in this file should be changed to point to the NameNode of the Hadoop File System the job should write to. By default, all data is written under a folder called gobblin-out ; to change this modify the gobblin.work.dir parameter in this file. gobblin-oozie-example-workflow.properties contains default Oozie properties for any job launched. It is also the entry point for launching an Oozie job (e.g. to launch an Oozie job from the command line you execute oozie job -config gobblin-oozie-example-workflow.properties -run ). In this file one needs to update the name.node and resource.manager to the values specific to their environment. Another important property in this file is oozie.wf.application.path ; it points to a folder on HDFS that contains any workflows to be run. It is important to note, that the workflow.xml files must be on HDFS in order for Oozie to pick them up (this is because Oozie typically runs on a separate machine as any client process). gobblin-oozie-example-workflow.xml contains an example Oozie workflow. This example simply launches a Java process that invokes the main method of the CliLocalJobLauncher . The main method of this class expects two file paths to be passed to it (once again these files need to be on HDFS). The jobconfig arg should point to a file on HDFS containing all job configuration parameters. An example jobconfig file can be found here . The sysconfig arg should point to a file on HDFS containing all system configuration parameters. An example sysconfig file for Oozie can be found here .","title":"Example Config Files"},{"location":"user-guide/Gobblin-Schedulers/#uploading-files-to-hdfs","text":"Oozie only reads a job properties file from the local file system (e.g. gobblin-oozie-example-workflow.properties ), it expects all other configuration and dependent files to be uploaded to HDFS. Specifically, it looks for these files under the directory specified by oozie.wf.application.path Make sure this is the case before trying to launch an Oozie job.","title":"Uploading Files to HDFS"},{"location":"user-guide/Gobblin-Schedulers/#adding-gobblin-jar-dependencies","text":"Gobblin has a number of jar dependencies that need to be used when launching a Gobblin job. These dependencies can be taken from the gobblin-dist.tar.gz file that is created after building Gobblin. The tarball should contain a lib folder will the necessary dependencies. This folder should be placed into a lib folder under the same same directory specified by oozie.wf.application.path in the gobblin-oozie-example-workflow.properties file.","title":"Adding Gobblin jar Dependencies"},{"location":"user-guide/Gobblin-Schedulers/#launching-the-job","text":"Assuming one has the Oozie CLI installed, the job can be launched using the following command: oozie job -config gobblin-oozie-example-workflow.properties -run .","title":"Launching the Job"},{"location":"user-guide/Gobblin-Schedulers/#launching-gobblin-in-mapreduce-mode","text":"Launching Gobblin in mapreduce Mode works quite similar to the local mode. In this mode, the oozie launcher action will spawn a second mapreduce process where gobblin will process its tasks in distributed mode across the cluster. Since each of the Mappers needs access to the gobblin libraries, we need to provide the jars via the job.hdfs.jars variable","title":"Launching Gobblin in MapReduce Mode"},{"location":"user-guide/Gobblin-Schedulers/#example-config-files_1","text":"gobblin-oozie/src/main/resources/mapreduce contains sample configuration files for launching Gobblin Oozie in Mapreduce mode. The main difference to launching Gobblin Oozie in Local mode are a view extra MapReduce related configuration variables in the sysconfig.properties file and launching CliMRJobLauncher instead CliLocalJobLauncher.","title":"Example Config Files"},{"location":"user-guide/Gobblin-Schedulers/#further-steps","text":"Everything else should be working the same way as in Local mode (see above)","title":"Further steps"},{"location":"user-guide/Gobblin-Schedulers/#debugging-tips","text":"Once the job has been launched, its status can be queried via the following command: oozie job -info oozie-job-id and the logs can be shown via the following command oozie job -log oozie-job-id . In order to get see the standard output of Gobblin, one needs to check the logs the Map task running the Gobblin process. oozie job -info oozie-job-id should show the Hadoop job_id of the Hadoop Job launched to run the Gobblin process. Using this id one should be able to find the logs of the Map tasks through the UI or other command line tools (e.g. yarn logs ).","title":"Debugging Tips"},{"location":"user-guide/Gobblin-as-a-Library/","text":"Table of Contents Table of Contents Using Gobblin as a Library Creating an Embedded Gobblin instance Configuring Embedded Gobblin Running Embedded Gobblin Extending Embedded Gobblin Using Gobblin as a Library A Gobblin ingestion flow can be embedded into a java application using the EmbeddedGobblin class. The following code will run a Hello-World Gobblin job as an embedded application using a template. This will simply print \"Hello World \\ i>!\" to stdout a few times. EmbeddedGobblin embeddedGobblin = new EmbeddedGobblin( TestJob ) .setTemplate(ResourceBasedJobTemplate.forResourcePath( templates/hello-world.template )); JobExecutionResult result = embeddedGobblin.run(); Note: EmbeddedGobblin starts and destroys an embedded Gobblin instance every time run() is called. If an application needs to run a large number of Gobblin jobs, it should instantiate and manage its own Gobblin driver. Creating an Embedded Gobblin instance The code snippet above creates an EmbeddedGobblin instance. This instance can run arbitrary Gobblin ingestion jobs, and allows the use of templates. However, the user needs to configure the job by using the exact key needed for each feature. An alternative is to use a subclass of EmbeddedGobblin which provides methods to more easily configure the job. For example, an easier way to run a Gobblin distcp job is to use EmbeddedGobblinDistcp : EmbeddedGobblinDistcp distcp = new EmbeddedGobblinDistcp(sourcePath, targetPath).delete(); distcp.run(); This subclass automatically knows which template to use, the required configurations for the job (which are included as constructor parameters), and also provides convenience methods for the most common configurations (in the case above, the method delete() instructs the job to delete files that exist in the target but not the source). The following is a non-extensive list of available subclasses of EmbeddedGobblin : EmbeddedGobblinDistcp : distributed copy between Hadoop compatible file systems. EmbeddedWikipediaExample : a getting started example that pulls page updated from Wikipedia. Configuring Embedded Gobblin EmbeddedGobblin allows any configuration that a standalone Gobblin job would allow. EmbeddedGobblin itself provides a few convenience methods to alter the behavior of the Gobblin framework. Other methods allow users to set a job template to use or set job level configurations. Method Parameters Description mrMode N/A Gobblin should run on MR mode. setTemplate Template object to use Use a job template. useStateStore State store directory By default, embedded Gobblin is stateless and disables state store. This method enables the state store at the indicated location allowing using watermarks from previous jobs. distributeJar Path to jar in local fs Indicates that a specific jar is needed by Gobblin workers when running in distributed mode (e.g. MR mode). Gobblin will automatically add this jar to the classpath of the workers. setConfiguration key - value pair Sets a job level configuration. setJobTimeout timeout and time unit, or ISO period Sets the timeout for the Gobblin job. run() will throw a TimeoutException if the job is not done after this period. (Default: 10 days) setLaunchTimeout timeout and time unit, or ISO period Sets the timeout for launching Gobblin job. run() will throw a TimeoutException if the job has not started after this period. (Default: 10 seconds) setShutdownTimeout timeout and time unit, or ISO period Sets the timeout for shutting down embedded Gobblin after the job has finished. run() will throw a TimeoutException if the method has not returned within the timeout after the job finishes. Note that a TimeoutException may indicate that Gobblin could not release JVM resources, including threads. Additional to the above, subclasses of EmbeddedGobblin might offer their own convenience methods. Running Embedded Gobblin After EmbeddedGobblin has been configured it can be run with one of two methods: run() : blocking call. Returns a JobExecutionResult after the job finishes and Gobblin shuts down. runAsync() : asynchronous call. Returns a JobExecutionDriver , which implements Future JobExecutionResult . Extending Embedded Gobblin Developers can extend EmbeddedGobblin to provide users with easier ways to launch a particular type of job. For an example see EmbeddedGobblinDistcp . Best practices: Generally, a subclass of EmbeddedGobblin is based on a template. The template should be automatically loaded on construction and the constructor should call setTemplate(myTemplate) . All required configurations for a job should be parsed from the constructor arguments. User should be able to run new MyEmbeddedGobblinExtension(params...).run() and get a sensible job run. * Convenience methods should be added for the most common configurations users would want to change. In general a convenience method will call a few other methods transparently to the user. For example: public EmbeddedGobblinDistcp simulate() { this.setConfiguration(CopySource.SIMULATE, Boolean.toString(true)); return this; } If the job requires additional jars in the workers that are not part of the minimal Gobblin ingestion classpath (see EmbeddedGobblin#getCoreGobblinJars for this list), then the constructor should call distributeJar(myJar) for the additional jars.","title":"Gobblin as a Library"},{"location":"user-guide/Gobblin-as-a-Library/#table-of-contents","text":"Table of Contents Using Gobblin as a Library Creating an Embedded Gobblin instance Configuring Embedded Gobblin Running Embedded Gobblin Extending Embedded Gobblin","title":"Table of Contents"},{"location":"user-guide/Gobblin-as-a-Library/#using-gobblin-as-a-library","text":"A Gobblin ingestion flow can be embedded into a java application using the EmbeddedGobblin class. The following code will run a Hello-World Gobblin job as an embedded application using a template. This will simply print \"Hello World \\ i>!\" to stdout a few times. EmbeddedGobblin embeddedGobblin = new EmbeddedGobblin( TestJob ) .setTemplate(ResourceBasedJobTemplate.forResourcePath( templates/hello-world.template )); JobExecutionResult result = embeddedGobblin.run(); Note: EmbeddedGobblin starts and destroys an embedded Gobblin instance every time run() is called. If an application needs to run a large number of Gobblin jobs, it should instantiate and manage its own Gobblin driver.","title":"Using Gobblin as a Library"},{"location":"user-guide/Gobblin-as-a-Library/#creating-an-embedded-gobblin-instance","text":"The code snippet above creates an EmbeddedGobblin instance. This instance can run arbitrary Gobblin ingestion jobs, and allows the use of templates. However, the user needs to configure the job by using the exact key needed for each feature. An alternative is to use a subclass of EmbeddedGobblin which provides methods to more easily configure the job. For example, an easier way to run a Gobblin distcp job is to use EmbeddedGobblinDistcp : EmbeddedGobblinDistcp distcp = new EmbeddedGobblinDistcp(sourcePath, targetPath).delete(); distcp.run(); This subclass automatically knows which template to use, the required configurations for the job (which are included as constructor parameters), and also provides convenience methods for the most common configurations (in the case above, the method delete() instructs the job to delete files that exist in the target but not the source). The following is a non-extensive list of available subclasses of EmbeddedGobblin : EmbeddedGobblinDistcp : distributed copy between Hadoop compatible file systems. EmbeddedWikipediaExample : a getting started example that pulls page updated from Wikipedia.","title":"Creating an Embedded Gobblin instance"},{"location":"user-guide/Gobblin-as-a-Library/#configuring-embedded-gobblin","text":"EmbeddedGobblin allows any configuration that a standalone Gobblin job would allow. EmbeddedGobblin itself provides a few convenience methods to alter the behavior of the Gobblin framework. Other methods allow users to set a job template to use or set job level configurations. Method Parameters Description mrMode N/A Gobblin should run on MR mode. setTemplate Template object to use Use a job template. useStateStore State store directory By default, embedded Gobblin is stateless and disables state store. This method enables the state store at the indicated location allowing using watermarks from previous jobs. distributeJar Path to jar in local fs Indicates that a specific jar is needed by Gobblin workers when running in distributed mode (e.g. MR mode). Gobblin will automatically add this jar to the classpath of the workers. setConfiguration key - value pair Sets a job level configuration. setJobTimeout timeout and time unit, or ISO period Sets the timeout for the Gobblin job. run() will throw a TimeoutException if the job is not done after this period. (Default: 10 days) setLaunchTimeout timeout and time unit, or ISO period Sets the timeout for launching Gobblin job. run() will throw a TimeoutException if the job has not started after this period. (Default: 10 seconds) setShutdownTimeout timeout and time unit, or ISO period Sets the timeout for shutting down embedded Gobblin after the job has finished. run() will throw a TimeoutException if the method has not returned within the timeout after the job finishes. Note that a TimeoutException may indicate that Gobblin could not release JVM resources, including threads. Additional to the above, subclasses of EmbeddedGobblin might offer their own convenience methods.","title":"Configuring Embedded Gobblin"},{"location":"user-guide/Gobblin-as-a-Library/#running-embedded-gobblin","text":"After EmbeddedGobblin has been configured it can be run with one of two methods: run() : blocking call. Returns a JobExecutionResult after the job finishes and Gobblin shuts down. runAsync() : asynchronous call. Returns a JobExecutionDriver , which implements Future JobExecutionResult .","title":"Running Embedded Gobblin"},{"location":"user-guide/Gobblin-as-a-Library/#extending-embedded-gobblin","text":"Developers can extend EmbeddedGobblin to provide users with easier ways to launch a particular type of job. For an example see EmbeddedGobblinDistcp . Best practices: Generally, a subclass of EmbeddedGobblin is based on a template. The template should be automatically loaded on construction and the constructor should call setTemplate(myTemplate) . All required configurations for a job should be parsed from the constructor arguments. User should be able to run new MyEmbeddedGobblinExtension(params...).run() and get a sensible job run. * Convenience methods should be added for the most common configurations users would want to change. In general a convenience method will call a few other methods transparently to the user. For example: public EmbeddedGobblinDistcp simulate() { this.setConfiguration(CopySource.SIMULATE, Boolean.toString(true)); return this; } If the job requires additional jars in the workers that are not part of the minimal Gobblin ingestion classpath (see EmbeddedGobblin#getCoreGobblinJars for this list), then the constructor should call distributeJar(myJar) for the additional jars.","title":"Extending Embedded Gobblin"},{"location":"user-guide/Gobblin-genericLoad/","text":"Table of Contents Table of Contents Overview How to submit .pull file through HDFS Overview Previously, the job configuration files could only be loaded from and monitored in the local file system. Efforts have been made to change the limitation and now Gobblin can also load job configuration files in other file systems. Users can easily submit .pull files through their preferred file system and specify it in system configuration accordingly. This page will use the wikipedia example of Gobblin-standalone interacting with job configuration files in HDFS. How to submit .pull file through HDFS Here are the steps to change the system configuration: - Set fs.uri to the HDFS uri that the .pull file will be submitted to. - Use jobconf.fullyQualifiedPath to specify the fully qualified location where pull files should be searched for (this replaces the previously used key jobconf.dir ) With all these changes to gobblin-standalone.properties , you can now submit the .pull to the target file system path.","title":"Generic Configuration Loading"},{"location":"user-guide/Gobblin-genericLoad/#table-of-contents","text":"Table of Contents Overview How to submit .pull file through HDFS","title":"Table of Contents"},{"location":"user-guide/Gobblin-genericLoad/#overview","text":"Previously, the job configuration files could only be loaded from and monitored in the local file system. Efforts have been made to change the limitation and now Gobblin can also load job configuration files in other file systems. Users can easily submit .pull files through their preferred file system and specify it in system configuration accordingly. This page will use the wikipedia example of Gobblin-standalone interacting with job configuration files in HDFS.","title":"Overview"},{"location":"user-guide/Gobblin-genericLoad/#how-to-submit-pull-file-through-hdfs","text":"Here are the steps to change the system configuration: - Set fs.uri to the HDFS uri that the .pull file will be submitted to. - Use jobconf.fullyQualifiedPath to specify the fully qualified location where pull files should be searched for (this replaces the previously used key jobconf.dir ) With all these changes to gobblin-standalone.properties , you can now submit the .pull to the target file system path.","title":"How to submit .pull file through HDFS"},{"location":"user-guide/Gobblin-on-Yarn/","text":"Table of Contents Table of Contents Introduction Architecture Overview The Role of Apache Helix Gobblin Yarn Application Launcher YarnAppSecurityManager LogCopier Gobblin ApplicationMaster YarnService GobblinHelixJobScheduler LogCopier YarnContainerSecurityManager Gobblin WorkUnitRunner TaskExecutor GobblinHelixTaskStateTracker LogCopier YarnContainerSecurityManager Failure Handling ApplicationMaster Failure Handling Container Failure Handling Handling Failures to get ApplicationReport Log Aggregation Security and Delegation Token Management Configuration Configuration Properties Job Lock Configuration System Deployment Deployment on a Unsecured Yarn Cluster Deployment on a Secured Yarn Cluster Supporting Existing Gobblin Jobs Monitoring Introduction Gobblin currently is capable of running in the standalone mode on a single machine or in the MapReduce (MR) mode as a MR job on a Hadoop cluster. A Gobblin job is typically running on a schedule through a scheduler, e.g., the built-in JobScheduler , Azkaban, or Oozie, and each job run ingests new data or data updated since the last run. So this is essentially a batch model for data ingestion and how soon new data becomes available on Hadoop depends on the schedule of the job. On another aspect, for high data volume data sources such as Kafka, Gobblin typically runs in the MR mode with a considerable number of tasks running in the mappers of a MR job. This helps Gobblin to scale out for data sources with large volumes of data. The MR mode, however, suffers from problems such as large overhead mostly due to the overhead of submitting and launching a MR job and poor cluster resource usage. The MR mode is also fundamentally not appropriate for real-time data ingestion given its batch nature. These deficiencies are summarized in more details below: In the MR mode, every Gobblin job run starts a new MR job, which costs a considerable amount of time to allocate and start the containers for running the mapper/reducer tasks. This cost can be totally eliminated if the containers are already up and running. Each Gobblin job running in the MR mode requests a new set of containers and releases them upon job completion. So it's impossible for two jobs to share the containers even though the containers are perfectly capable of running tasks of both jobs. In the MR mode, All WorkUnit s are pre-assigned to the mappers before launching the MR job. The assignment is fixed by evenly distributing the WorkUnit s to the mappers so each mapper gets a fair share of the work in terms of the number of WorkUnits . However, an evenly distributed number of WorkUnit s per mapper does not always guarantee a fair share of the work in terms of the volume of data to pull. This, combined with the fact that the mappers that finish earlier cannot \"steal\" WorkUnit s assigned to other mappers, means the responsibility of load balancing is on the Source implementations, which is not trivial to do, and is virtually impossible in heterogeneous Hadoop clusters where different nodes have different capacity. This also means the duration of a job is determined by the slowest mapper. A MR job can only hold its containers for a limited of time, beyond which the job may get killed. Real-time data ingestion, however, requires the ingestion tasks to be running all the time or alternatively dividing a continuous data stream into well-defined mini-batches (as in Spark Streaming) that can be promptly executed once created. Both require long-running containers, which are not supported in the MR mode. Those deficiencies motivated the work on making Gobblin run on Yarn as a native Yarn application. Running Gobblin as a native Yarn application allows much more control over container provisioning and lifecycle management so it's possible to keep the containers running continuously. It also makes it possible to dynamically change the number of containers at runtime depending on the load to further improve the resource efficiency, something that's impossible in the MR mode. This wiki page documents the design and architecture of the native Gobblin Yarn application and some implementation details. It also covers the configuration system and properties for the application, as well as deployment settings on both unsecured and secured Yarn clusters. Architecture Overview The architecture of Gobblin on Yarn is illustrated in the following diagram. In addition to Yarn, Gobblin on Yarn also leverages Apache Helix , whose role is discussed in The Role of Apache Helix . A Gobblin Yarn application consists of three components: the Yarn Application Launcher, the Yarn ApplicationMaster (serving as the Helix controller ), and the Yarn WorkUnitRunner (serving as the Helix participant ). The following sections describe each component in details. The Role of Apache Helix Apache Helix is mainly used for managing the cluster of containers and running the WorkUnit s through its Distributed Task Execution Framework . The assignment of tasks to available containers (or participants in Helix's term) is handled by Helix through a finite state model named the TaskStateModel . Using this TaskStateModel , Helix is also able to do task rebalancing in case new containers get added or some existing containers die. Clients can also choose to force a task rebalancing if some tasks take much longer time than the others. Helix also supports a way of doing messaging between different components of a cluster, e.g., between the controller to the participants, or between the client and the controller. The Gobblin Yarn application uses this messaging mechanism to implement graceful shutdown initiated by the client as well as delegation token renew notifications from the client to the ApplicationMaster and the WorkUnitRunner containers. Heiix relies on ZooKeeper for its operations, and particularly for maintaining the state of the cluster and the resources (tasks in this case). Both the Helix controller and participants connect to ZooKeeper during their entire lifetime. The ApplicationMaster serves as the Helix controller and the worker containers serve as the Helix participants, respectively, as discussed in details below. Gobblin Yarn Application Launcher The Gobblin Yarn Application Launcher (implemented by class GobblinYarnAppLauncher ) is the client/driver of a Gobblin Yarn application. The first thing the GobblinYarnAppLauncher does when it starts is to register itself with Helix as a spectator and creates a new Helix cluster with name specified through the configuration property gobblin.yarn.helix.cluster.name , if no cluster with the name exists. The GobblinYarnAppLauncher then sets up the Gobblin Yarn application and submits it to run on Yarn. Once the Yarn application successfully starts running, it starts an application state monitor that periodically checks the state of the Gobblin Yarn application. If the state is one of the exit states ( FINISHED , FAILED , or KILLED ), the GobblinYarnAppLauncher shuts down itself. Upon successfully submitting the application to run on Yarn, the GobblinYarnAppLauncher also starts a ServiceManager that manages the following services that auxiliate the running of the application: YarnAppSecurityManager The YarnAppSecurityManager works with the YarnContainerSecurityManager running in the ApplicationMaster and the WorkUnitRunner for a complete solution for security and delegation token management. The YarnAppSecurityManager is responsible for periodically logging in through a Kerberos keytab and getting the delegation token refreshed regularly after each login. Each time the delegation token is refreshed, the YarnContainerSecurityManager writes the new token to a file on HDFS and sends a message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Checkout YarnContainerSecurityManager on how the other side of this system works. LogCopier The service LogCopier in GobblinYarnAppLauncher streams the ApplicationMaster and WorkUnitRunner logs in near real-time from the central location on HDFS where the logs are streamed to from the ApplicationMaster and WorkUnitRunner containers, to the local directory specified through the configuration property gobblin.yarn.logs.sink.root.dir on the machine where the GobblinYarnAppLauncher runs. More details on this can be found in Log Aggregation . Gobblin ApplicationMaster The ApplicationMaster process runs the GobblinApplicationMaster , which uses a ServiceManager to manage the services supporting the operation of the ApplicationMaster process. The services running in GobblinApplicationMaster will be discussed later. When it starts, the first thing GobblinApplicationMaster does is to connect to ZooKeeper and register itself as a Helix controller . It then starts the ServiceManager , which in turn starts the services it manages, as described below. YarnService The service YarnService handles all Yarn-related task including the following: Registering and un-registering the ApplicationMaster with the Yarn ResourceManager. Requesting the initial set of containers from the Yarn ResourceManager. Handling any container changes at runtime, e.g., adding more containers or shutting down containers no longer needed. This also includes stopping running containers when the application is asked to stop. This design makes it switch to a different resource manager, e.g., Mesos, by replacing the service YarnService with something else specific to the resource manager, e.g., MesosService . GobblinHelixJobScheduler GobblinApplicationMaster runs the GobblinHelixJobScheduler that schedules jobs to run through the Helix Distributed Task Execution Framework . For each Gobblin job run, the GobblinHelixJobScheduler starts a GobblinHelixJobLauncher that wraps the Gobblin job into a GobblinHelixJob and each Gobblin Task into a GobblinHelixTask , which implements the Helix's Task interface so Helix knows how to execute it. The GobblinHelixJobLauncher then submits the job to a Helix job queue named after the Gobblin job name, from which the Helix Distributed Task Execution Framework picks up the job and runs its tasks through the live participants (available containers). Like the LocalJobLauncher and MRJobLauncher , the GobblinHelixJobLauncher handles output data commit and job state persistence. LogCopier The service LogCopier in GobblinApplicationMaster streams the ApplicationMaster logs in near real-time from the machine running the ApplicationMaster container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in Log Aggregation . YarnContainerSecurityManager The YarnContainerSecurityManager runs in both the ApplicationMaster and the WorkUnitRunner. When it starts, it registers a message handler with the HelixManager for handling messages on refreshes of the delegation token. Once such a message is received, the YarnContainerSecurityManager gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file. Gobblin WorkUnitRunner The WorkUnitRunner process runs as part of GobblinTaskRunner , which uses a ServiceManager to manage the services supporting the operation of the WorkUnitRunner process. The services running in GobblinWorkUnitRunner will be discussed later. When it starts, the first thing GobblinWorkUnitRunner does is to connect to ZooKeeper and register itself as a Helix participant . It then starts the ServiceManager , which in turn starts the services it manages, as discussed below. TaskExecutor The TaskExecutor remains the same as in the standalone and MR modes, and is purely responsible for running tasks assigned to a WorkUnitRunner. GobblinHelixTaskStateTracker The GobblinHelixTaskStateTracker has a similar responsibility as the LocalTaskStateTracker and MRTaskStateTracker : keeping track of the state of running tasks including operational metrics, e.g., total records pulled, records pulled per second, total bytes pulled, bytes pulled per second, etc. LogCopier The service LogCopier in GobblinWorkUnitRunner streams the WorkUnitRunner logs in near real-time from the machine running the WorkUnitRunner container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in Log Aggregation . YarnContainerSecurityManager The YarnContainerSecurityManager in GobblinWorkUnitRunner works in the same way as it in GobblinApplicationMaster . Failure Handling ApplicationMaster Failure Handling Under normal operation, the Gobblin ApplicationMaster stays alive unless being asked to stop through a message sent from the launcher (the GobblinYarnAppLauncher ) as part of the orderly shutdown process. It may, however, fail or get killed by the Yarn ResourceManager for various reasons. For example, the container running the ApplicationMaster may fail and exit due to node failures, or get killed because of using more memory than claimed. When a shutdown of the ApplicationMaster is triggered (e.g., when the shutdown hook is triggered) for any reason, it does so gracefully, i.e., it attempts to stop every services it manages, stop all the running containers, and unregister itself with the ResourceManager. Shutting down the ApplicationMaster shuts down the Yarn application and the application launcher will eventually know that the application completes through a periodic check on the application status. Container Failure Handling Under normal operation, a Gobblin Yarn container stays alive unless being released and stopped by the Gobblin ApplicationMaster, and in this case the exit status of the container will be zero. However, a container may exit unexpectedly due to various reasons. For example, a container may fail and exit due to node failures, or be killed because of using more memory than claimed. In this case when a container exits abnormally with a non-zero exit code, Gobblin Yarn tries to restart the Helix instance running in the container by requesting a new Yarn container as a replacement to run the instance. The maximum number of retries can be configured through the key gobblin.yarn.helix.instance.max.retries . When requesting a new container to replace the one that completes and exits abnormally, the application has a choice of specifying the same host that runs the completed container as the preferred host, depending on the boolean value of configuration key gobblin.yarn.container.affinity.enabled . Note that for certain exit codes that indicate something wrong with the host, the value of gobblin.yarn.container.affinity.enabled is ignored and no preferred host gets specified, leaving Yarn to figure out a good candidate node for the new container. Handling Failures to get ApplicationReport As mentioned above, once the Gobblin Yarn application successfully starts running, the GobblinYarnAppLauncher starts an application state monitor that periodically checks the state of the Yarn application by getting an ApplicationReport . It may fail to do so and throw an exception, however, if the Yarn client is having some problem connecting and communicating with the Yarn cluster. For example, if the Yarn cluster is down for maintenance, the Yarn client will not be able to get an ApplicationReport . The GobblinYarnAppLauncher keeps track of the number of consecutive failures to get an ApplicationReport and initiates a shutdown if this number exceeds the threshold as specified through the configuration property gobblin.yarn.max.get.app.report.failures . The shutdown will trigger an email notification if the configuration property gobblin.yarn.email.notification.on.shutdown is set to true . Log Aggregation Yarn provides both a Web UI and a command-line tool to access the logs of an application, and also does log aggregation so the logs of all the containers become available on the client side upon requested. However, there are a few limitations that make it hard to access the logs of an application at runtime: The command-line utility for downloading the aggregated logs will only be able to do so after the application finishes, making it useless for getting access to the logs at the application runtime. The Web UI does allow logs to be viewed at runtime, but only when the user that access the UI is the same as the user that launches the application. On a Yarn cluster where security is enabled, the user launching the Gobblin Yarn application is typically a user of some headless account. Because Gobblin runs on Yarn as a long-running native Yarn application, getting access to the logs at runtime is critical to know what's going on in the application and to detect any issues in the application as early as possible. Unfortunately we cannot use the log facility provided by Yarn here due to the above limitations. Alternatively, Gobblin on Yarn has its own mechanism for doing log aggregation and providing access to the logs at runtime, described as follows. Both the Gobblin ApplicationMaster and WorkUnitRunner run a LogCopier that periodically copies new entries of both stdout and stderr logs of the corresponding processes from the containers to a central location on HDFS under the directory ${gobblin.yarn.work.dir}/_applogs in the subdirectories named after the container IDs, one per container. The names of the log files on HDFS combine the container IDs and the original log file names so it's easy to tell which container generates which log file. More specifically, the log files produced by the ApplicationMaster are named container id .GobblinApplicationMaster.{stdout,stderr} , and the log files produced by the WorkUnitRunner are named container id .GobblinWorkUnitRunner.{stdout,stderr} . The Gobblin YarnApplicationLauncher also runs a LogCopier that periodically copies new log entries from log files under ${gobblin.yarn.work.dir}/_applogs on HDFS to the local filesystem under the directory configured by the property gobblin.yarn.logs.sink.root.dir . By default, the LogCopier checks for new log entries every 60 seconds and will keep reading new log entries until it reaches the end of the log file. This setup enables the Gobblin Yarn application to stream container process logs near real-time all the way to the client/driver. Security and Delegation Token Management On a Yarn cluster with security enabled (e.g., Kerberos authentication is required to access HDFS), security and delegation token management is necessary to allow Gobblin run as a long-running Yarn application. Specifically, Gobblin running on a secured Yarn cluster needs to get its delegation token for accessing HDFS renewed periodically, which also requires periodic keytab re-logins because a delegation token can only be renewed up to a limited number of times in one login. The Gobblin Yarn application supports Kerberos-based authentication and login through a keytab file. The YarnAppSecurityManager running in the Yarn Application Launcher and the YarnContainerSecurityManager running in the ApplicationMaster and WorkUnitRunner work together to get every Yarn containers updated whenever the delegation token gets updated on the client side by the YarnAppSecurityManager . More specifically, the YarnAppSecurityManager periodically logins through the keytab and gets the delegation token refreshed regularly after each successful login. Every time the YarnAppSecurityManager refreshes the delegation token, the YarnContainerSecurityManager writes the new token to a file on HDFS and sends a TOKEN_FILE_UPDATED message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Upon receiving such a message, the YarnContainerSecurityManager running in the ApplicationMaster or WorkUnitRunner gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file. Both the interval between two Kerberos keytab logins and the interval between two delegation token refreshes are configurable, through the configuration properties gobblin.yarn.login.interval.minutes and gobblin.yarn.token.renew.interval.minutes , respectively. Configuration Configuration Properties In additional to the common Gobblin configuration properties, documented in Configuration Properties Glossary , Gobblin on Yarn uses the following configuration properties. Property Default Value Description gobblin.yarn.app.name GobblinYarn The Gobblin Yarn appliation name. gobblin.yarn.app.queue default The Yarn queue the Gobblin Yarn application will run in. gobblin.yarn.work.dir /gobblin The working directory (typically on HDFS) for the Gobblin Yarn application. gobblin.yarn.app.report.interval.minutes 5 The interval in minutes between two Gobblin Yarn application status reports. gobblin.yarn.max.get.app.report.failures 4 Maximum allowed number of consecutive failures to get a Yarn ApplicationReport . gobblin.yarn.email.notification.on.shutdown false Whether email notification is enabled or not on shutdown of the GobblinYarnAppLauncher . If this is set to true , the following configuration properties also need to be set for email notification to work: email.host , email.smtp.port , email.user , email.password , email.from , and email.tos . Refer to Email Alert Properties for more information on those configuration properties. gobblin.yarn.app.master.memory.mbs 512 How much memory in MBs to request for the container running the Gobblin ApplicationMaster. gobblin.yarn.app.master.cores 1 The number of vcores to request for the container running the Gobblin ApplicationMaster. gobblin.yarn.app.master.jars A comma-separated list of jars the Gobblin ApplicationMaster depends on but not in the lib directory. gobblin.yarn.app.master.files.local A comma-separated list of files on the local filesystem the Gobblin ApplicationMaster depends on. gobblin.yarn.app.master.files.remote A comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin ApplicationMaster depends on. gobblin.yarn.app.master.jvm.args Additional JVM arguments for the JVM process running the Gobblin ApplicationMaster, e.g., -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads . gobblin.yarn.initial.containers 1 The number of containers to request initially when the application starts to run the WorkUnitRunner. gobblin.yarn.container.memory.mbs 512 How much memory in MBs to request for the container running the Gobblin WorkUnitRunner. gobblin.yarn.container.cores 1 The number of vcores to request for the container running the Gobblin WorkUnitRunner. gobblin.yarn.container.jars A comma-separated list of jars the Gobblin WorkUnitRunner depends on but not in the lib directory. gobblin.yarn.container.files.local A comma-separated list of files on the local filesystem the Gobblin WorkUnitRunner depends on. gobblin.yarn.container.files.remote A comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin WorkUnitRunner depends on. gobblin.yarn.container.jvm.args Additional JVM arguments for the JVM process running the Gobblin WorkUnitRunner, e.g., -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads . gobblin.yarn.container.affinity.enabled true Whether the same host should be used as the preferred host when requesting a replacement container for the one that exits. gobblin.yarn.helix.cluster.name GobblinYarn The name of the Helix cluster that will be registered with ZooKeeper. gobblin.yarn.zk.connection.string localhost:2181 The ZooKeeper connection string used by Helix. helix.instance.max.retries 2 Maximum number of times the application tries to restart a failed Helix instance (corresponding to a Yarn container). gobblin.yarn.lib.jars.dir The directory where library jars are stored, typically gobblin-dist/lib . gobblin.yarn.job.conf.path The path to either a directory where Gobblin job configuration files are stored or a single job configuration file. Internally Gobblin Yarn will package the configuration files as a tarball so you don't need to. gobblin.yarn.logs.sink.root.dir The directory on local filesystem on the driver/client side where the aggregated container logs of both the ApplicationMaster and WorkUnitRunner are stored. gobblin.yarn.log.copier.max.file.size Unbounded The maximum bytes per log file. When this is exceeded a new log file will be created. gobblin.yarn.log.copier.scheduler ScheduledExecutorService The scheduler to use to copy the log files. Possible values: ScheduledExecutorService , HashedWheelTimer . The HashedWheelTimer scheduler is experimental but is expected to become the default after a sufficient burn in period. gobblin.yarn.keytab.file.path The path to the Kerberos keytab file used for keytab-based authentication/login. gobblin.yarn.keytab.principal.name The principal name of the keytab. gobblin.yarn.login.interval.minutes 1440 The interval in minutes between two keytab logins. gobblin.yarn.token.renew.interval.minutes 720 The interval in minutes between two delegation token renews. Job Lock It is recommended to use zookeeper for maintaining job locks. See ZookeeperBasedJobLock Properties for the relevant configuration properties. Configuration System The Gobblin Yarn application uses the Typesafe Config library to handle the application configuration. Following Typesafe Config 's model, the Gobblin Yarn application uses a single file named application.conf for all configuration properties and another file named reference.conf for default values. A sample application.conf is shown below: # Yarn/Helix configuration properties gobblin.yarn.helix.cluster.name=GobblinYarnTest gobblin.yarn.app.name=GobblinYarnTest gobblin.yarn.lib.jars.dir= /home/gobblin/gobblin-dist/lib/ gobblin.yarn.app.master.files.local= /home/gobblin/gobblin-dist/conf/log4j-yarn.properties,/home/gobblin/gobblin-dist/conf/application.conf,/home/gobblin/gobblin-dist/conf/reference.conf gobblin.yarn.container.files.local=${gobblin.yarn.app.master.files.local} gobblin.yarn.job.conf.path= /home/gobblin/gobblin-dist/job-conf gobblin.yarn.keytab.file.path= /home/gobblin/gobblin.headless.keytab gobblin.yarn.keytab.principal.name=gobblin gobblin.yarn.app.master.jvm.args= -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m gobblin.yarn.container.jvm.args= -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m gobblin.yarn.logs.sink.root.dir=/home/gobblin/gobblin-dist/applogs # File system URIs writer.fs.uri=${fs.uri} state.store.fs.uri=${fs.uri} # Writer related configuration properties writer.destination.type=HDFS writer.output.format=AVRO writer.staging.dir=${gobblin.yarn.work.dir}/task-staging writer.output.dir=${gobblin.yarn.work.dir}/task-output # Data publisher related configuration properties data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher data.publisher.final.dir=${gobblin.yarn.work.dir}/job-output data.publisher.replace.final.dir=false # Directory where job/task state files are stored state.store.dir=${gobblin.yarn.work.dir}/state-store # Directory where error files from the quality checkers are stored qualitychecker.row.err.file=${gobblin.yarn.work.dir}/err # Use zookeeper for maintaining the job lock job.lock.enabled=true job.lock.type=ZookeeperBasedJobLock # Directory where job locks are stored job.lock.dir=${gobblin.yarn.work.dir}/locks # Directory where metrics log files are stored metrics.log.dir=${gobblin.yarn.work.dir}/metrics A sample reference.conf is shown below: # Yarn/Helix configuration properties gobblin.yarn.app.queue=default gobblin.yarn.helix.cluster.name=GobblinYarn gobblin.yarn.app.name=GobblinYarn gobblin.yarn.app.master.memory.mbs=512 gobblin.yarn.app.master.cores=1 gobblin.yarn.app.report.interval.minutes=5 gobblin.yarn.max.get.app.report.failures=4 gobblin.yarn.email.notification.on.shutdown=false gobblin.yarn.initial.containers=1 gobblin.yarn.container.memory.mbs=512 gobblin.yarn.container.cores=1 gobblin.yarn.container.affinity.enabled=true gobblin.yarn.helix.instance.max.retries=2 gobblin.yarn.keytab.login.interval.minutes=1440 gobblin.yarn.token.renew.interval.minutes=720 gobblin.yarn.work.dir=/user/gobblin/gobblin-yarn gobblin.yarn.zk.connection.string=${zookeeper.connection.string} fs.uri= hdfs://localhost:9000 zookeeper.connection.string= localhost:2181 Deployment A standard deployment of Gobblin on Yarn requires a Yarn cluster running Hadoop 2.x ( 2.3.0 and above recommended) and a ZooKeeper cluster. Make sure the client machine (typically the gateway of the Yarn cluster) is able to access the ZooKeeper instance. Deployment on a Unsecured Yarn Cluster To do a deployment of the Gobblin Yarn application, first build Gobblin using the following command from the root directory of the Gobblin project. ./gradlew clean build To build Gobblin against a specific version of Hadoop 2.x, e.g., 2.7.0 , run the following command instead: ./gradlew clean build -PhadoopVersion=2.7.0 After Gobblin is successfully built, a tarball named gobblin-dist-[project-version].tar.gz should have been created under the root directory of the project. To deploy the Gobblin Yarn application on a unsecured Yarn cluster, uncompress the tarball somewhere and run the following commands: cd gobblin-dist bin/gobblin-yarn.sh Note that for the above commands to work, the Hadoop/Yarn configuration directory must be on the classpath and the configuration must be pointing to the right Yarn cluster, or specifically the right ResourceManager and NameNode URLs. This is defined like the following in gobblin-yarn.sh : CLASSPATH=${FWDIR_CONF}:${GOBBLIN_JARS}:${YARN_CONF_DIR}:${HADOOP_YARN_HOME}/lib Deployment on a Secured Yarn Cluster When deploying the Gobblin Yarn application on a secured Yarn cluster, make sure the keytab file path is correctly specified in application.conf and the correct principal for the keytab is used as follows. The rest of the deployment is the same as that on a unsecured Yarn cluster. gobblin.yarn.keytab.file.path= /home/gobblin/gobblin.headless.keytab gobblin.yarn.keytab.principal.name=gobblin Supporting Existing Gobblin Jobs Gobblin on Yarn is backward compatible and supports existing Gobblin jobs running in the standalone and MR modes. To run existing Gobblin jobs, simply put the job configuration files into a directory on the local file system of the driver and setting the configuration property gobblin.yarn.job.conf.path to point to the directory. When the Gobblin Yarn application starts, Yarn will package the configuration files as a tarball and make sure the tarball gets copied to the ApplicationMaster and properly uncompressed. The GobblinHelixJobScheduler then loads the job configuration files and schedule the jobs to run. Monitoring Gobblin Yarn uses the Gobblin Metrics library for collecting and reporting metrics at the container, job, and task levels. Each GobblinWorkUnitRunner maintains a ContainerMetrics that is the parent of the JobMetrics of each job run the container is involved, which is the parent of the TaskMetrics of each task of the job run. This hierarchical structure allows us to do pre-aggregation in the containers before reporting the metrics to the backend. Collected metrics can be reported to various sinks such as Kafka, files, and JMX, depending on the configuration. Specifically, metrics.enabled controls whether metrics collecting and reporting are enabled or not. metrics.reporting.kafka.enabled , metrics.reporting.file.enabled , and metrics.reporting.jmx.enabled control whether collected metrics should be reported or not to Kafka, files, and JMX, respectively. Please refer to Metrics Properties for the available configuration properties related to metrics collecting and reporting. In addition to metric collecting and reporting, Gobblin Yarn also supports writing job execution information to a MySQL-backed job execution history store, which keeps track of job execution information. Please refer to the DDL for the relevant MySQL tables. Detailed information on the job execution history store including how to configure it can be found here .","title":"Gobblin on Yarn"},{"location":"user-guide/Gobblin-on-Yarn/#table-of-contents","text":"Table of Contents Introduction Architecture Overview The Role of Apache Helix Gobblin Yarn Application Launcher YarnAppSecurityManager LogCopier Gobblin ApplicationMaster YarnService GobblinHelixJobScheduler LogCopier YarnContainerSecurityManager Gobblin WorkUnitRunner TaskExecutor GobblinHelixTaskStateTracker LogCopier YarnContainerSecurityManager Failure Handling ApplicationMaster Failure Handling Container Failure Handling Handling Failures to get ApplicationReport Log Aggregation Security and Delegation Token Management Configuration Configuration Properties Job Lock Configuration System Deployment Deployment on a Unsecured Yarn Cluster Deployment on a Secured Yarn Cluster Supporting Existing Gobblin Jobs Monitoring","title":"Table of Contents"},{"location":"user-guide/Gobblin-on-Yarn/#introduction","text":"Gobblin currently is capable of running in the standalone mode on a single machine or in the MapReduce (MR) mode as a MR job on a Hadoop cluster. A Gobblin job is typically running on a schedule through a scheduler, e.g., the built-in JobScheduler , Azkaban, or Oozie, and each job run ingests new data or data updated since the last run. So this is essentially a batch model for data ingestion and how soon new data becomes available on Hadoop depends on the schedule of the job. On another aspect, for high data volume data sources such as Kafka, Gobblin typically runs in the MR mode with a considerable number of tasks running in the mappers of a MR job. This helps Gobblin to scale out for data sources with large volumes of data. The MR mode, however, suffers from problems such as large overhead mostly due to the overhead of submitting and launching a MR job and poor cluster resource usage. The MR mode is also fundamentally not appropriate for real-time data ingestion given its batch nature. These deficiencies are summarized in more details below: In the MR mode, every Gobblin job run starts a new MR job, which costs a considerable amount of time to allocate and start the containers for running the mapper/reducer tasks. This cost can be totally eliminated if the containers are already up and running. Each Gobblin job running in the MR mode requests a new set of containers and releases them upon job completion. So it's impossible for two jobs to share the containers even though the containers are perfectly capable of running tasks of both jobs. In the MR mode, All WorkUnit s are pre-assigned to the mappers before launching the MR job. The assignment is fixed by evenly distributing the WorkUnit s to the mappers so each mapper gets a fair share of the work in terms of the number of WorkUnits . However, an evenly distributed number of WorkUnit s per mapper does not always guarantee a fair share of the work in terms of the volume of data to pull. This, combined with the fact that the mappers that finish earlier cannot \"steal\" WorkUnit s assigned to other mappers, means the responsibility of load balancing is on the Source implementations, which is not trivial to do, and is virtually impossible in heterogeneous Hadoop clusters where different nodes have different capacity. This also means the duration of a job is determined by the slowest mapper. A MR job can only hold its containers for a limited of time, beyond which the job may get killed. Real-time data ingestion, however, requires the ingestion tasks to be running all the time or alternatively dividing a continuous data stream into well-defined mini-batches (as in Spark Streaming) that can be promptly executed once created. Both require long-running containers, which are not supported in the MR mode. Those deficiencies motivated the work on making Gobblin run on Yarn as a native Yarn application. Running Gobblin as a native Yarn application allows much more control over container provisioning and lifecycle management so it's possible to keep the containers running continuously. It also makes it possible to dynamically change the number of containers at runtime depending on the load to further improve the resource efficiency, something that's impossible in the MR mode. This wiki page documents the design and architecture of the native Gobblin Yarn application and some implementation details. It also covers the configuration system and properties for the application, as well as deployment settings on both unsecured and secured Yarn clusters.","title":"Introduction"},{"location":"user-guide/Gobblin-on-Yarn/#architecture","text":"","title":"Architecture"},{"location":"user-guide/Gobblin-on-Yarn/#overview","text":"The architecture of Gobblin on Yarn is illustrated in the following diagram. In addition to Yarn, Gobblin on Yarn also leverages Apache Helix , whose role is discussed in The Role of Apache Helix . A Gobblin Yarn application consists of three components: the Yarn Application Launcher, the Yarn ApplicationMaster (serving as the Helix controller ), and the Yarn WorkUnitRunner (serving as the Helix participant ). The following sections describe each component in details.","title":"Overview"},{"location":"user-guide/Gobblin-on-Yarn/#the-role-of-apache-helix","text":"Apache Helix is mainly used for managing the cluster of containers and running the WorkUnit s through its Distributed Task Execution Framework . The assignment of tasks to available containers (or participants in Helix's term) is handled by Helix through a finite state model named the TaskStateModel . Using this TaskStateModel , Helix is also able to do task rebalancing in case new containers get added or some existing containers die. Clients can also choose to force a task rebalancing if some tasks take much longer time than the others. Helix also supports a way of doing messaging between different components of a cluster, e.g., between the controller to the participants, or between the client and the controller. The Gobblin Yarn application uses this messaging mechanism to implement graceful shutdown initiated by the client as well as delegation token renew notifications from the client to the ApplicationMaster and the WorkUnitRunner containers. Heiix relies on ZooKeeper for its operations, and particularly for maintaining the state of the cluster and the resources (tasks in this case). Both the Helix controller and participants connect to ZooKeeper during their entire lifetime. The ApplicationMaster serves as the Helix controller and the worker containers serve as the Helix participants, respectively, as discussed in details below.","title":"The Role of Apache Helix"},{"location":"user-guide/Gobblin-on-Yarn/#gobblin-yarn-application-launcher","text":"The Gobblin Yarn Application Launcher (implemented by class GobblinYarnAppLauncher ) is the client/driver of a Gobblin Yarn application. The first thing the GobblinYarnAppLauncher does when it starts is to register itself with Helix as a spectator and creates a new Helix cluster with name specified through the configuration property gobblin.yarn.helix.cluster.name , if no cluster with the name exists. The GobblinYarnAppLauncher then sets up the Gobblin Yarn application and submits it to run on Yarn. Once the Yarn application successfully starts running, it starts an application state monitor that periodically checks the state of the Gobblin Yarn application. If the state is one of the exit states ( FINISHED , FAILED , or KILLED ), the GobblinYarnAppLauncher shuts down itself. Upon successfully submitting the application to run on Yarn, the GobblinYarnAppLauncher also starts a ServiceManager that manages the following services that auxiliate the running of the application:","title":"Gobblin Yarn Application Launcher"},{"location":"user-guide/Gobblin-on-Yarn/#yarnappsecuritymanager","text":"The YarnAppSecurityManager works with the YarnContainerSecurityManager running in the ApplicationMaster and the WorkUnitRunner for a complete solution for security and delegation token management. The YarnAppSecurityManager is responsible for periodically logging in through a Kerberos keytab and getting the delegation token refreshed regularly after each login. Each time the delegation token is refreshed, the YarnContainerSecurityManager writes the new token to a file on HDFS and sends a message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Checkout YarnContainerSecurityManager on how the other side of this system works.","title":"YarnAppSecurityManager"},{"location":"user-guide/Gobblin-on-Yarn/#logcopier","text":"The service LogCopier in GobblinYarnAppLauncher streams the ApplicationMaster and WorkUnitRunner logs in near real-time from the central location on HDFS where the logs are streamed to from the ApplicationMaster and WorkUnitRunner containers, to the local directory specified through the configuration property gobblin.yarn.logs.sink.root.dir on the machine where the GobblinYarnAppLauncher runs. More details on this can be found in Log Aggregation .","title":"LogCopier"},{"location":"user-guide/Gobblin-on-Yarn/#gobblin-applicationmaster","text":"The ApplicationMaster process runs the GobblinApplicationMaster , which uses a ServiceManager to manage the services supporting the operation of the ApplicationMaster process. The services running in GobblinApplicationMaster will be discussed later. When it starts, the first thing GobblinApplicationMaster does is to connect to ZooKeeper and register itself as a Helix controller . It then starts the ServiceManager , which in turn starts the services it manages, as described below.","title":"Gobblin ApplicationMaster"},{"location":"user-guide/Gobblin-on-Yarn/#yarnservice","text":"The service YarnService handles all Yarn-related task including the following: Registering and un-registering the ApplicationMaster with the Yarn ResourceManager. Requesting the initial set of containers from the Yarn ResourceManager. Handling any container changes at runtime, e.g., adding more containers or shutting down containers no longer needed. This also includes stopping running containers when the application is asked to stop. This design makes it switch to a different resource manager, e.g., Mesos, by replacing the service YarnService with something else specific to the resource manager, e.g., MesosService .","title":"YarnService"},{"location":"user-guide/Gobblin-on-Yarn/#gobblinhelixjobscheduler","text":"GobblinApplicationMaster runs the GobblinHelixJobScheduler that schedules jobs to run through the Helix Distributed Task Execution Framework . For each Gobblin job run, the GobblinHelixJobScheduler starts a GobblinHelixJobLauncher that wraps the Gobblin job into a GobblinHelixJob and each Gobblin Task into a GobblinHelixTask , which implements the Helix's Task interface so Helix knows how to execute it. The GobblinHelixJobLauncher then submits the job to a Helix job queue named after the Gobblin job name, from which the Helix Distributed Task Execution Framework picks up the job and runs its tasks through the live participants (available containers). Like the LocalJobLauncher and MRJobLauncher , the GobblinHelixJobLauncher handles output data commit and job state persistence.","title":"GobblinHelixJobScheduler"},{"location":"user-guide/Gobblin-on-Yarn/#logcopier_1","text":"The service LogCopier in GobblinApplicationMaster streams the ApplicationMaster logs in near real-time from the machine running the ApplicationMaster container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in Log Aggregation .","title":"LogCopier"},{"location":"user-guide/Gobblin-on-Yarn/#yarncontainersecuritymanager","text":"The YarnContainerSecurityManager runs in both the ApplicationMaster and the WorkUnitRunner. When it starts, it registers a message handler with the HelixManager for handling messages on refreshes of the delegation token. Once such a message is received, the YarnContainerSecurityManager gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file.","title":"YarnContainerSecurityManager"},{"location":"user-guide/Gobblin-on-Yarn/#gobblin-workunitrunner","text":"The WorkUnitRunner process runs as part of GobblinTaskRunner , which uses a ServiceManager to manage the services supporting the operation of the WorkUnitRunner process. The services running in GobblinWorkUnitRunner will be discussed later. When it starts, the first thing GobblinWorkUnitRunner does is to connect to ZooKeeper and register itself as a Helix participant . It then starts the ServiceManager , which in turn starts the services it manages, as discussed below.","title":"Gobblin WorkUnitRunner"},{"location":"user-guide/Gobblin-on-Yarn/#taskexecutor","text":"The TaskExecutor remains the same as in the standalone and MR modes, and is purely responsible for running tasks assigned to a WorkUnitRunner.","title":"TaskExecutor"},{"location":"user-guide/Gobblin-on-Yarn/#gobblinhelixtaskstatetracker","text":"The GobblinHelixTaskStateTracker has a similar responsibility as the LocalTaskStateTracker and MRTaskStateTracker : keeping track of the state of running tasks including operational metrics, e.g., total records pulled, records pulled per second, total bytes pulled, bytes pulled per second, etc.","title":"GobblinHelixTaskStateTracker"},{"location":"user-guide/Gobblin-on-Yarn/#logcopier_2","text":"The service LogCopier in GobblinWorkUnitRunner streams the WorkUnitRunner logs in near real-time from the machine running the WorkUnitRunner container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in Log Aggregation .","title":"LogCopier"},{"location":"user-guide/Gobblin-on-Yarn/#yarncontainersecuritymanager_1","text":"The YarnContainerSecurityManager in GobblinWorkUnitRunner works in the same way as it in GobblinApplicationMaster .","title":"YarnContainerSecurityManager"},{"location":"user-guide/Gobblin-on-Yarn/#failure-handling","text":"","title":"Failure Handling"},{"location":"user-guide/Gobblin-on-Yarn/#applicationmaster-failure-handling","text":"Under normal operation, the Gobblin ApplicationMaster stays alive unless being asked to stop through a message sent from the launcher (the GobblinYarnAppLauncher ) as part of the orderly shutdown process. It may, however, fail or get killed by the Yarn ResourceManager for various reasons. For example, the container running the ApplicationMaster may fail and exit due to node failures, or get killed because of using more memory than claimed. When a shutdown of the ApplicationMaster is triggered (e.g., when the shutdown hook is triggered) for any reason, it does so gracefully, i.e., it attempts to stop every services it manages, stop all the running containers, and unregister itself with the ResourceManager. Shutting down the ApplicationMaster shuts down the Yarn application and the application launcher will eventually know that the application completes through a periodic check on the application status.","title":"ApplicationMaster Failure Handling"},{"location":"user-guide/Gobblin-on-Yarn/#container-failure-handling","text":"Under normal operation, a Gobblin Yarn container stays alive unless being released and stopped by the Gobblin ApplicationMaster, and in this case the exit status of the container will be zero. However, a container may exit unexpectedly due to various reasons. For example, a container may fail and exit due to node failures, or be killed because of using more memory than claimed. In this case when a container exits abnormally with a non-zero exit code, Gobblin Yarn tries to restart the Helix instance running in the container by requesting a new Yarn container as a replacement to run the instance. The maximum number of retries can be configured through the key gobblin.yarn.helix.instance.max.retries . When requesting a new container to replace the one that completes and exits abnormally, the application has a choice of specifying the same host that runs the completed container as the preferred host, depending on the boolean value of configuration key gobblin.yarn.container.affinity.enabled . Note that for certain exit codes that indicate something wrong with the host, the value of gobblin.yarn.container.affinity.enabled is ignored and no preferred host gets specified, leaving Yarn to figure out a good candidate node for the new container.","title":"Container Failure Handling"},{"location":"user-guide/Gobblin-on-Yarn/#handling-failures-to-get-applicationreport","text":"As mentioned above, once the Gobblin Yarn application successfully starts running, the GobblinYarnAppLauncher starts an application state monitor that periodically checks the state of the Yarn application by getting an ApplicationReport . It may fail to do so and throw an exception, however, if the Yarn client is having some problem connecting and communicating with the Yarn cluster. For example, if the Yarn cluster is down for maintenance, the Yarn client will not be able to get an ApplicationReport . The GobblinYarnAppLauncher keeps track of the number of consecutive failures to get an ApplicationReport and initiates a shutdown if this number exceeds the threshold as specified through the configuration property gobblin.yarn.max.get.app.report.failures . The shutdown will trigger an email notification if the configuration property gobblin.yarn.email.notification.on.shutdown is set to true .","title":"Handling Failures to get ApplicationReport"},{"location":"user-guide/Gobblin-on-Yarn/#log-aggregation","text":"Yarn provides both a Web UI and a command-line tool to access the logs of an application, and also does log aggregation so the logs of all the containers become available on the client side upon requested. However, there are a few limitations that make it hard to access the logs of an application at runtime: The command-line utility for downloading the aggregated logs will only be able to do so after the application finishes, making it useless for getting access to the logs at the application runtime. The Web UI does allow logs to be viewed at runtime, but only when the user that access the UI is the same as the user that launches the application. On a Yarn cluster where security is enabled, the user launching the Gobblin Yarn application is typically a user of some headless account. Because Gobblin runs on Yarn as a long-running native Yarn application, getting access to the logs at runtime is critical to know what's going on in the application and to detect any issues in the application as early as possible. Unfortunately we cannot use the log facility provided by Yarn here due to the above limitations. Alternatively, Gobblin on Yarn has its own mechanism for doing log aggregation and providing access to the logs at runtime, described as follows. Both the Gobblin ApplicationMaster and WorkUnitRunner run a LogCopier that periodically copies new entries of both stdout and stderr logs of the corresponding processes from the containers to a central location on HDFS under the directory ${gobblin.yarn.work.dir}/_applogs in the subdirectories named after the container IDs, one per container. The names of the log files on HDFS combine the container IDs and the original log file names so it's easy to tell which container generates which log file. More specifically, the log files produced by the ApplicationMaster are named container id .GobblinApplicationMaster.{stdout,stderr} , and the log files produced by the WorkUnitRunner are named container id .GobblinWorkUnitRunner.{stdout,stderr} . The Gobblin YarnApplicationLauncher also runs a LogCopier that periodically copies new log entries from log files under ${gobblin.yarn.work.dir}/_applogs on HDFS to the local filesystem under the directory configured by the property gobblin.yarn.logs.sink.root.dir . By default, the LogCopier checks for new log entries every 60 seconds and will keep reading new log entries until it reaches the end of the log file. This setup enables the Gobblin Yarn application to stream container process logs near real-time all the way to the client/driver.","title":"Log Aggregation"},{"location":"user-guide/Gobblin-on-Yarn/#security-and-delegation-token-management","text":"On a Yarn cluster with security enabled (e.g., Kerberos authentication is required to access HDFS), security and delegation token management is necessary to allow Gobblin run as a long-running Yarn application. Specifically, Gobblin running on a secured Yarn cluster needs to get its delegation token for accessing HDFS renewed periodically, which also requires periodic keytab re-logins because a delegation token can only be renewed up to a limited number of times in one login. The Gobblin Yarn application supports Kerberos-based authentication and login through a keytab file. The YarnAppSecurityManager running in the Yarn Application Launcher and the YarnContainerSecurityManager running in the ApplicationMaster and WorkUnitRunner work together to get every Yarn containers updated whenever the delegation token gets updated on the client side by the YarnAppSecurityManager . More specifically, the YarnAppSecurityManager periodically logins through the keytab and gets the delegation token refreshed regularly after each successful login. Every time the YarnAppSecurityManager refreshes the delegation token, the YarnContainerSecurityManager writes the new token to a file on HDFS and sends a TOKEN_FILE_UPDATED message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Upon receiving such a message, the YarnContainerSecurityManager running in the ApplicationMaster or WorkUnitRunner gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file. Both the interval between two Kerberos keytab logins and the interval between two delegation token refreshes are configurable, through the configuration properties gobblin.yarn.login.interval.minutes and gobblin.yarn.token.renew.interval.minutes , respectively.","title":"Security and Delegation Token Management"},{"location":"user-guide/Gobblin-on-Yarn/#configuration","text":"","title":"Configuration"},{"location":"user-guide/Gobblin-on-Yarn/#configuration-properties","text":"In additional to the common Gobblin configuration properties, documented in Configuration Properties Glossary , Gobblin on Yarn uses the following configuration properties. Property Default Value Description gobblin.yarn.app.name GobblinYarn The Gobblin Yarn appliation name. gobblin.yarn.app.queue default The Yarn queue the Gobblin Yarn application will run in. gobblin.yarn.work.dir /gobblin The working directory (typically on HDFS) for the Gobblin Yarn application. gobblin.yarn.app.report.interval.minutes 5 The interval in minutes between two Gobblin Yarn application status reports. gobblin.yarn.max.get.app.report.failures 4 Maximum allowed number of consecutive failures to get a Yarn ApplicationReport . gobblin.yarn.email.notification.on.shutdown false Whether email notification is enabled or not on shutdown of the GobblinYarnAppLauncher . If this is set to true , the following configuration properties also need to be set for email notification to work: email.host , email.smtp.port , email.user , email.password , email.from , and email.tos . Refer to Email Alert Properties for more information on those configuration properties. gobblin.yarn.app.master.memory.mbs 512 How much memory in MBs to request for the container running the Gobblin ApplicationMaster. gobblin.yarn.app.master.cores 1 The number of vcores to request for the container running the Gobblin ApplicationMaster. gobblin.yarn.app.master.jars A comma-separated list of jars the Gobblin ApplicationMaster depends on but not in the lib directory. gobblin.yarn.app.master.files.local A comma-separated list of files on the local filesystem the Gobblin ApplicationMaster depends on. gobblin.yarn.app.master.files.remote A comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin ApplicationMaster depends on. gobblin.yarn.app.master.jvm.args Additional JVM arguments for the JVM process running the Gobblin ApplicationMaster, e.g., -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads . gobblin.yarn.initial.containers 1 The number of containers to request initially when the application starts to run the WorkUnitRunner. gobblin.yarn.container.memory.mbs 512 How much memory in MBs to request for the container running the Gobblin WorkUnitRunner. gobblin.yarn.container.cores 1 The number of vcores to request for the container running the Gobblin WorkUnitRunner. gobblin.yarn.container.jars A comma-separated list of jars the Gobblin WorkUnitRunner depends on but not in the lib directory. gobblin.yarn.container.files.local A comma-separated list of files on the local filesystem the Gobblin WorkUnitRunner depends on. gobblin.yarn.container.files.remote A comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin WorkUnitRunner depends on. gobblin.yarn.container.jvm.args Additional JVM arguments for the JVM process running the Gobblin WorkUnitRunner, e.g., -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads . gobblin.yarn.container.affinity.enabled true Whether the same host should be used as the preferred host when requesting a replacement container for the one that exits. gobblin.yarn.helix.cluster.name GobblinYarn The name of the Helix cluster that will be registered with ZooKeeper. gobblin.yarn.zk.connection.string localhost:2181 The ZooKeeper connection string used by Helix. helix.instance.max.retries 2 Maximum number of times the application tries to restart a failed Helix instance (corresponding to a Yarn container). gobblin.yarn.lib.jars.dir The directory where library jars are stored, typically gobblin-dist/lib . gobblin.yarn.job.conf.path The path to either a directory where Gobblin job configuration files are stored or a single job configuration file. Internally Gobblin Yarn will package the configuration files as a tarball so you don't need to. gobblin.yarn.logs.sink.root.dir The directory on local filesystem on the driver/client side where the aggregated container logs of both the ApplicationMaster and WorkUnitRunner are stored. gobblin.yarn.log.copier.max.file.size Unbounded The maximum bytes per log file. When this is exceeded a new log file will be created. gobblin.yarn.log.copier.scheduler ScheduledExecutorService The scheduler to use to copy the log files. Possible values: ScheduledExecutorService , HashedWheelTimer . The HashedWheelTimer scheduler is experimental but is expected to become the default after a sufficient burn in period. gobblin.yarn.keytab.file.path The path to the Kerberos keytab file used for keytab-based authentication/login. gobblin.yarn.keytab.principal.name The principal name of the keytab. gobblin.yarn.login.interval.minutes 1440 The interval in minutes between two keytab logins. gobblin.yarn.token.renew.interval.minutes 720 The interval in minutes between two delegation token renews.","title":"Configuration Properties"},{"location":"user-guide/Gobblin-on-Yarn/#job-lock","text":"It is recommended to use zookeeper for maintaining job locks. See ZookeeperBasedJobLock Properties for the relevant configuration properties.","title":"Job Lock"},{"location":"user-guide/Gobblin-on-Yarn/#configuration-system","text":"The Gobblin Yarn application uses the Typesafe Config library to handle the application configuration. Following Typesafe Config 's model, the Gobblin Yarn application uses a single file named application.conf for all configuration properties and another file named reference.conf for default values. A sample application.conf is shown below: # Yarn/Helix configuration properties gobblin.yarn.helix.cluster.name=GobblinYarnTest gobblin.yarn.app.name=GobblinYarnTest gobblin.yarn.lib.jars.dir= /home/gobblin/gobblin-dist/lib/ gobblin.yarn.app.master.files.local= /home/gobblin/gobblin-dist/conf/log4j-yarn.properties,/home/gobblin/gobblin-dist/conf/application.conf,/home/gobblin/gobblin-dist/conf/reference.conf gobblin.yarn.container.files.local=${gobblin.yarn.app.master.files.local} gobblin.yarn.job.conf.path= /home/gobblin/gobblin-dist/job-conf gobblin.yarn.keytab.file.path= /home/gobblin/gobblin.headless.keytab gobblin.yarn.keytab.principal.name=gobblin gobblin.yarn.app.master.jvm.args= -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m gobblin.yarn.container.jvm.args= -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m gobblin.yarn.logs.sink.root.dir=/home/gobblin/gobblin-dist/applogs # File system URIs writer.fs.uri=${fs.uri} state.store.fs.uri=${fs.uri} # Writer related configuration properties writer.destination.type=HDFS writer.output.format=AVRO writer.staging.dir=${gobblin.yarn.work.dir}/task-staging writer.output.dir=${gobblin.yarn.work.dir}/task-output # Data publisher related configuration properties data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher data.publisher.final.dir=${gobblin.yarn.work.dir}/job-output data.publisher.replace.final.dir=false # Directory where job/task state files are stored state.store.dir=${gobblin.yarn.work.dir}/state-store # Directory where error files from the quality checkers are stored qualitychecker.row.err.file=${gobblin.yarn.work.dir}/err # Use zookeeper for maintaining the job lock job.lock.enabled=true job.lock.type=ZookeeperBasedJobLock # Directory where job locks are stored job.lock.dir=${gobblin.yarn.work.dir}/locks # Directory where metrics log files are stored metrics.log.dir=${gobblin.yarn.work.dir}/metrics A sample reference.conf is shown below: # Yarn/Helix configuration properties gobblin.yarn.app.queue=default gobblin.yarn.helix.cluster.name=GobblinYarn gobblin.yarn.app.name=GobblinYarn gobblin.yarn.app.master.memory.mbs=512 gobblin.yarn.app.master.cores=1 gobblin.yarn.app.report.interval.minutes=5 gobblin.yarn.max.get.app.report.failures=4 gobblin.yarn.email.notification.on.shutdown=false gobblin.yarn.initial.containers=1 gobblin.yarn.container.memory.mbs=512 gobblin.yarn.container.cores=1 gobblin.yarn.container.affinity.enabled=true gobblin.yarn.helix.instance.max.retries=2 gobblin.yarn.keytab.login.interval.minutes=1440 gobblin.yarn.token.renew.interval.minutes=720 gobblin.yarn.work.dir=/user/gobblin/gobblin-yarn gobblin.yarn.zk.connection.string=${zookeeper.connection.string} fs.uri= hdfs://localhost:9000 zookeeper.connection.string= localhost:2181","title":"Configuration System"},{"location":"user-guide/Gobblin-on-Yarn/#deployment","text":"A standard deployment of Gobblin on Yarn requires a Yarn cluster running Hadoop 2.x ( 2.3.0 and above recommended) and a ZooKeeper cluster. Make sure the client machine (typically the gateway of the Yarn cluster) is able to access the ZooKeeper instance.","title":"Deployment"},{"location":"user-guide/Gobblin-on-Yarn/#deployment-on-a-unsecured-yarn-cluster","text":"To do a deployment of the Gobblin Yarn application, first build Gobblin using the following command from the root directory of the Gobblin project. ./gradlew clean build To build Gobblin against a specific version of Hadoop 2.x, e.g., 2.7.0 , run the following command instead: ./gradlew clean build -PhadoopVersion=2.7.0 After Gobblin is successfully built, a tarball named gobblin-dist-[project-version].tar.gz should have been created under the root directory of the project. To deploy the Gobblin Yarn application on a unsecured Yarn cluster, uncompress the tarball somewhere and run the following commands: cd gobblin-dist bin/gobblin-yarn.sh Note that for the above commands to work, the Hadoop/Yarn configuration directory must be on the classpath and the configuration must be pointing to the right Yarn cluster, or specifically the right ResourceManager and NameNode URLs. This is defined like the following in gobblin-yarn.sh : CLASSPATH=${FWDIR_CONF}:${GOBBLIN_JARS}:${YARN_CONF_DIR}:${HADOOP_YARN_HOME}/lib","title":"Deployment on a Unsecured Yarn Cluster"},{"location":"user-guide/Gobblin-on-Yarn/#deployment-on-a-secured-yarn-cluster","text":"When deploying the Gobblin Yarn application on a secured Yarn cluster, make sure the keytab file path is correctly specified in application.conf and the correct principal for the keytab is used as follows. The rest of the deployment is the same as that on a unsecured Yarn cluster. gobblin.yarn.keytab.file.path= /home/gobblin/gobblin.headless.keytab gobblin.yarn.keytab.principal.name=gobblin","title":"Deployment on a Secured Yarn Cluster"},{"location":"user-guide/Gobblin-on-Yarn/#supporting-existing-gobblin-jobs","text":"Gobblin on Yarn is backward compatible and supports existing Gobblin jobs running in the standalone and MR modes. To run existing Gobblin jobs, simply put the job configuration files into a directory on the local file system of the driver and setting the configuration property gobblin.yarn.job.conf.path to point to the directory. When the Gobblin Yarn application starts, Yarn will package the configuration files as a tarball and make sure the tarball gets copied to the ApplicationMaster and properly uncompressed. The GobblinHelixJobScheduler then loads the job configuration files and schedule the jobs to run.","title":"Supporting Existing Gobblin Jobs"},{"location":"user-guide/Gobblin-on-Yarn/#monitoring","text":"Gobblin Yarn uses the Gobblin Metrics library for collecting and reporting metrics at the container, job, and task levels. Each GobblinWorkUnitRunner maintains a ContainerMetrics that is the parent of the JobMetrics of each job run the container is involved, which is the parent of the TaskMetrics of each task of the job run. This hierarchical structure allows us to do pre-aggregation in the containers before reporting the metrics to the backend. Collected metrics can be reported to various sinks such as Kafka, files, and JMX, depending on the configuration. Specifically, metrics.enabled controls whether metrics collecting and reporting are enabled or not. metrics.reporting.kafka.enabled , metrics.reporting.file.enabled , and metrics.reporting.jmx.enabled control whether collected metrics should be reported or not to Kafka, files, and JMX, respectively. Please refer to Metrics Properties for the available configuration properties related to metrics collecting and reporting. In addition to metric collecting and reporting, Gobblin Yarn also supports writing job execution information to a MySQL-backed job execution history store, which keeps track of job execution information. Please refer to the DDL for the relevant MySQL tables. Detailed information on the job execution history store including how to configure it can be found here .","title":"Monitoring"},{"location":"user-guide/Gobblin-template/","text":"Table of Contents Table of Contents Overview How to Use Templates Available Templates How to Create Your Own Template How does Template Work in Gobblin Overview The job configuration template is implemented for saving efforts of Gobblin users. For a specific type of job, e.g. Gobblin-Kafka data pulling, there exists quite amount of repetitive options to fill in. We are aiming at moving those repetitive options into a template for specific type of job, only exposing some essential configurable options for user to specify. This does not sacrifice flexibility, users can still specify options that already exist in the template to override the default value. Here is the .pull file for wikipedia example with template support: job.template=templates/wikiSample.template source.page.titles=NASA,LinkedIn,Parris_Cues,Barbara_Corcoran How to Use Templates Users need only submit the .pull file above to the specified directory as described in wikipedia example. Although there are far fewer options there are still some mandatory options to specify in .pull file. In general, to use a template: - Specify which template to use in the key job.template . - All the keys specified in gobblin.template.required_attributes must be provided. - As mentioned before, user can also specify existing options in template to override the default value. Available Templates wikiSample.template gobblin-kafka.template Templates above are available on Github repo. How to Create Your Own Template To create a template, simply create a file with all the common configurations for that template (recommended to use .template extension). Place this file into Gobblin's classpath, and set job.template to the path to that file in the classpath. For reference, this is how the Wikipedia template looks: job.name=PullFromWikipedia job.group=Wikipedia job.description=A getting started example for Gobblin source.class=org.apache.gobblin.example.wikipedia.WikipediaSource source.revisions.cnt=5 wikipedia.api.rooturl=https://en.wikipedia.org/w/api.php?format=json action=query prop=revisions rvprop=content|timestamp|user|userid|size wikipedia.avro.schema={ namespace : example.wikipedia.avro , type : record , name : WikipediaArticle , fields : [{ name : pageid , type : [ double , null ]},{ name : title , type : [ string , null ]},{ name : user , type : [ string , null ]},{ name : anon , type : [ string , null ]},{ name : userid , type : [ double , null ]},{ name : timestamp , type : [ string , null ]},{ name : size , type : [ double , null ]},{ name : contentformat , type : [ string , null ]},{ name : contentmodel , type : [ string , null ]},{ name : content , type : [ string , null ]}]} converter.classes=org.apache.gobblin.example.wikipedia.WikipediaConverter extract.namespace=org.apache.gobblin.example.wikipedia writer.destination.type=HDFS writer.output.format=AVRO writer.partitioner.class=org.apache.gobblin.example.wikipedia.WikipediaPartitioner data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher gobblin.template.required_attributes=source.page.titles How does Template Work in Gobblin Currently Gobblin stores and loads existing templates as resources in the classpath. Gobblin will then resolve this template with the user-specified .pull file. Note that there is an option in template named gobblin.template.required_attributes which lists all options that are required for users to fill in. If any of options in the required list is absent, the configuration will be detected as invalid by Gobblin throw an runtime excpetion accordingly. Gobblin provides methods to retrieve all options inside .template file and resolved configuration option list. These interactive funtions will be integrated soon.","title":"Template"},{"location":"user-guide/Gobblin-template/#table-of-contents","text":"Table of Contents Overview How to Use Templates Available Templates How to Create Your Own Template How does Template Work in Gobblin","title":"Table of Contents"},{"location":"user-guide/Gobblin-template/#overview","text":"The job configuration template is implemented for saving efforts of Gobblin users. For a specific type of job, e.g. Gobblin-Kafka data pulling, there exists quite amount of repetitive options to fill in. We are aiming at moving those repetitive options into a template for specific type of job, only exposing some essential configurable options for user to specify. This does not sacrifice flexibility, users can still specify options that already exist in the template to override the default value. Here is the .pull file for wikipedia example with template support: job.template=templates/wikiSample.template source.page.titles=NASA,LinkedIn,Parris_Cues,Barbara_Corcoran","title":"Overview"},{"location":"user-guide/Gobblin-template/#how-to-use-templates","text":"Users need only submit the .pull file above to the specified directory as described in wikipedia example. Although there are far fewer options there are still some mandatory options to specify in .pull file. In general, to use a template: - Specify which template to use in the key job.template . - All the keys specified in gobblin.template.required_attributes must be provided. - As mentioned before, user can also specify existing options in template to override the default value.","title":"How to Use Templates"},{"location":"user-guide/Gobblin-template/#available-templates","text":"wikiSample.template gobblin-kafka.template Templates above are available on Github repo.","title":"Available Templates"},{"location":"user-guide/Gobblin-template/#how-to-create-your-own-template","text":"To create a template, simply create a file with all the common configurations for that template (recommended to use .template extension). Place this file into Gobblin's classpath, and set job.template to the path to that file in the classpath. For reference, this is how the Wikipedia template looks: job.name=PullFromWikipedia job.group=Wikipedia job.description=A getting started example for Gobblin source.class=org.apache.gobblin.example.wikipedia.WikipediaSource source.revisions.cnt=5 wikipedia.api.rooturl=https://en.wikipedia.org/w/api.php?format=json action=query prop=revisions rvprop=content|timestamp|user|userid|size wikipedia.avro.schema={ namespace : example.wikipedia.avro , type : record , name : WikipediaArticle , fields : [{ name : pageid , type : [ double , null ]},{ name : title , type : [ string , null ]},{ name : user , type : [ string , null ]},{ name : anon , type : [ string , null ]},{ name : userid , type : [ double , null ]},{ name : timestamp , type : [ string , null ]},{ name : size , type : [ double , null ]},{ name : contentformat , type : [ string , null ]},{ name : contentmodel , type : [ string , null ]},{ name : content , type : [ string , null ]}]} converter.classes=org.apache.gobblin.example.wikipedia.WikipediaConverter extract.namespace=org.apache.gobblin.example.wikipedia writer.destination.type=HDFS writer.output.format=AVRO writer.partitioner.class=org.apache.gobblin.example.wikipedia.WikipediaPartitioner data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher gobblin.template.required_attributes=source.page.titles","title":"How to Create Your Own Template"},{"location":"user-guide/Gobblin-template/#how-does-template-work-in-gobblin","text":"Currently Gobblin stores and loads existing templates as resources in the classpath. Gobblin will then resolve this template with the user-specified .pull file. Note that there is an option in template named gobblin.template.required_attributes which lists all options that are required for users to fill in. If any of options in the required list is absent, the configuration will be detected as invalid by Gobblin throw an runtime excpetion accordingly. Gobblin provides methods to retrieve all options inside .template file and resolved configuration option list. These interactive funtions will be integrated soon.","title":"How does Template Work in Gobblin"},{"location":"user-guide/Hive-Registration/","text":"Table of Contents Table of Contents How Hive Registration Works in Gobblin HiveSpec HiveRegistrationPolicy HiveSerDeManager Predicate and Activity How to Use Hive Registration in Your Gobblin Job Hive Registration Config Properties Gobblin has the ability to register the ingested/compacted data in Hive. This allows registering data in Hive immediately after data is published at the destination, offering much lower latency compared to doing data ingestion and Hive registration separately. How Hive Registration Works in Gobblin Hive registration is done in HiveRegister . After the data is published, the publisher or compaction runner will create an instance of HiveRegister , and for each path that should be registered in Hive, the publisher or compaction runner will use a specific HiveRegistrationPolicy to create a list of HiveSpec s for the path. It creates a list of HiveSpec s rather than a single HiveSpec for each path, so that the same path can be registered in multiple tables or databases. HiveSpec A HiveSpec specifies how a path should be registered in Hive, i.e., which database, which table, which partition should the path be registered. An example is SimpleHiveSpec . HiveRegistrationPolicy HiveRegistrationPolicy is responsible for generating HiveSpec s given a path. For example, if you want paths ending with a date (e.g., /(something)/2016/05/22 ) to be registered in the corresponding daily partition (e.g., daily-2016-05-22 ), you can create an implementation of HiveRegistrationPolicy that contains the logic of converting such a path into a Hive partition. An example is HiveRegistrationPolicyBase , which provides base implementation for getting database names and table names for a path: A database/table name can be specified explicitly in hive.database.name or hive.table.name . Alternatively, a database/table regex can be provided in hive.database.regex or hive.table.regex . The regex will be matched against the path to be registered, and if they match, the first group is considered the database/table name. It is possible to register a path to multiple databases or tables by specifying additional.hive.database.names and additional.hive.table.names . If multiple databases and tables are specified, the path will be registered to the cross product. If the provided/derived Hive database/table names are invalid, they are sanitized into a valid name. A database/table name is valid if it starts with an alphanumeric character, contains only alphanumeric characters and _ , and is not composed of numbers only. One should in general extend HiveRegistrationPolicyBase when implementing a new HiveRegistrationPolicy . HiveSerDeManager If the data to be registered is in a format other than plain text (CSV, TSV, etc.), you often need to use a SerDe and specify some SerDe properties including the type of SerDe, input format, output format, schema, etc. This is done in HiveSerDeManager , which is part of a HiveRegistrationUnit (i.e., HiveTable or HivePartition ). An example is HiveAvroSerDeManager . Predicate and Activity One or more Predicate s can be attached to a HiveSpec . If a HiveSpec contains Predicate s, unless Predicate s return true , the HiveSpec will not be registered. This is useful in cases where, for example, one only wants to register a path in Hive if a particular Hive table or partition doesn't already exist. An example is TableNotExistPredicate . One or more Activity s can be attached to a HiveSpec . There are two types of activities: pre-activities and post-activities, which will be executed before and after a HiveSpec is registered, respectively. This is useful, for example, when you need to drop/alter a table/partition before or after a path is registered. An example is DropTableActivity . How to Use Hive Registration in Your Gobblin Job First, is to implement a HiveRegistrationPolicy (or reuse an existing one), then specify its class name in config property hive.registration.policy . Then, specify the appropriate table/partition properties in hive.table.partition.props , storage descriptor properties in hive.storage.props , and SerDe properties in hive.serde.props . Some SerDe properties are usually dynamic (e.g., schema), which are added in the HiveSerDeManager . Example table/partition properties are \"owner\" and \"retention\", example storage descriptor properties are \"location\", \"compressed\", \"numBuckets\", example SerDe properties are \"serializationLib\", \"avro.schema.url\". If you are running a Gobblin ingestion job: If data is published in the job (which is the default case), use a job-level data publisher that can perform Hive registration, such as BaseDataPublisherWithHiveRegistration . If you need to do Hive registration with a different publisher than BaseDataPublisher , you will need to extend that publisher to do Hive registration, which will be similar as how BaseDataPublisher is extended into BaseDataPublisherWithHiveRegistration . If data is published in the tasks, use HiveRegistrationPublisher as the job-level data publisher. This publisher does not publish any data; it only does Hive registration. If you are running a Gobblin compaction job: add HiveRegistrationCompactorListener to the list of compaction listeners by adding the class name to property compaction.listeners . Hive Registration Config Properties Property Name Semantics hive.registration.policy Class name which implements HiveRegistrationPolicy hive.row.format Either AVRO , or the class name which implements HiveSerDeManager hive.database.name Hive database name hive.database.regex Hive database regex hive.database.name.prefix Hive database name prefix hive.database.name.suffix Hive database name suffix additional.hive.database.names Additional Hive database names hive.table.name Hive table name hive.table.regex Hive table regex hive.table.name.prefix Hive table name prefix hive.table.name.suffix Hive table name suffix additional.hive.table.names Additional Hive table names hive.register.threads Thread pool size used for Hive registration hive.db.root.dir The root dir of Hive db hive.table.partition.props Table/partition properties hive.storage.props Storage descriptor properties hive.serde.props SerDe properties hive.registration.fs.uri File system URI for Hive registration hive.upstream.data.attr.names Attributes to describe upstream data source as Hive Metadata","title":"Hive Registration"},{"location":"user-guide/Hive-Registration/#table-of-contents","text":"Table of Contents How Hive Registration Works in Gobblin HiveSpec HiveRegistrationPolicy HiveSerDeManager Predicate and Activity How to Use Hive Registration in Your Gobblin Job Hive Registration Config Properties Gobblin has the ability to register the ingested/compacted data in Hive. This allows registering data in Hive immediately after data is published at the destination, offering much lower latency compared to doing data ingestion and Hive registration separately.","title":"Table of Contents"},{"location":"user-guide/Hive-Registration/#how-hive-registration-works-in-gobblin","text":"Hive registration is done in HiveRegister . After the data is published, the publisher or compaction runner will create an instance of HiveRegister , and for each path that should be registered in Hive, the publisher or compaction runner will use a specific HiveRegistrationPolicy to create a list of HiveSpec s for the path. It creates a list of HiveSpec s rather than a single HiveSpec for each path, so that the same path can be registered in multiple tables or databases.","title":"How Hive Registration Works in Gobblin"},{"location":"user-guide/Hive-Registration/#hivespec","text":"A HiveSpec specifies how a path should be registered in Hive, i.e., which database, which table, which partition should the path be registered. An example is SimpleHiveSpec .","title":"HiveSpec"},{"location":"user-guide/Hive-Registration/#hiveregistrationpolicy","text":"HiveRegistrationPolicy is responsible for generating HiveSpec s given a path. For example, if you want paths ending with a date (e.g., /(something)/2016/05/22 ) to be registered in the corresponding daily partition (e.g., daily-2016-05-22 ), you can create an implementation of HiveRegistrationPolicy that contains the logic of converting such a path into a Hive partition. An example is HiveRegistrationPolicyBase , which provides base implementation for getting database names and table names for a path: A database/table name can be specified explicitly in hive.database.name or hive.table.name . Alternatively, a database/table regex can be provided in hive.database.regex or hive.table.regex . The regex will be matched against the path to be registered, and if they match, the first group is considered the database/table name. It is possible to register a path to multiple databases or tables by specifying additional.hive.database.names and additional.hive.table.names . If multiple databases and tables are specified, the path will be registered to the cross product. If the provided/derived Hive database/table names are invalid, they are sanitized into a valid name. A database/table name is valid if it starts with an alphanumeric character, contains only alphanumeric characters and _ , and is not composed of numbers only. One should in general extend HiveRegistrationPolicyBase when implementing a new HiveRegistrationPolicy .","title":"HiveRegistrationPolicy"},{"location":"user-guide/Hive-Registration/#hiveserdemanager","text":"If the data to be registered is in a format other than plain text (CSV, TSV, etc.), you often need to use a SerDe and specify some SerDe properties including the type of SerDe, input format, output format, schema, etc. This is done in HiveSerDeManager , which is part of a HiveRegistrationUnit (i.e., HiveTable or HivePartition ). An example is HiveAvroSerDeManager .","title":"HiveSerDeManager"},{"location":"user-guide/Hive-Registration/#predicate-and-activity","text":"One or more Predicate s can be attached to a HiveSpec . If a HiveSpec contains Predicate s, unless Predicate s return true , the HiveSpec will not be registered. This is useful in cases where, for example, one only wants to register a path in Hive if a particular Hive table or partition doesn't already exist. An example is TableNotExistPredicate . One or more Activity s can be attached to a HiveSpec . There are two types of activities: pre-activities and post-activities, which will be executed before and after a HiveSpec is registered, respectively. This is useful, for example, when you need to drop/alter a table/partition before or after a path is registered. An example is DropTableActivity .","title":"Predicate and Activity"},{"location":"user-guide/Hive-Registration/#how-to-use-hive-registration-in-your-gobblin-job","text":"First, is to implement a HiveRegistrationPolicy (or reuse an existing one), then specify its class name in config property hive.registration.policy . Then, specify the appropriate table/partition properties in hive.table.partition.props , storage descriptor properties in hive.storage.props , and SerDe properties in hive.serde.props . Some SerDe properties are usually dynamic (e.g., schema), which are added in the HiveSerDeManager . Example table/partition properties are \"owner\" and \"retention\", example storage descriptor properties are \"location\", \"compressed\", \"numBuckets\", example SerDe properties are \"serializationLib\", \"avro.schema.url\". If you are running a Gobblin ingestion job: If data is published in the job (which is the default case), use a job-level data publisher that can perform Hive registration, such as BaseDataPublisherWithHiveRegistration . If you need to do Hive registration with a different publisher than BaseDataPublisher , you will need to extend that publisher to do Hive registration, which will be similar as how BaseDataPublisher is extended into BaseDataPublisherWithHiveRegistration . If data is published in the tasks, use HiveRegistrationPublisher as the job-level data publisher. This publisher does not publish any data; it only does Hive registration. If you are running a Gobblin compaction job: add HiveRegistrationCompactorListener to the list of compaction listeners by adding the class name to property compaction.listeners .","title":"How to Use Hive Registration in Your Gobblin Job"},{"location":"user-guide/Hive-Registration/#hive-registration-config-properties","text":"Property Name Semantics hive.registration.policy Class name which implements HiveRegistrationPolicy hive.row.format Either AVRO , or the class name which implements HiveSerDeManager hive.database.name Hive database name hive.database.regex Hive database regex hive.database.name.prefix Hive database name prefix hive.database.name.suffix Hive database name suffix additional.hive.database.names Additional Hive database names hive.table.name Hive table name hive.table.regex Hive table regex hive.table.name.prefix Hive table name prefix hive.table.name.suffix Hive table name suffix additional.hive.table.names Additional Hive table names hive.register.threads Thread pool size used for Hive registration hive.db.root.dir The root dir of Hive db hive.table.partition.props Table/partition properties hive.storage.props Storage descriptor properties hive.serde.props SerDe properties hive.registration.fs.uri File system URI for Hive registration hive.upstream.data.attr.names Attributes to describe upstream data source as Hive Metadata","title":"Hive Registration Config Properties"},{"location":"user-guide/Job-Execution-History-Store/","text":"Table of Contents Table of Contents Overview Information Recorded Job Execution Information Task Execution Information Default Implementation Rest Query API Example Queries Job Execution History Server Overview Gobblin provides the users a way of keeping tracking of executions of their jobs through the Job Execution History Store, which can be queried either directly if the implementation supports queries directly or through a Rest API. Note that using the Rest API needs the Job Execution History Server to be up and running. The Job Execution History Server will be discussed later. By default, writing to the Job Execution History Store is not enabled. To enable it, set configuration property job.history.store.enabled to true . Information Recorded The Job Execution History Store stores various pieces of information of a job execution, including both job-level and task-level stats and measurements that are summarized below. Job Execution Information The following table summarizes job-level execution information the Job Execution History Store stores. Information Description Job name Gobblin job name. Job ID Gobblin job ID. Start time Start time in epoch time (of unit milliseconds) of the job in the local time zone. End time End time in epoch time (of unit milliseconds) of the job in the local time zone. Duration Duration of the job in milliseconds. Job state Running state of the job. Possible values are PENDING , RUNNING , SUCCESSFUL , COMMITTED , FAILED , CANCELLED . Launched tasks Number of launched tasks of the job. Completed tasks Number of tasks of the job that completed. Launcher type The type of the launcher used to launch and run the task. Job tracking URL This will be set to the MapReduce job URL if the Gobblin job is running on Hadoop MapReduce. This may also be set to the Azkaban job execution tracking URL if the job is running through Azkaban but not on Hadoop MapReduce. Otherwise, this will be empty. Job-level metrics Values of job-level metrics. Note that this data is not time-series based so the values will be overwritten on every update. Job configuration properties Job configuration properties used at runtime for job execution. Note that it may include changes made at runtime by the job. Task Execution Information The following table summarizes task-level execution information the Job Execution History Store stores. Information Description Task ID Gobblin task ID. Job ID Gobblin job ID. Start time Start time in epoch time (of unit milliseconds) of the task in the local time zone. End time End time in epoch time (of unit milliseconds) of the task in the local time zone. Duration Duration of the task in milliseconds. Task state Running state of the task. Possible values are PENDING , RUNNING , SUCCESSFUL , COMMITTED , FAILED , CANCELLED . Task failure exception Exception message in case of task failure. Low watermark The low watermark of the task if avaialble. High watermark The high watermark of the task if available. Extract namespace The namespace of the Extract . An Extract is a concept describing the ingestion work of a job. This stores the value specified through the configuration property extract.namespace . Extract name The name of the Extract . This stores the value specified through the configuration property extract.table.name . Extract type The type of the Extract . This stores the value specified through the configuration property extract.table.type . Task-level metrics Values of task-level metrics. Note that this data is not time-series based so the values will be overwritten on every update. Task configuration properties Task configuration properties used at runtime for task execution. Note that it may include changes made at runtime by the task. Default Implementation The default implementation of the Job Execution History Store stores job execution information into a MySQL database in a few different tables. Specifically, the following tables are used and should be created before writing to the store is enabled. Checkout the MySQL DDLs of the tables for detailed columns of each table. Table gobblin_job_executions stores basic information about a job execution including the start and end times, job running state, number of launched and completed tasks, etc. Table gobblin_task_executions stores basic information on task executions of a job, including the start and end times, task running state, task failure message if any, etc, of each task. Table gobblin_job_metrics stores values of job-level metrics collected through the JobMetrics class. Note that this data is not time-series based and values of metrics are overwritten on every update to the job execution information. Table gobblin_task_metrics stores values of task-level metrics collected through the TaskMetrics class. Again, this data is not time-series based and values of metrics are overwritten on updates. Table gobblin_job_properties stores the job configuration properties used at runtime for the job execution, which may include changes made at runtime by the job. Table gobblin_task_properties stores the task configuration properties used at runtime for task executions, which also may include changes made at runtime by the tasks. To enable writing to the MySQL-backed Job Execution History Store, the following configuration properties (with sample values) need to be set: job.history.store.url=jdbc:mysql://localhost/gobblin job.history.store.jdbc.driver=com.mysql.jdbc.Driver job.history.store.user=gobblin job.history.store.password=gobblin Rest Query API The Job Execution History Store Rest API supports three types of queries: query by job name, query by job ID, or query by extract name. The query type can be specified using the field idType in the query json object and can have one of the values JOB_NAME , JOB_ID , or TABLE . All three query types require the field id in the query json object, which should have a proper value as documented in the following table. Query type Query ID JOB_NAME Gobblin job name. JOB_ID Gobblin job ID. TABLE A json object following the TABLE schema shown below. { type : record , name : Table , namespace : gobblin.rest , doc : Gobblin table definition , fields : [ { name : namespace , type : string , optional : true, doc : Table namespace }, { name : name , type : string , doc : Table name }, { name : type , type : { name : TableTypeEnum , type : enum , symbols : [ SNAPSHOT_ONLY , SNAPSHOT_APPEND , APPEND_ONLY ] }, optional : true, doc : Table type } ] } For each query type, there are also some option fields that can be used to control the number of records returned and what should be included in the query result. The optional fields are summarized in the following table. Optional field Type Description limit int Limit on the number of records returned. timeRange TimeRange The query time range. The schema of TimeRange is shown below. jobProperties boolean This controls whether the returned record should include the job configuration properties. taskProperties boolean This controls whether the returned record should include the task configuration properties. { type : record , name : TimeRange , namespace : gobblin.rest , doc : Query time range , fields : [ { name : startTime , type : string , optional : true, doc : Start time of the query range }, { name : endTime , type : string , optional : true, doc : End time of the query range }, { name : timeFormat , type : string , doc : Date/time format used to parse the start time and end time } ] } The API is built with rest.li , which generates documentation on compilation and can be found at http:// hostname:port /restli/docs . Example Queries Fetch the 10 most recent job executions with a job name TestJobName curl http:// hostname:port /jobExecutions/idType=JOB_NAME id.string=TestJobName limit=10 Job Execution History Server The Job Execution History Server is a Rest server for serving queries on the Job Execution History Store through the Rest API described above. The Rest endpoint URL is configurable through the following configuration properties (with their default values): rest.server.host=localhost rest.server.port=8080 Note: This server is started in the standalone deployment if configuration property job.execinfo.server.enabled is set to true .","title":"Job Execution History Store"},{"location":"user-guide/Job-Execution-History-Store/#table-of-contents","text":"Table of Contents Overview Information Recorded Job Execution Information Task Execution Information Default Implementation Rest Query API Example Queries Job Execution History Server","title":"Table of Contents"},{"location":"user-guide/Job-Execution-History-Store/#overview","text":"Gobblin provides the users a way of keeping tracking of executions of their jobs through the Job Execution History Store, which can be queried either directly if the implementation supports queries directly or through a Rest API. Note that using the Rest API needs the Job Execution History Server to be up and running. The Job Execution History Server will be discussed later. By default, writing to the Job Execution History Store is not enabled. To enable it, set configuration property job.history.store.enabled to true .","title":"Overview"},{"location":"user-guide/Job-Execution-History-Store/#information-recorded","text":"The Job Execution History Store stores various pieces of information of a job execution, including both job-level and task-level stats and measurements that are summarized below.","title":"Information Recorded"},{"location":"user-guide/Job-Execution-History-Store/#job-execution-information","text":"The following table summarizes job-level execution information the Job Execution History Store stores. Information Description Job name Gobblin job name. Job ID Gobblin job ID. Start time Start time in epoch time (of unit milliseconds) of the job in the local time zone. End time End time in epoch time (of unit milliseconds) of the job in the local time zone. Duration Duration of the job in milliseconds. Job state Running state of the job. Possible values are PENDING , RUNNING , SUCCESSFUL , COMMITTED , FAILED , CANCELLED . Launched tasks Number of launched tasks of the job. Completed tasks Number of tasks of the job that completed. Launcher type The type of the launcher used to launch and run the task. Job tracking URL This will be set to the MapReduce job URL if the Gobblin job is running on Hadoop MapReduce. This may also be set to the Azkaban job execution tracking URL if the job is running through Azkaban but not on Hadoop MapReduce. Otherwise, this will be empty. Job-level metrics Values of job-level metrics. Note that this data is not time-series based so the values will be overwritten on every update. Job configuration properties Job configuration properties used at runtime for job execution. Note that it may include changes made at runtime by the job.","title":"Job Execution Information"},{"location":"user-guide/Job-Execution-History-Store/#task-execution-information","text":"The following table summarizes task-level execution information the Job Execution History Store stores. Information Description Task ID Gobblin task ID. Job ID Gobblin job ID. Start time Start time in epoch time (of unit milliseconds) of the task in the local time zone. End time End time in epoch time (of unit milliseconds) of the task in the local time zone. Duration Duration of the task in milliseconds. Task state Running state of the task. Possible values are PENDING , RUNNING , SUCCESSFUL , COMMITTED , FAILED , CANCELLED . Task failure exception Exception message in case of task failure. Low watermark The low watermark of the task if avaialble. High watermark The high watermark of the task if available. Extract namespace The namespace of the Extract . An Extract is a concept describing the ingestion work of a job. This stores the value specified through the configuration property extract.namespace . Extract name The name of the Extract . This stores the value specified through the configuration property extract.table.name . Extract type The type of the Extract . This stores the value specified through the configuration property extract.table.type . Task-level metrics Values of task-level metrics. Note that this data is not time-series based so the values will be overwritten on every update. Task configuration properties Task configuration properties used at runtime for task execution. Note that it may include changes made at runtime by the task.","title":"Task Execution Information"},{"location":"user-guide/Job-Execution-History-Store/#default-implementation","text":"The default implementation of the Job Execution History Store stores job execution information into a MySQL database in a few different tables. Specifically, the following tables are used and should be created before writing to the store is enabled. Checkout the MySQL DDLs of the tables for detailed columns of each table. Table gobblin_job_executions stores basic information about a job execution including the start and end times, job running state, number of launched and completed tasks, etc. Table gobblin_task_executions stores basic information on task executions of a job, including the start and end times, task running state, task failure message if any, etc, of each task. Table gobblin_job_metrics stores values of job-level metrics collected through the JobMetrics class. Note that this data is not time-series based and values of metrics are overwritten on every update to the job execution information. Table gobblin_task_metrics stores values of task-level metrics collected through the TaskMetrics class. Again, this data is not time-series based and values of metrics are overwritten on updates. Table gobblin_job_properties stores the job configuration properties used at runtime for the job execution, which may include changes made at runtime by the job. Table gobblin_task_properties stores the task configuration properties used at runtime for task executions, which also may include changes made at runtime by the tasks. To enable writing to the MySQL-backed Job Execution History Store, the following configuration properties (with sample values) need to be set: job.history.store.url=jdbc:mysql://localhost/gobblin job.history.store.jdbc.driver=com.mysql.jdbc.Driver job.history.store.user=gobblin job.history.store.password=gobblin","title":"Default Implementation"},{"location":"user-guide/Job-Execution-History-Store/#rest-query-api","text":"The Job Execution History Store Rest API supports three types of queries: query by job name, query by job ID, or query by extract name. The query type can be specified using the field idType in the query json object and can have one of the values JOB_NAME , JOB_ID , or TABLE . All three query types require the field id in the query json object, which should have a proper value as documented in the following table. Query type Query ID JOB_NAME Gobblin job name. JOB_ID Gobblin job ID. TABLE A json object following the TABLE schema shown below. { type : record , name : Table , namespace : gobblin.rest , doc : Gobblin table definition , fields : [ { name : namespace , type : string , optional : true, doc : Table namespace }, { name : name , type : string , doc : Table name }, { name : type , type : { name : TableTypeEnum , type : enum , symbols : [ SNAPSHOT_ONLY , SNAPSHOT_APPEND , APPEND_ONLY ] }, optional : true, doc : Table type } ] } For each query type, there are also some option fields that can be used to control the number of records returned and what should be included in the query result. The optional fields are summarized in the following table. Optional field Type Description limit int Limit on the number of records returned. timeRange TimeRange The query time range. The schema of TimeRange is shown below. jobProperties boolean This controls whether the returned record should include the job configuration properties. taskProperties boolean This controls whether the returned record should include the task configuration properties. { type : record , name : TimeRange , namespace : gobblin.rest , doc : Query time range , fields : [ { name : startTime , type : string , optional : true, doc : Start time of the query range }, { name : endTime , type : string , optional : true, doc : End time of the query range }, { name : timeFormat , type : string , doc : Date/time format used to parse the start time and end time } ] } The API is built with rest.li , which generates documentation on compilation and can be found at http:// hostname:port /restli/docs .","title":"Rest Query API"},{"location":"user-guide/Job-Execution-History-Store/#example-queries","text":"Fetch the 10 most recent job executions with a job name TestJobName curl http:// hostname:port /jobExecutions/idType=JOB_NAME id.string=TestJobName limit=10","title":"Example Queries"},{"location":"user-guide/Job-Execution-History-Store/#job-execution-history-server","text":"The Job Execution History Server is a Rest server for serving queries on the Job Execution History Store through the Rest API described above. The Rest endpoint URL is configurable through the following configuration properties (with their default values): rest.server.host=localhost rest.server.port=8080 Note: This server is started in the standalone deployment if configuration property job.execinfo.server.enabled is set to true .","title":"Job Execution History Server"},{"location":"user-guide/Monitoring/","text":"Table of Contents Table of Contents Overview Metrics Collecting and Reporting Metrics Reporting Metrics collection JVM Metrics Pre-defined Job Execution Metrics Job Execution History Store Email Notifications Overview As a framework for ingesting potentially huge volume of data from many different sources, it's critical to monitor the health and status of the system and job executions. Gobblin employs a variety of approaches introduced below for this purpose. All the approaches are optional and can be configured to be turned on and off in different combinations through the framework and job configurations. Metrics Collecting and Reporting Metrics Reporting Out-of-the-box, Gobblin reports metrics though: JMX : used in the standalone deployment. Metrics reported to JMX can be checked using using tools such as VisualVM or JConsole. Metric log files : Files are stored in a root directory defined by the property metrics.log.dir . Each Gobblin job has its own subdirectory under the root directory and each run of the job has its own metric log file named after the job ID as ${job_id}.metrics.log . Hadoop counters : used for M/R deployments. Gobblin-specific metrics are reported in the \"JOB\" or \"TASK\" groups for job- and task- level metrics. By default, task-level metrics are not reported through Hadoop counters as doing so may cause the number of Hadoop counters to go beyond the system-wide limit. However, users can choose to turn on reporting task-level metrics as Hadoop counters by setting mr.include.task.counters=true . Metrics collection JVM Metrics The standalone deployment of Gobblin runs in a single JVM so it's important to monitor the health of the JVM, through a set of pre-defined JVM metrics in the following four categories. jvm.gc : this covers metrics related to garbage collection, e.g., counts and time spent on garbage collection. jvm.memory : this covers metrics related to memory usage, e.g., detailed heap usage. jvm.threads : this covers metrics related to thread states, e.g., thread count and thread deadlocks. jvm.fileDescriptorRatio : this measures the ratio of open file descriptors. All JVM metrics are reported via JMX and can be checked using tools such as VisualVM or JConsole. Pre-defined Job Execution Metrics Internally, Gobblin pre-defines a minimum set of metrics listed below in two metric groups: JOB and TASK for job-level metrics and task-level metrics, respectively. Those metrics are useful in keeping track of the progress and performance of job executions. ${metric_group}.${id}.records : this metric keeps track of the total number of data records extracted by the job or task depending on the ${metric_group} . The ${id} is either a job ID or a task ID depending on the ${metric_group} . ${metric_group}.${id}.recordsPerSecond : this metric keeps track of the rate of data extraction as data records extracted per second by the job or task depending on the ${metric_group} . ${metric_group}.${id}.bytes : this metric keeps track of the total number of bytes extracted by the job or task depending on the ${metric_group} . ${metric_group}.${id}.bytesPerSecond : this metric keeps track of the rate of data extraction as bytes extracted per second by the job or task depending on the ${metric_group} . Among the above metrics, ${metric_group}.${id}.records and ${metric_group}.${id}.bytes are reported as Hadoop MapReduce counters for Gobblin jobs running on Hadoop. Job Execution History Store Gobblin also supports writing job execution information to a job execution history store backed by a database of choice. Gobblin uses MySQL by default and it ships with the SQL DDLs of the relevant MySQL tables, although it still allows users to choose which database to use as long as the schema of the tables is compatible. Users can use the properties job.history.store.url and job.history.store.jdbc.driver to specify the database URL and the JDBC driver to work with the database of choice. The user name and password used to access the database can be specified using the properties job.history.store.user and job.history.store.password . An example configuration is shown below: job.history.store.url=jdbc:mysql://localhost/gobblin job.history.store.jdbc.driver=com.mysql.jdbc.Driver job.history.store.user=gobblin job.history.store.password=gobblin Email Notifications In addition to writing job execution information to the job execution history store, Gobblin also supports sending email notifications about job status. Job status notifications fall into two categories: alerts in case of job failures and normal notifications in case of successful job completions. Users can choose to enable or disable both categories using the properties email.alert.enabled and email.notification.enabled . The main content of an email alert or notification is a job status report in Json format. Below is an example job status report: { job name : Gobblin_Demo_Job , job id : job_Gobblin_Demo_Job_1417487480842 , job state : COMMITTED , start time : 1417487480874, end time : 1417490858913, duration : 3378039, tasks : 1, completed tasks : 1, task states : [ { task id : task_Gobblin_Demo_Job_1417487480842_0 , task state : COMMITTED , start time : 1417490795903, end time : 1417490858908, duration : 63005, high watermark : -1, exception : } ] }","title":"Monitoring"},{"location":"user-guide/Monitoring/#table-of-contents","text":"Table of Contents Overview Metrics Collecting and Reporting Metrics Reporting Metrics collection JVM Metrics Pre-defined Job Execution Metrics Job Execution History Store Email Notifications","title":"Table of Contents"},{"location":"user-guide/Monitoring/#overview","text":"As a framework for ingesting potentially huge volume of data from many different sources, it's critical to monitor the health and status of the system and job executions. Gobblin employs a variety of approaches introduced below for this purpose. All the approaches are optional and can be configured to be turned on and off in different combinations through the framework and job configurations.","title":"Overview"},{"location":"user-guide/Monitoring/#metrics-collecting-and-reporting","text":"","title":"Metrics Collecting and Reporting"},{"location":"user-guide/Monitoring/#metrics-reporting","text":"Out-of-the-box, Gobblin reports metrics though: JMX : used in the standalone deployment. Metrics reported to JMX can be checked using using tools such as VisualVM or JConsole. Metric log files : Files are stored in a root directory defined by the property metrics.log.dir . Each Gobblin job has its own subdirectory under the root directory and each run of the job has its own metric log file named after the job ID as ${job_id}.metrics.log . Hadoop counters : used for M/R deployments. Gobblin-specific metrics are reported in the \"JOB\" or \"TASK\" groups for job- and task- level metrics. By default, task-level metrics are not reported through Hadoop counters as doing so may cause the number of Hadoop counters to go beyond the system-wide limit. However, users can choose to turn on reporting task-level metrics as Hadoop counters by setting mr.include.task.counters=true .","title":"Metrics Reporting"},{"location":"user-guide/Monitoring/#metrics-collection","text":"","title":"Metrics collection"},{"location":"user-guide/Monitoring/#jvm-metrics","text":"The standalone deployment of Gobblin runs in a single JVM so it's important to monitor the health of the JVM, through a set of pre-defined JVM metrics in the following four categories. jvm.gc : this covers metrics related to garbage collection, e.g., counts and time spent on garbage collection. jvm.memory : this covers metrics related to memory usage, e.g., detailed heap usage. jvm.threads : this covers metrics related to thread states, e.g., thread count and thread deadlocks. jvm.fileDescriptorRatio : this measures the ratio of open file descriptors. All JVM metrics are reported via JMX and can be checked using tools such as VisualVM or JConsole.","title":"JVM Metrics"},{"location":"user-guide/Monitoring/#pre-defined-job-execution-metrics","text":"Internally, Gobblin pre-defines a minimum set of metrics listed below in two metric groups: JOB and TASK for job-level metrics and task-level metrics, respectively. Those metrics are useful in keeping track of the progress and performance of job executions. ${metric_group}.${id}.records : this metric keeps track of the total number of data records extracted by the job or task depending on the ${metric_group} . The ${id} is either a job ID or a task ID depending on the ${metric_group} . ${metric_group}.${id}.recordsPerSecond : this metric keeps track of the rate of data extraction as data records extracted per second by the job or task depending on the ${metric_group} . ${metric_group}.${id}.bytes : this metric keeps track of the total number of bytes extracted by the job or task depending on the ${metric_group} . ${metric_group}.${id}.bytesPerSecond : this metric keeps track of the rate of data extraction as bytes extracted per second by the job or task depending on the ${metric_group} . Among the above metrics, ${metric_group}.${id}.records and ${metric_group}.${id}.bytes are reported as Hadoop MapReduce counters for Gobblin jobs running on Hadoop.","title":"Pre-defined Job Execution Metrics"},{"location":"user-guide/Monitoring/#job-execution-history-store","text":"Gobblin also supports writing job execution information to a job execution history store backed by a database of choice. Gobblin uses MySQL by default and it ships with the SQL DDLs of the relevant MySQL tables, although it still allows users to choose which database to use as long as the schema of the tables is compatible. Users can use the properties job.history.store.url and job.history.store.jdbc.driver to specify the database URL and the JDBC driver to work with the database of choice. The user name and password used to access the database can be specified using the properties job.history.store.user and job.history.store.password . An example configuration is shown below: job.history.store.url=jdbc:mysql://localhost/gobblin job.history.store.jdbc.driver=com.mysql.jdbc.Driver job.history.store.user=gobblin job.history.store.password=gobblin","title":"Job Execution History Store"},{"location":"user-guide/Monitoring/#email-notifications","text":"In addition to writing job execution information to the job execution history store, Gobblin also supports sending email notifications about job status. Job status notifications fall into two categories: alerts in case of job failures and normal notifications in case of successful job completions. Users can choose to enable or disable both categories using the properties email.alert.enabled and email.notification.enabled . The main content of an email alert or notification is a job status report in Json format. Below is an example job status report: { job name : Gobblin_Demo_Job , job id : job_Gobblin_Demo_Job_1417487480842 , job state : COMMITTED , start time : 1417487480874, end time : 1417490858913, duration : 3378039, tasks : 1, completed tasks : 1, task states : [ { task id : task_Gobblin_Demo_Job_1417487480842_0 , task state : COMMITTED , start time : 1417490795903, end time : 1417490858908, duration : 63005, high watermark : -1, exception : } ] }","title":"Email Notifications"},{"location":"user-guide/Partitioned-Writers/","text":"Table of Contents Table of Contents Existing Partition Aware Writers Existing Partitioners Design Implementing a partitioner Implementing a Partition Aware Writer Builder Gobblin allows partitioning output data using a writer partitioner. This allows, for example, to write timestamped records to a different file depending on the timestamp of the record. To partition output records, two things are needed: Set writer.builder.class to a class that implements PartitionAwareDataWriterBuilder . Set writer.partitioner.class to the class of the desired partitioner, which must be subclass of WriterPartitioner . The partitioner will get all Gobblin configuration options, so some partitioners may require additional configurations. If writer.partitioner.class is set but writer.builder.class is not a PartitionAwareDataWriterBuilder , Gobblin will throw an error. If writer.builder.class is a PartitionAwareDataWriterBuilder , but no partitioner is set, Gobblin will attempt to still create the writer with no partition, however, the writer may not support unpartitioned data, in which case it will throw an error. WriterPartitioner s compute a partition key for each record. Some PartitionAwareDataWriterBuilder are unable to handle certain partition keys (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer cannot handle the partitioner key, Gobblin will throw an error. The Javadoc of partitioners should always include the schema it emits and the writer Javadoc should contain which schemas it accepts for ease of use. Existing Partition Aware Writers gobblin.writer.AvroDataWriterBuilder : If partition is present, creates directory structure based on partition. For example, if partition is {name=\"foo\", type=\"bar\"} , the record will be written to a file in directory /path/to/data/name=foo/type=bar/file.avro . Existing Partitioners gobblin.example.wikipedia.WikipediaPartitioner : Sample partitioner for the Wikipedia example. Partitions record by article title. Design Gobblin always instantiates a PartitionedDataWriter for each fork. On construction, the partitioned writer: checks whether a partitioner is present in the configuration. If no partitioner is present, then the instance of PartitionedDataWriter is simply a thin wrapper around a normal writer. If a partitioner is present, the partitioned writer will check if the class configured at writer.builder.class is an instance of PartitionAwareDataWriterBuilder , throwing an error in case this is not true. The partitioned writer instantiate the partitioner, runs partitionSchema() , and then checks whether the partition aware writer builder accepts that schema using validatePartitionSchema . If this returns false, Gobblin will throw an error. Every time the partitioned writer gets a record, it uses the partitioner to get a partition key for that record. The partitioned writer keeps an internal map from partition key to instances of writers for each partition. If a writer is already created for this key, it will call write on that writer for the new record. If the writer is not present, the partitioned writer will instantiate a new writer with the computed partition, and then pass in the record. WriterPartitioner partitions records by returning a partition key for each record, which is of type GenericRecord . Each WriterPartitioner emits keys with a particular Schema which is available by using the method WriterPartitioner#partitionSchema() . Implementations of PartitionAwareDataWriterBuilder must check the partition schema to decide if they can understand and correctly handle that schema when the method PartitionAwareDataWriterBuilder#validateSchema is called (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer rejects the partition schema, then Gobblin will throw an error before writing anything. Implementing a partitioner The interface for a partitioner is /** * Partitions records in the writer phase. */ public interface WriterPartitioner D { /** * @return The schema that {@link GenericRecord} returned by {@link #partitionForRecord} will have. */ public Schema partitionSchema(); /** * Returns the partition that the input record belongs to. If * partitionFoRecord(record1).equals(partitionForRecord(record2)), then record1 and record2 * belong to the same partition. * @param record input to compute partition for. * @return {@link GenericRecord} representing partition record belongs to. */ public GenericRecord partitionForRecord(D record); } For an example of a partitioner implementation see gobblin.example.wikipedia.WikipediaPartitioner . Each class that implements WriterPartitioner is required to have a public constructor with signature (State state, int numBranches, int branchId) . Implementing a Partition Aware Writer Builder This is very similar to a regular DataWriterBuilder , with two differences: You must implement the method validatePartitionSchema(Schema) that must return false unless the builder can handle that schema. The field partition is available, which is a GenericRecord that contains the partition key for the built writer. For any two different keys, Gobblin may create a writer for each key, so it is important that writers for different keys do not collide (e.g. do not try to use the same path). For an example of a simple PartitionAwareWriterBuilder see gobblin.writer.AvroDataWriterBuilder .","title":"Partitioned Writers"},{"location":"user-guide/Partitioned-Writers/#table-of-contents","text":"Table of Contents Existing Partition Aware Writers Existing Partitioners Design Implementing a partitioner Implementing a Partition Aware Writer Builder Gobblin allows partitioning output data using a writer partitioner. This allows, for example, to write timestamped records to a different file depending on the timestamp of the record. To partition output records, two things are needed: Set writer.builder.class to a class that implements PartitionAwareDataWriterBuilder . Set writer.partitioner.class to the class of the desired partitioner, which must be subclass of WriterPartitioner . The partitioner will get all Gobblin configuration options, so some partitioners may require additional configurations. If writer.partitioner.class is set but writer.builder.class is not a PartitionAwareDataWriterBuilder , Gobblin will throw an error. If writer.builder.class is a PartitionAwareDataWriterBuilder , but no partitioner is set, Gobblin will attempt to still create the writer with no partition, however, the writer may not support unpartitioned data, in which case it will throw an error. WriterPartitioner s compute a partition key for each record. Some PartitionAwareDataWriterBuilder are unable to handle certain partition keys (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer cannot handle the partitioner key, Gobblin will throw an error. The Javadoc of partitioners should always include the schema it emits and the writer Javadoc should contain which schemas it accepts for ease of use.","title":"Table of Contents"},{"location":"user-guide/Partitioned-Writers/#existing-partition-aware-writers","text":"gobblin.writer.AvroDataWriterBuilder : If partition is present, creates directory structure based on partition. For example, if partition is {name=\"foo\", type=\"bar\"} , the record will be written to a file in directory /path/to/data/name=foo/type=bar/file.avro .","title":"Existing Partition Aware Writers"},{"location":"user-guide/Partitioned-Writers/#existing-partitioners","text":"gobblin.example.wikipedia.WikipediaPartitioner : Sample partitioner for the Wikipedia example. Partitions record by article title.","title":"Existing Partitioners"},{"location":"user-guide/Partitioned-Writers/#design","text":"Gobblin always instantiates a PartitionedDataWriter for each fork. On construction, the partitioned writer: checks whether a partitioner is present in the configuration. If no partitioner is present, then the instance of PartitionedDataWriter is simply a thin wrapper around a normal writer. If a partitioner is present, the partitioned writer will check if the class configured at writer.builder.class is an instance of PartitionAwareDataWriterBuilder , throwing an error in case this is not true. The partitioned writer instantiate the partitioner, runs partitionSchema() , and then checks whether the partition aware writer builder accepts that schema using validatePartitionSchema . If this returns false, Gobblin will throw an error. Every time the partitioned writer gets a record, it uses the partitioner to get a partition key for that record. The partitioned writer keeps an internal map from partition key to instances of writers for each partition. If a writer is already created for this key, it will call write on that writer for the new record. If the writer is not present, the partitioned writer will instantiate a new writer with the computed partition, and then pass in the record. WriterPartitioner partitions records by returning a partition key for each record, which is of type GenericRecord . Each WriterPartitioner emits keys with a particular Schema which is available by using the method WriterPartitioner#partitionSchema() . Implementations of PartitionAwareDataWriterBuilder must check the partition schema to decide if they can understand and correctly handle that schema when the method PartitionAwareDataWriterBuilder#validateSchema is called (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer rejects the partition schema, then Gobblin will throw an error before writing anything.","title":"Design"},{"location":"user-guide/Partitioned-Writers/#implementing-a-partitioner","text":"The interface for a partitioner is /** * Partitions records in the writer phase. */ public interface WriterPartitioner D { /** * @return The schema that {@link GenericRecord} returned by {@link #partitionForRecord} will have. */ public Schema partitionSchema(); /** * Returns the partition that the input record belongs to. If * partitionFoRecord(record1).equals(partitionForRecord(record2)), then record1 and record2 * belong to the same partition. * @param record input to compute partition for. * @return {@link GenericRecord} representing partition record belongs to. */ public GenericRecord partitionForRecord(D record); } For an example of a partitioner implementation see gobblin.example.wikipedia.WikipediaPartitioner . Each class that implements WriterPartitioner is required to have a public constructor with signature (State state, int numBranches, int branchId) .","title":"Implementing a partitioner"},{"location":"user-guide/Partitioned-Writers/#implementing-a-partition-aware-writer-builder","text":"This is very similar to a regular DataWriterBuilder , with two differences: You must implement the method validatePartitionSchema(Schema) that must return false unless the builder can handle that schema. The field partition is available, which is a GenericRecord that contains the partition key for the built writer. For any two different keys, Gobblin may create a writer for each key, so it is important that writers for different keys do not collide (e.g. do not try to use the same path). For an example of a simple PartitionAwareWriterBuilder see gobblin.writer.AvroDataWriterBuilder .","title":"Implementing a Partition Aware Writer Builder"},{"location":"user-guide/Source-schema-and-Converters/","text":"Table of Contents Table of Contents Source schema Converters Converters available in Gobblin Schema specification Supported data types by different converters Primitive types Complex types Array Map Record Enum Nesting types Source schema A source schema has to be declared before extracting the data from the source. To define the source schema source.schema property is available which takes a JSON value defining the source schema. This schema is used by Converters to perform data type or data format conversions. The java class representation of a source schema can be found here Schema.java . Converters In Gobblin library a Converter is an interface for classes that implement data transformations, e.g., data type conversions, schema projections, data manipulations, data filtering, etc. This interface is responsible for converting both schema and data records. Classes implementing this interface are composible and can be chained together to achieve more complex data transformations. A converter basically needs four inputs: - Input schema - Output schema type - Input data - Output data type There are various inbuilt Converters available within gobblin-core. However, you can also implement your own converter by extending abstract class org.apache.gobblin.converter.Converter . Below, is example of such a custom implementation of Gobblin Converter which replaces multiple newlines and spaces from JSON values. package org.apache.gobblin.example.sample; import org.apache.gobblin.configuration.WorkUnitState; import org.apache.gobblin.converter.Converter; import org.apache.gobblin.converter.DataConversionException; import org.apache.gobblin.converter.SchemaConversionException; import org.apache.gobblin.converter.SingleRecordIterable; import com.google.gson.JsonArray; import com.google.gson.JsonObject; import com.google.gson.JsonParser; public class FilterSpacesConverter extends Converter JsonArray, JsonArray, JsonObject, JsonObject { @Override public JsonArray convertSchema(JsonArray inputSchema, WorkUnitState workUnit) throws SchemaConversionException { return inputSchema; //We are not doing any schema conversion } @Override public Iterable JsonObject convertRecord(JsonArray outputSchema, JsonObject inputRecord, WorkUnitState workUnit) throws DataConversionException { String jsonStr = inputRecord.toString().replaceAll( \\\\s{2,} , ); return new SingleRecordIterable (new JsonParser().parse(jsonStr).getAsJsonObject()); } } The converters can also be chained to perform sequential conversion on each input record. To chain converters use the property converter.classes and provide a list of comma separated converters with full reference name of converters. The execution order of the converters is same as defined in the comma separated list. For example: If you are reading data from a JsonSource and you want to write data into Avro format. For this you can chain the converters to convert from Json string to Json and the convert Json into Avro. By using the following property in your .pull file. converter.classes=\"org.apache.gobblin.converter.json.JsonStringToJsonIntermediateConverter,org.apache.gobblin.converter.avro.JsonIntermediateToAvroConverter\" Converters available in Gobblin AvroFieldRetrieverConverter.java AvroRecordToAvroWritableConverter.java AvroToAvroCopyableConverter.java AvroToBytesConverter.java BytesToAvroConverter.java FlattenNestedKeyConverter.java JsonIntermediateToAvroConverter.java JsonRecordAvroSchemaToAvroConverter.java CsvToJsonConverter.java CsvToJsonConverterV2.java AvroFieldsPickConverter.java AvroFilterConverter.java AvroToRestJsonEntryConverter.java BytesToJsonConverter.java JsonStringToJsonIntermediateConverter.java JsonToStringConverter.java ObjectStoreConverter.java ObjectStoreDeleteConverter.java HiveSerDeConverter.java ObjectToStringConverter.java StringFilterConverter.java StringSplitterConverter.java StringSplitterToListConverter.java StringToBytesConverter.java TextToStringConverter.java GobblinMetricsPinotFlattenerConverter.java Schema specification The following section discusses the specification to define source schema using a JSON format. Key Name Value data type Description columnName String The name of the JSON key which will contain the data. isNullable Boolean Can data be null? comment String Field description just for documentation purpose. dataType JSON Provides more information about the data type. dataType.type String Type of data to store. ex: int, long etc dataType.name String Provide a name to your data type. dataType.items String/JSON Used for array type to define the data type of items contained by the array. If data type of array items is primitive the String is used as value otherwise for complex type dataType JSON should be used as a value to provide further information on complex array items. dataType.values String/JSON/Array Used by map and record types to define the data type of the values. In case of records it will always be Array type defining fields. In case of map it could be String or JSON based on primitive or complex data type involved. dataype.symbols Array Array of strings to define the enum symbols. watermark Boolean To specify if the key is used as a watermark. Or use extract.delta.fields property to define comma separated list of watermark fields. unique Boolean To specify if the key should be unique set of records. defaultValue Object To specify the default value. Supported data types by different converters The converters which perform data format conversions such as CSV to JSON, JSON to AVRO etc. will have to perform data type conversions. Below, is the list of such converters and the data types they support. Converter Data types JsonIntermediateToAvroConverter.java DATE TIMESTAMP TIME STRING BYTES INT LONG FLOAT DOUBLE BOOLEAN ARRAY MAP ENUM JsonIntermediateToParquetGroupConverter.java DATE TIMESTAMP TIME STRING BYTES INT LONG FLOAT DOUBLE BOOLEAN ARRAY MAP ENUM Primitive types The following primitive types are available int, float, string, double, long, null, boolean. Sample data { jobRoles : 42, peopleWeightAvg : 50.5, peopleOrg : EvilCorp , peopleAvgSal : 342222.65, peopleCount : 8344242342, peopleBrain : null, public : false } Sample schema [ { columnName : jobRoles , isNullable : false, comment : Number of roles in the org dataType : { type : int } }, { columnName : peopleWeightAvg , isNullable : false, comment : Avg weight of people in org dataType : { type : float } }, { columnName : peopleOrg , isNullable : false, comment : Name of org people works for dataType : { type : string } }, { columnName : peopleAvgSal , isNullable : false, comment : Avg salary of people in org dataType : { type : double } }, { columnName : peopleCount , isNullable : false, comment : Count of people in org dataType : { type : long } }, { columnName : peopleBrain , comment : Brain obj of people dataType : { type : null } }, { columnName : public , isNullable : false, comment : Is data public dataType : { type : boolean } } ] Complex types Array Sample data { arrayOfInts : [25, 50, 75] } Sample schema [ { columnName : arrayOfInts , isNullable : false, comment : Items in array have same data type as defined in dataType. dataType : { type : array , items : int } } ] Map Maps can contain n number of key value pairs with constraint of same data type for values and keys are always string. Sample data { bookDetails :{ harry potter and the deathly hallows : 10245, harry potter and the cursed child : 20362 } } Sample schema [ { columnName : bookDetails , isNullable : false, comment : Maps always have string as keys and all values have same type as defined in dataType dataType : { type : map , values : long } } ] Record Unlike map, values in record type are not bound by single value type. Keys and values have to be declared in the schema with data type. Sample data { userDetails : { userName : anonyoumous , userAge : 50, } } Sample schema [ { columnName : userDetails , isNullable : false, comment : user detail dataType : { type : record , values : [ { columnName : userName , dataType :{ type : string } }, { columnName : userAge , dataType :{ type : int } } ] } } ] Enum Sample data { userStatus : ACTIVE } Sample schema [ { columnName : userStatus , dataType :{ type : enum , symbols :[ ACTIVE , INACTIVE ] } } ] Nesting types Complex types can be used to created nested schemas. Array, Map and Record can have complex items instead of just primitive types. Few of the examples to show how nested schema is written Array with nested record [ { columnName : userName , dataType : { type : string } }, { columnName : purchase , dataType : { type : array , items : { dataType : { type : record , values : [ { columnName : ProductName , dataType : { type : string } }, { columnName : ProductPrice , dataType : { type : long } } ] } } } } ] Map with nested array [ { columnName : persons , dataType : { type : map , values : { dataType : { type : array , items : int } } } } ]","title":"Source schema and Converters"},{"location":"user-guide/Source-schema-and-Converters/#table-of-contents","text":"Table of Contents Source schema Converters Converters available in Gobblin Schema specification Supported data types by different converters Primitive types Complex types Array Map Record Enum Nesting types","title":"Table of Contents"},{"location":"user-guide/Source-schema-and-Converters/#source-schema","text":"A source schema has to be declared before extracting the data from the source. To define the source schema source.schema property is available which takes a JSON value defining the source schema. This schema is used by Converters to perform data type or data format conversions. The java class representation of a source schema can be found here Schema.java .","title":"Source schema"},{"location":"user-guide/Source-schema-and-Converters/#converters","text":"In Gobblin library a Converter is an interface for classes that implement data transformations, e.g., data type conversions, schema projections, data manipulations, data filtering, etc. This interface is responsible for converting both schema and data records. Classes implementing this interface are composible and can be chained together to achieve more complex data transformations. A converter basically needs four inputs: - Input schema - Output schema type - Input data - Output data type There are various inbuilt Converters available within gobblin-core. However, you can also implement your own converter by extending abstract class org.apache.gobblin.converter.Converter . Below, is example of such a custom implementation of Gobblin Converter which replaces multiple newlines and spaces from JSON values. package org.apache.gobblin.example.sample; import org.apache.gobblin.configuration.WorkUnitState; import org.apache.gobblin.converter.Converter; import org.apache.gobblin.converter.DataConversionException; import org.apache.gobblin.converter.SchemaConversionException; import org.apache.gobblin.converter.SingleRecordIterable; import com.google.gson.JsonArray; import com.google.gson.JsonObject; import com.google.gson.JsonParser; public class FilterSpacesConverter extends Converter JsonArray, JsonArray, JsonObject, JsonObject { @Override public JsonArray convertSchema(JsonArray inputSchema, WorkUnitState workUnit) throws SchemaConversionException { return inputSchema; //We are not doing any schema conversion } @Override public Iterable JsonObject convertRecord(JsonArray outputSchema, JsonObject inputRecord, WorkUnitState workUnit) throws DataConversionException { String jsonStr = inputRecord.toString().replaceAll( \\\\s{2,} , ); return new SingleRecordIterable (new JsonParser().parse(jsonStr).getAsJsonObject()); } } The converters can also be chained to perform sequential conversion on each input record. To chain converters use the property converter.classes and provide a list of comma separated converters with full reference name of converters. The execution order of the converters is same as defined in the comma separated list. For example: If you are reading data from a JsonSource and you want to write data into Avro format. For this you can chain the converters to convert from Json string to Json and the convert Json into Avro. By using the following property in your .pull file. converter.classes=\"org.apache.gobblin.converter.json.JsonStringToJsonIntermediateConverter,org.apache.gobblin.converter.avro.JsonIntermediateToAvroConverter\"","title":"Converters"},{"location":"user-guide/Source-schema-and-Converters/#converters-available-in-gobblin","text":"AvroFieldRetrieverConverter.java AvroRecordToAvroWritableConverter.java AvroToAvroCopyableConverter.java AvroToBytesConverter.java BytesToAvroConverter.java FlattenNestedKeyConverter.java JsonIntermediateToAvroConverter.java JsonRecordAvroSchemaToAvroConverter.java CsvToJsonConverter.java CsvToJsonConverterV2.java AvroFieldsPickConverter.java AvroFilterConverter.java AvroToRestJsonEntryConverter.java BytesToJsonConverter.java JsonStringToJsonIntermediateConverter.java JsonToStringConverter.java ObjectStoreConverter.java ObjectStoreDeleteConverter.java HiveSerDeConverter.java ObjectToStringConverter.java StringFilterConverter.java StringSplitterConverter.java StringSplitterToListConverter.java StringToBytesConverter.java TextToStringConverter.java GobblinMetricsPinotFlattenerConverter.java","title":"Converters available in Gobblin"},{"location":"user-guide/Source-schema-and-Converters/#schema-specification","text":"The following section discusses the specification to define source schema using a JSON format. Key Name Value data type Description columnName String The name of the JSON key which will contain the data. isNullable Boolean Can data be null? comment String Field description just for documentation purpose. dataType JSON Provides more information about the data type. dataType.type String Type of data to store. ex: int, long etc dataType.name String Provide a name to your data type. dataType.items String/JSON Used for array type to define the data type of items contained by the array. If data type of array items is primitive the String is used as value otherwise for complex type dataType JSON should be used as a value to provide further information on complex array items. dataType.values String/JSON/Array Used by map and record types to define the data type of the values. In case of records it will always be Array type defining fields. In case of map it could be String or JSON based on primitive or complex data type involved. dataype.symbols Array Array of strings to define the enum symbols. watermark Boolean To specify if the key is used as a watermark. Or use extract.delta.fields property to define comma separated list of watermark fields. unique Boolean To specify if the key should be unique set of records. defaultValue Object To specify the default value.","title":"Schema specification"},{"location":"user-guide/Source-schema-and-Converters/#supported-data-types-by-different-converters","text":"The converters which perform data format conversions such as CSV to JSON, JSON to AVRO etc. will have to perform data type conversions. Below, is the list of such converters and the data types they support. Converter Data types JsonIntermediateToAvroConverter.java DATE TIMESTAMP TIME STRING BYTES INT LONG FLOAT DOUBLE BOOLEAN ARRAY MAP ENUM JsonIntermediateToParquetGroupConverter.java DATE TIMESTAMP TIME STRING BYTES INT LONG FLOAT DOUBLE BOOLEAN ARRAY MAP ENUM","title":"Supported data types by different converters"},{"location":"user-guide/Source-schema-and-Converters/#primitive-types","text":"The following primitive types are available int, float, string, double, long, null, boolean. Sample data { jobRoles : 42, peopleWeightAvg : 50.5, peopleOrg : EvilCorp , peopleAvgSal : 342222.65, peopleCount : 8344242342, peopleBrain : null, public : false } Sample schema [ { columnName : jobRoles , isNullable : false, comment : Number of roles in the org dataType : { type : int } }, { columnName : peopleWeightAvg , isNullable : false, comment : Avg weight of people in org dataType : { type : float } }, { columnName : peopleOrg , isNullable : false, comment : Name of org people works for dataType : { type : string } }, { columnName : peopleAvgSal , isNullable : false, comment : Avg salary of people in org dataType : { type : double } }, { columnName : peopleCount , isNullable : false, comment : Count of people in org dataType : { type : long } }, { columnName : peopleBrain , comment : Brain obj of people dataType : { type : null } }, { columnName : public , isNullable : false, comment : Is data public dataType : { type : boolean } } ]","title":"Primitive types"},{"location":"user-guide/Source-schema-and-Converters/#complex-types","text":"","title":"Complex types"},{"location":"user-guide/Source-schema-and-Converters/#array","text":"Sample data { arrayOfInts : [25, 50, 75] } Sample schema [ { columnName : arrayOfInts , isNullable : false, comment : Items in array have same data type as defined in dataType. dataType : { type : array , items : int } } ]","title":"Array"},{"location":"user-guide/Source-schema-and-Converters/#map","text":"Maps can contain n number of key value pairs with constraint of same data type for values and keys are always string. Sample data { bookDetails :{ harry potter and the deathly hallows : 10245, harry potter and the cursed child : 20362 } } Sample schema [ { columnName : bookDetails , isNullable : false, comment : Maps always have string as keys and all values have same type as defined in dataType dataType : { type : map , values : long } } ]","title":"Map"},{"location":"user-guide/Source-schema-and-Converters/#record","text":"Unlike map, values in record type are not bound by single value type. Keys and values have to be declared in the schema with data type. Sample data { userDetails : { userName : anonyoumous , userAge : 50, } } Sample schema [ { columnName : userDetails , isNullable : false, comment : user detail dataType : { type : record , values : [ { columnName : userName , dataType :{ type : string } }, { columnName : userAge , dataType :{ type : int } } ] } } ]","title":"Record"},{"location":"user-guide/Source-schema-and-Converters/#enum","text":"Sample data { userStatus : ACTIVE } Sample schema [ { columnName : userStatus , dataType :{ type : enum , symbols :[ ACTIVE , INACTIVE ] } } ]","title":"Enum"},{"location":"user-guide/Source-schema-and-Converters/#nesting-types","text":"Complex types can be used to created nested schemas. Array, Map and Record can have complex items instead of just primitive types. Few of the examples to show how nested schema is written Array with nested record [ { columnName : userName , dataType : { type : string } }, { columnName : purchase , dataType : { type : array , items : { dataType : { type : record , values : [ { columnName : ProductName , dataType : { type : string } }, { columnName : ProductPrice , dataType : { type : long } } ] } } } } ] Map with nested array [ { columnName : persons , dataType : { type : map , values : { dataType : { type : array , items : int } } } } ]","title":"Nesting types"},{"location":"user-guide/State-Management-and-Watermarks/","text":"Table of Contents Table of Contents Managing Watermarks in a Job Basics Task Failures Multi-Dataset Jobs Gobblin State Deep Dive State class hierarchy How States are Used in a Gobblin Job This page has two parts. Section 1 is an instruction on how to carry over checkpoints between two runs of a scheduled batch ingestion job, so that each run can start at where the previous run left off. Section 2 is a deep dive of different types of states in Gobblin and how they are used in a typical job run. Managing Watermarks in a Job When scheduling a Gobblin job to run in batches and pull data incrementally, each run, upon finishing its tasks, should check in the state of its work into the state store, so that the next run can continue the work based on the previous run. This is done through a concept called Watermark. Basics low watermark and expected high watermark When the Source creates WorkUnit s, each WorkUnit should generally contain a low watermark and an expected high watermark. They are the start and finish points for the corresponding task, and the task is expected to pull the data from the low watermark to the expected high watermark. actual high watermark When a task finishes extracting data, it should write the actual high watermark into its WorkUnitState . To do so, the Extractor may maintain a nextWatermark field, and in Extractor.close() , call this.workUnitState.setActualHighWatermark(this.nextWatermark) . The actual high Watermark is normally the same as the expected high Watermark if the task completes successfully, and may be smaller than the expected high Watermark if the task failed or timed-out. In some cases, the expected high watermark may not be available so the actual high watermark is the only stat available that tells where the previous run left off. In the next run, the Source will call SourceState.getPreviousWorkUnitStates() which should contain the actual high watermarks the last run checked in, to be used as the low watermarks of the new run. watermark type A watermark can be of any custom type by implementing the Watermark interface. For example, for Kafka-HDFS ingestion, if each WorkUnit is responsible for pulling a single Kafka topic partition, a watermark is a single long value representing a Kafka offset. If each WorkUnit is responsible for pulling multiple Kafka topic partitions, a watermark can be a list of long values, such as MultiLongWatermark . Task Failures A task may pull some data and then fail. If a task fails and job commit policy specified by configuration property job.commit.policy is set to full , the data it pulled won't be published. In this case, it doesn't matter what value Extractor.nextWatermark is, the actual high watermark will be automatically rolled back to the low watermark by Gobblin internally. On the other hand, if the commit policy is set to partial , the failed task may get committed and the data may get published. In this case the Extractor is responsible for setting the correct actual high watermark in Extractor.close() . Therefore, it is recommended that the Extractor update nextWatermark every time it pulls a record, so that nextWatermark is always up to date (unless you are OK with the next run re-doing the work which may cause some data to be published twice). Multi-Dataset Jobs Currently the only state store implementation Gobblin provides is FsStateStore which uses Hadoop SequenceFiles to store the states. By default, each job run reads the SequenceFile created by the previous run, and generates a new SequenceFile. This creates a pitfall when a job pulls data from multiple datasets: if a data set is skipped in a job run for whatever reason (e.g., it is blacklisted), its watermark will be unavailable for the next run. Example : suppose we schedule a Gobblin job to pull a Kafka topic from a Kafka broker, which has 10 partitions. In this case each partition is a dataset. In one of the job runs, a partition is skipped due to either being blacklisted or some failure. If no WorkUnit is created for this partition, this partition's watermark will not be checked in to the state store, and will not be available for the next run. The are two solutions to the above problem (three if you count the one that implements a different state store that behaves differently and doesn't have this problem). Solution 1 : make sure to create a WorkUnit for every dataset. Even if a dataset should be skipped, an empty WorkUnit should still be created for the dataset ('empty' means low watermark = expected high watermark). Solution 2 : use Dataset URNs. When a job pulls multiple datasets, the Source class may define a URN for each dataset, e.g., we may use PageViewEvent.5 as the URN of the 5th partition of topic PageViewEvent . When the Source creates the WorkUnit for this partition, it should set property dataset.urn in this WorkUnit with value PageViewEvent.5 . This is the solution gobblin current uses to support jobs pulling data for multiple datasets. If different WorkUnit s have different values of dataset.urn , the job will create one state store SequenceFile for each dataset.urn . In the next run, instead of calling SourceState.getPreviousWorkUnitStates() , one should use SourceState.getPreviousWorkUnitStatesByDatasetUrns() . In this way, each run will look for the most recent state store SequenceFile for each dataset, and therefore, even if a dataset is not processed by a job run, its watermark won't be lost. Note that when using Dataset URNs, each WorkUnit can only have one dataset.urn , which means, for example, in the Kafka ingestion case, each WorkUnit can only process one partition. This is usually not a big problem except that it may output too many small files (as explained in Kafka HDFS ingestion , by having a WorkUnit pull multiple partitions of the same topic, these partitions can share output files). On the other hand, different WorkUnit s may have the same dataset.urn . Gobblin State Deep Dive Gobblin involves several types of states during a job run, such as JobState , TaskState , WorkUnit , etc. They all extend the State class, which is a wrapper around Properties and provides some useful utility functions. State class hierarchy SourceState , JobState and DatasetState : SourceState contains properties that define the current job run. It contains properties in the job config file, and the states the previous run persisted in the state store. It is passed to Source to create WorkUnit s. Compared to SourceState , a JobState also contains properties of a job run such as job ID, starting time, end time, etc., as well as status of a job run, e.g, PENDING , RUNNING , COMMITTED , FAILED , etc. When the data pulled by a job is separated into different datasets (by using dataset.urn explained above), each dataset will have a DatasetState object in the JobState, and each dataset will persist its states separately. WorkUnit and MultiWorkUnit : A WorkUnit defines a unit of work. It may contain properties such as which data set to be pulled, where to start (low watermark), where to finish (expected high watermark), among others. A MultiWorkUnit contains one or more WorkUnit s. All WorkUnit s in a MultiWorkUnit will be run by a single Task. The MultiWorkUnit is useful for finer-grained control and load balancing. Without MultiWorkUnit s, if the number of WorkUnit s exceeds the number of mappers in the MR mode, the job launcher can only balance the number of WorkUnit s in the mappers. If different WorkUnit s have very different workloads (e.g., some pull from very large partitions and others pull from small partitions), this may lead to mapper skew. With MultiWorkUnit , if the Source class knows or can estimate the workload of the WorkUnit s, it can pack a large number of WorkUnit s into a smaller number of MultiWorkUnit s using its own logic, achieving better load balancing. WorkUnitState and TaskState : A WorkUnitState contains the runtime properties of a WorkUnit , e.g., actual high watermark, as well as the status of a WorkUnit, e.g., PENDING , RUNNING , COMMITTED , FAILED , etc. A TaskState additionally contains properties of a Task that runs a WorkUnit , e.g., task ID, start time, end time, etc. Extract : Extract is mainly used for ingesting from databases. It contains properties such as job type (snapshot-only, append-only, snapshot-append), primary keys, delta fields, etc. How States are Used in a Gobblin Job When a job run starts, the job launcher first creates a JobState , which contains (1) all properties specified in the job config file, and (2) the JobState / DatasetState of the previous run, which contains, among other properties, the actual high watermark the previous run checked in for each of its tasks / datasets. The job launcher then passes the JobState (as a SourceState object) to the Source , based on which the Source will create a set of WorkUnit s. Note that when creating WorkUnit s, the Source should not add properties in SourceState into the WorkUnit s, which will be done when each WorkUnit is executed in a Task . The reason is that since the job launcher runs in a single JVM, creating a large number of WorkUnit s, each containing a copy of the SourceState , may cause OOM. The job launcher prepares to run the WorkUnit s. In standalone mode, the job launcher will add properties in the JobState into each WorkUnit (if a property in JobState already exists in the WorkUnit , it will NOT be overwritten, i.e., the value in the WorkUnit takes precedence). Then for each WorkUnit it creates a Task to run the WorkUnit , and submits all these Tasks to a TaskExecutor which will run these Task s in a thread pool. In MR mode, the job launcher will serialize the JobState and each WorkUnit into a file, which will be picked up by the mappers. It then creates, configures and submits a Hadoop job. After this step, the job launcher will be waiting till all tasks finish. Each Task corresponding to a WorkUnit contains a TaskState . The TaskState initially contains all properties in JobState and the corresponding WorkUnit , and during the Task run, more runtime properties can be added to TaskState by Extractor , Converter and Writer , such as the actual high watermark explained in Section 1. After all Task s finish, DatasetState s will be created from all TaskState s based on the dataset.urn specified in the WorkUnit s. For each dataset whose data is committed, the job launcher will persist its DatasetState . If no dataset.urn is specified, there will be a single DatasetState, and thus the DatasetState will be persisted if either all Task s successfully committed, or some task failed but the commit policy is set to partial , in which case the watermarks of these failed tasks will be rolled back, as explained in Section 1.","title":"State Management and Watermarks"},{"location":"user-guide/State-Management-and-Watermarks/#table-of-contents","text":"Table of Contents Managing Watermarks in a Job Basics Task Failures Multi-Dataset Jobs Gobblin State Deep Dive State class hierarchy How States are Used in a Gobblin Job This page has two parts. Section 1 is an instruction on how to carry over checkpoints between two runs of a scheduled batch ingestion job, so that each run can start at where the previous run left off. Section 2 is a deep dive of different types of states in Gobblin and how they are used in a typical job run.","title":"Table of Contents"},{"location":"user-guide/State-Management-and-Watermarks/#managing-watermarks-in-a-job","text":"When scheduling a Gobblin job to run in batches and pull data incrementally, each run, upon finishing its tasks, should check in the state of its work into the state store, so that the next run can continue the work based on the previous run. This is done through a concept called Watermark.","title":"Managing Watermarks in a Job"},{"location":"user-guide/State-Management-and-Watermarks/#basics","text":"low watermark and expected high watermark When the Source creates WorkUnit s, each WorkUnit should generally contain a low watermark and an expected high watermark. They are the start and finish points for the corresponding task, and the task is expected to pull the data from the low watermark to the expected high watermark. actual high watermark When a task finishes extracting data, it should write the actual high watermark into its WorkUnitState . To do so, the Extractor may maintain a nextWatermark field, and in Extractor.close() , call this.workUnitState.setActualHighWatermark(this.nextWatermark) . The actual high Watermark is normally the same as the expected high Watermark if the task completes successfully, and may be smaller than the expected high Watermark if the task failed or timed-out. In some cases, the expected high watermark may not be available so the actual high watermark is the only stat available that tells where the previous run left off. In the next run, the Source will call SourceState.getPreviousWorkUnitStates() which should contain the actual high watermarks the last run checked in, to be used as the low watermarks of the new run. watermark type A watermark can be of any custom type by implementing the Watermark interface. For example, for Kafka-HDFS ingestion, if each WorkUnit is responsible for pulling a single Kafka topic partition, a watermark is a single long value representing a Kafka offset. If each WorkUnit is responsible for pulling multiple Kafka topic partitions, a watermark can be a list of long values, such as MultiLongWatermark .","title":"Basics"},{"location":"user-guide/State-Management-and-Watermarks/#task-failures","text":"A task may pull some data and then fail. If a task fails and job commit policy specified by configuration property job.commit.policy is set to full , the data it pulled won't be published. In this case, it doesn't matter what value Extractor.nextWatermark is, the actual high watermark will be automatically rolled back to the low watermark by Gobblin internally. On the other hand, if the commit policy is set to partial , the failed task may get committed and the data may get published. In this case the Extractor is responsible for setting the correct actual high watermark in Extractor.close() . Therefore, it is recommended that the Extractor update nextWatermark every time it pulls a record, so that nextWatermark is always up to date (unless you are OK with the next run re-doing the work which may cause some data to be published twice).","title":"Task Failures"},{"location":"user-guide/State-Management-and-Watermarks/#multi-dataset-jobs","text":"Currently the only state store implementation Gobblin provides is FsStateStore which uses Hadoop SequenceFiles to store the states. By default, each job run reads the SequenceFile created by the previous run, and generates a new SequenceFile. This creates a pitfall when a job pulls data from multiple datasets: if a data set is skipped in a job run for whatever reason (e.g., it is blacklisted), its watermark will be unavailable for the next run. Example : suppose we schedule a Gobblin job to pull a Kafka topic from a Kafka broker, which has 10 partitions. In this case each partition is a dataset. In one of the job runs, a partition is skipped due to either being blacklisted or some failure. If no WorkUnit is created for this partition, this partition's watermark will not be checked in to the state store, and will not be available for the next run. The are two solutions to the above problem (three if you count the one that implements a different state store that behaves differently and doesn't have this problem). Solution 1 : make sure to create a WorkUnit for every dataset. Even if a dataset should be skipped, an empty WorkUnit should still be created for the dataset ('empty' means low watermark = expected high watermark). Solution 2 : use Dataset URNs. When a job pulls multiple datasets, the Source class may define a URN for each dataset, e.g., we may use PageViewEvent.5 as the URN of the 5th partition of topic PageViewEvent . When the Source creates the WorkUnit for this partition, it should set property dataset.urn in this WorkUnit with value PageViewEvent.5 . This is the solution gobblin current uses to support jobs pulling data for multiple datasets. If different WorkUnit s have different values of dataset.urn , the job will create one state store SequenceFile for each dataset.urn . In the next run, instead of calling SourceState.getPreviousWorkUnitStates() , one should use SourceState.getPreviousWorkUnitStatesByDatasetUrns() . In this way, each run will look for the most recent state store SequenceFile for each dataset, and therefore, even if a dataset is not processed by a job run, its watermark won't be lost. Note that when using Dataset URNs, each WorkUnit can only have one dataset.urn , which means, for example, in the Kafka ingestion case, each WorkUnit can only process one partition. This is usually not a big problem except that it may output too many small files (as explained in Kafka HDFS ingestion , by having a WorkUnit pull multiple partitions of the same topic, these partitions can share output files). On the other hand, different WorkUnit s may have the same dataset.urn .","title":"Multi-Dataset Jobs"},{"location":"user-guide/State-Management-and-Watermarks/#gobblin-state-deep-dive","text":"Gobblin involves several types of states during a job run, such as JobState , TaskState , WorkUnit , etc. They all extend the State class, which is a wrapper around Properties and provides some useful utility functions.","title":"Gobblin State Deep Dive"},{"location":"user-guide/State-Management-and-Watermarks/#state-class-hierarchy","text":"SourceState , JobState and DatasetState : SourceState contains properties that define the current job run. It contains properties in the job config file, and the states the previous run persisted in the state store. It is passed to Source to create WorkUnit s. Compared to SourceState , a JobState also contains properties of a job run such as job ID, starting time, end time, etc., as well as status of a job run, e.g, PENDING , RUNNING , COMMITTED , FAILED , etc. When the data pulled by a job is separated into different datasets (by using dataset.urn explained above), each dataset will have a DatasetState object in the JobState, and each dataset will persist its states separately. WorkUnit and MultiWorkUnit : A WorkUnit defines a unit of work. It may contain properties such as which data set to be pulled, where to start (low watermark), where to finish (expected high watermark), among others. A MultiWorkUnit contains one or more WorkUnit s. All WorkUnit s in a MultiWorkUnit will be run by a single Task. The MultiWorkUnit is useful for finer-grained control and load balancing. Without MultiWorkUnit s, if the number of WorkUnit s exceeds the number of mappers in the MR mode, the job launcher can only balance the number of WorkUnit s in the mappers. If different WorkUnit s have very different workloads (e.g., some pull from very large partitions and others pull from small partitions), this may lead to mapper skew. With MultiWorkUnit , if the Source class knows or can estimate the workload of the WorkUnit s, it can pack a large number of WorkUnit s into a smaller number of MultiWorkUnit s using its own logic, achieving better load balancing. WorkUnitState and TaskState : A WorkUnitState contains the runtime properties of a WorkUnit , e.g., actual high watermark, as well as the status of a WorkUnit, e.g., PENDING , RUNNING , COMMITTED , FAILED , etc. A TaskState additionally contains properties of a Task that runs a WorkUnit , e.g., task ID, start time, end time, etc. Extract : Extract is mainly used for ingesting from databases. It contains properties such as job type (snapshot-only, append-only, snapshot-append), primary keys, delta fields, etc.","title":"State class hierarchy"},{"location":"user-guide/State-Management-and-Watermarks/#how-states-are-used-in-a-gobblin-job","text":"When a job run starts, the job launcher first creates a JobState , which contains (1) all properties specified in the job config file, and (2) the JobState / DatasetState of the previous run, which contains, among other properties, the actual high watermark the previous run checked in for each of its tasks / datasets. The job launcher then passes the JobState (as a SourceState object) to the Source , based on which the Source will create a set of WorkUnit s. Note that when creating WorkUnit s, the Source should not add properties in SourceState into the WorkUnit s, which will be done when each WorkUnit is executed in a Task . The reason is that since the job launcher runs in a single JVM, creating a large number of WorkUnit s, each containing a copy of the SourceState , may cause OOM. The job launcher prepares to run the WorkUnit s. In standalone mode, the job launcher will add properties in the JobState into each WorkUnit (if a property in JobState already exists in the WorkUnit , it will NOT be overwritten, i.e., the value in the WorkUnit takes precedence). Then for each WorkUnit it creates a Task to run the WorkUnit , and submits all these Tasks to a TaskExecutor which will run these Task s in a thread pool. In MR mode, the job launcher will serialize the JobState and each WorkUnit into a file, which will be picked up by the mappers. It then creates, configures and submits a Hadoop job. After this step, the job launcher will be waiting till all tasks finish. Each Task corresponding to a WorkUnit contains a TaskState . The TaskState initially contains all properties in JobState and the corresponding WorkUnit , and during the Task run, more runtime properties can be added to TaskState by Extractor , Converter and Writer , such as the actual high watermark explained in Section 1. After all Task s finish, DatasetState s will be created from all TaskState s based on the dataset.urn specified in the WorkUnit s. For each dataset whose data is committed, the job launcher will persist its DatasetState . If no dataset.urn is specified, there will be a single DatasetState, and thus the DatasetState will be persisted if either all Task s successfully committed, or some task failed but the commit policy is set to partial , in which case the watermarks of these failed tasks will be rolled back, as explained in Section 1.","title":"How States are Used in a Gobblin Job"},{"location":"user-guide/Troubleshooting/","text":"Table of Contents Table of Contents Checking Job State Checking Job State When there's an issue with a Gobblin job to troubleshoot, it is often helpful to check the state of the job persisted in the state store. Gobblin provides a tool `gobblin-dist/bin/statestore-checker.sh' for checking job states. The tool print job state(s) as a Json document that are easily readable. The usage of the tool is as follows: usage: statestore-checker.sh -a,--all Whether to convert all past job states of the given job -i,--id gobblin job id Gobblin job id -kc,--keepConfig Whether to keep all configuration properties -n,--name gobblin job name Gobblin job name -u,--storeurl gobblin state store URL Gobblin state store root path URL For example, assume that the state store is located at file://gobblin/state-store/ , to check the job state of the most recent run of a job named \"Foo\", run the following command: statestore-checker.sh -u file://gobblin/state-store/ -n Foo To check the job state of a particular run (say, with job ID job_Foo_123456) of job \"Foo\", run the following command: statestore-checker.sh -u file://gobblin/state-store/ -n Foo -i job_Foo_123456 To check the job states of all past runs of job \"Foo\", run the following command: statestore-checker.sh -u file://gobblin/state-store/ -n Foo -a To include job configuration in the output Json document, add option -kc or --keepConfig in the command. A sample output Json document is as follows: { job name : GobblinMRTest , job id : job_GobblinMRTest_1425622600239 , job state : COMMITTED , start time : 1425622600240, end time : 1425622601326, duration : 1086, tasks : 4, completed tasks : 4, task states : [ { task id : task_GobblinMRTest_1425622600239_3 , task state : COMMITTED , start time : 1425622600383, end time : 1425622600395, duration : 12, high watermark : -1, retry count : 0 }, { task id : task_GobblinMRTest_1425622600239_2 , task state : COMMITTED , start time : 1425622600354, end time : 1425622600374, duration : 20, high watermark : -1, retry count : 0 }, { task id : task_GobblinMRTest_1425622600239_1 , task state : COMMITTED , start time : 1425622600325, end time : 1425622600344, duration : 19, high watermark : -1, retry count : 0 }, { task id : task_GobblinMRTest_1425622600239_0 , task state : COMMITTED , start time : 1425622600405, end time : 1425622600421, duration : 16, high watermark : -1, retry count : 0 } ] }","title":"Troubleshooting"},{"location":"user-guide/Troubleshooting/#table-of-contents","text":"Table of Contents Checking Job State","title":"Table of Contents"},{"location":"user-guide/Troubleshooting/#checking-job-state","text":"When there's an issue with a Gobblin job to troubleshoot, it is often helpful to check the state of the job persisted in the state store. Gobblin provides a tool `gobblin-dist/bin/statestore-checker.sh' for checking job states. The tool print job state(s) as a Json document that are easily readable. The usage of the tool is as follows: usage: statestore-checker.sh -a,--all Whether to convert all past job states of the given job -i,--id gobblin job id Gobblin job id -kc,--keepConfig Whether to keep all configuration properties -n,--name gobblin job name Gobblin job name -u,--storeurl gobblin state store URL Gobblin state store root path URL For example, assume that the state store is located at file://gobblin/state-store/ , to check the job state of the most recent run of a job named \"Foo\", run the following command: statestore-checker.sh -u file://gobblin/state-store/ -n Foo To check the job state of a particular run (say, with job ID job_Foo_123456) of job \"Foo\", run the following command: statestore-checker.sh -u file://gobblin/state-store/ -n Foo -i job_Foo_123456 To check the job states of all past runs of job \"Foo\", run the following command: statestore-checker.sh -u file://gobblin/state-store/ -n Foo -a To include job configuration in the output Json document, add option -kc or --keepConfig in the command. A sample output Json document is as follows: { job name : GobblinMRTest , job id : job_GobblinMRTest_1425622600239 , job state : COMMITTED , start time : 1425622600240, end time : 1425622601326, duration : 1086, tasks : 4, completed tasks : 4, task states : [ { task id : task_GobblinMRTest_1425622600239_3 , task state : COMMITTED , start time : 1425622600383, end time : 1425622600395, duration : 12, high watermark : -1, retry count : 0 }, { task id : task_GobblinMRTest_1425622600239_2 , task state : COMMITTED , start time : 1425622600354, end time : 1425622600374, duration : 20, high watermark : -1, retry count : 0 }, { task id : task_GobblinMRTest_1425622600239_1 , task state : COMMITTED , start time : 1425622600325, end time : 1425622600344, duration : 19, high watermark : -1, retry count : 0 }, { task id : task_GobblinMRTest_1425622600239_0 , task state : COMMITTED , start time : 1425622600405, end time : 1425622600421, duration : 16, high watermark : -1, retry count : 0 } ] }","title":"Checking Job State"},{"location":"user-guide/Working-with-Job-Configuration-Files/","text":"Table of Contents Table of Contents Job Configuration Basics Hierarchical Structure of Job Configuration Files Password Encryption Adding or Changing Job Configuration Files Scheduled Jobs One Time Jobs Disabled Jobs Job Configuration Basics A Job configuration file is a text file with extension .pull or .job that defines the job properties that can be loaded into a Java Properties object. Gobblin uses commons-configuration to allow variable substitutions in job configuration files. You can find some example Gobblin job configuration files here . A Job configuration file typically includes the following properties, in additional to any mandatory configuration properties required by the custom Gobblin Constructs classes. For a complete reference of all configuration properties supported by Gobblin, please refer to Configuration Properties Glossary . job.name : job name. job.group : the group the job belongs to. source.class : the Source class the job uses. converter.classes : a comma-separated list of Converter classes to use in the job. This property is optional. Quality checker related configuration properties: a Gobblin job typically has both row-level and task-level quality checkers specified. Please refer to Quality Checker Properties for configuration properties related to quality checkers. Hierarchical Structure of Job Configuration Files It is often the case that a Gobblin instance runs many jobs and manages the job configuration files corresponding to those jobs. The jobs may belong to different job groups and are for different data sources. It is also highly likely that jobs for the same data source shares a lot of common properties. So it is very useful to support the following features: Job configuration files can be grouped by the job groups they belong to and put into different subdirectories under the root job configuration file directory. Common job properties shared among multiple jobs can be extracted out to a common properties file that will be applied into the job configurations of all these jobs. Gobblin supports the above features using a hierarchical structure to organize job configuration files under the root job configuration file directory. The basic idea is that there can be arbitrarily deep nesting of subdirectories under the root job configuration file directory. Each directory regardless how deep it is can have a single .properties file storing common properties that will be included when loading the job configuration files under the same directory or in any subdirectories. Below is an example directory structure. root_job_config_dir/ common.properties foo/ foo1.job foo2.job foo.properties bar/ bar1.job bar2.job bar.properties baz/ baz1.pull baz2.pull baz.properties In this example, common.properties will be included when loading foo1.job , foo2.job , bar1.job , bar2.job , baz1.pull , and baz2.pull . foo.properties will be included when loading foo1.job and foo2.job and properties set here are considered more special and will overwrite the same properties defined in common.properties . Similarly, bar.properties will be included when loading bar1.job and bar2.job , as well as baz1.pull and baz2.pull . baz.properties will be included when loading baz1.pull and baz2.pull and will overwrite the same properties defined in bar.properties and common.properties . Password Encryption To avoid storing passwords in configuration files in plain text, Gobblin supports encryption of the password configuration properties. All such properties can be encrypted (and decrypted) using a master password. The master password is stored in a file available at runtime. The file can be on a local file system or HDFS and has restricted access. The URI of the master password file is controlled by the configuration option encrypt.key.loc . By default, Gobblin will use org.jasypt.util.password.BasicPasswordEncryptor . If you have installed the JCE Unlimited Strength Policy , you can set encrypt.use.strong.encryptor=true which will configure Gobblin to use org.jasypt.util.password.StrongPasswordEncryptor . Encrypted passwords can be generated using the CLIPasswordEncryptor tool. $ gradle :gobblin-utility:assemble $ cd build/gobblin-utility/distributions/ $ tar -zxf gobblin-utility.tar.gz $ bin/gobblin_password_encryptor.sh usage: -f master password file file that contains the master password used to encrypt the plain password -h print this message -m master password master password used to encrypt the plain password -p plain password plain password to be encrypted -s use strong encryptor $ bin/gobblin_password_encryptor.sh -m Hello -p Bye ENC(AQWoQ2Ybe8KXDXwPOA1Ziw==) If you are extending Gobblin and you want some of your configurations (e.g. the ones containing credentials) to support encryption, you can use gobblin.password.PasswordManager.getInstance() methods to get an instance of PasswordManager . You can then use PasswordManager.readPassword(String) which will transparently decrypt the value if needed, i.e. if it is in the form ENC(...) and a master password is provided. Adding or Changing Job Configuration Files The Gobblin job scheduler in the standalone deployment monitors any changes to the job configuration file directory and reloads any new or updated job configuration files when detected. This allows adding new job configuration files or making changes to existing ones without bringing down the standalone instance. Currently, the following types of changes are monitored and supported: Adding a new job configuration file with a .job or .pull extension. The new job configuration file is loaded once it is detected. In the example hierarchical structure above, if a new job configuration file baz3.pull is added under bar/baz , it is loaded with properties included from common.properties , bar.properties , and baz.properties in that order. Changing an existing job configuration file with a .job or .pull extension. The job configuration file is reloaded once the change is detected. In the example above, if a change is made to foo2.job , it is reloaded with properties included from common.properties and foo.properties in that order. Changing an existing common properties file with a .properties extension. All job configuration files that include properties in the common properties file will be reloaded once the change is detected. In the example above, if bar.properties is updated, job configuration files bar1.job , bar2.job , baz1.pull , and baz2.pull will be reloaded. Properties from bar.properties will be included when loading bar1.job and bar2.job . Properties from bar.properties and baz.properties will be included when loading baz1.pull and baz2.pull in that order. Note that this job configuration file change monitoring mechanism uses the FileAlterationMonitor of Apache's commons-io with a custom FileAlterationListener . Regardless of how close two adjacent file system checks are, there are still chances that more than one files are changed between two file system checks. In case more than one file including at least one common properties file are changed between two adjacent checks, the reloading of affected job configuration files may be intermixed and applied in an order that is not desirable. This is because the order the listener is called on the changes is not controlled by Gobblin, but instead by the monitor itself. So the best practice to use this feature is to avoid making multiple changes together in a short period of time. Scheduled Jobs Gobblin ships with a job scheduler backed by a Quartz scheduler and supports Quartz's cron triggers . A job that is to be scheduled should have a cron schedule defined using the property job.schedule . Here is an example cron schedule that triggers every two minutes: job.schedule=0 0/2 * * * ? One Time Jobs Some Gobblin jobs may only need to be run once. A job without a cron schedule in the job configuration is considered a run-once job and will not be scheduled but run immediately after being loaded. A job with a cron schedule but also the property job.runonce=true specified in the job configuration is also treated as a run-once job and will only be run the first time the cron schedule is triggered. Disabled Jobs A Gobblin job can be disabled by setting the property job.disabled to true . A disabled job will not be loaded nor scheduled to run.","title":"Job Configuration Files"},{"location":"user-guide/Working-with-Job-Configuration-Files/#table-of-contents","text":"Table of Contents Job Configuration Basics Hierarchical Structure of Job Configuration Files Password Encryption Adding or Changing Job Configuration Files Scheduled Jobs One Time Jobs Disabled Jobs","title":"Table of Contents"},{"location":"user-guide/Working-with-Job-Configuration-Files/#job-configuration-basics","text":"A Job configuration file is a text file with extension .pull or .job that defines the job properties that can be loaded into a Java Properties object. Gobblin uses commons-configuration to allow variable substitutions in job configuration files. You can find some example Gobblin job configuration files here . A Job configuration file typically includes the following properties, in additional to any mandatory configuration properties required by the custom Gobblin Constructs classes. For a complete reference of all configuration properties supported by Gobblin, please refer to Configuration Properties Glossary . job.name : job name. job.group : the group the job belongs to. source.class : the Source class the job uses. converter.classes : a comma-separated list of Converter classes to use in the job. This property is optional. Quality checker related configuration properties: a Gobblin job typically has both row-level and task-level quality checkers specified. Please refer to Quality Checker Properties for configuration properties related to quality checkers.","title":"Job Configuration Basics"},{"location":"user-guide/Working-with-Job-Configuration-Files/#hierarchical-structure-of-job-configuration-files","text":"It is often the case that a Gobblin instance runs many jobs and manages the job configuration files corresponding to those jobs. The jobs may belong to different job groups and are for different data sources. It is also highly likely that jobs for the same data source shares a lot of common properties. So it is very useful to support the following features: Job configuration files can be grouped by the job groups they belong to and put into different subdirectories under the root job configuration file directory. Common job properties shared among multiple jobs can be extracted out to a common properties file that will be applied into the job configurations of all these jobs. Gobblin supports the above features using a hierarchical structure to organize job configuration files under the root job configuration file directory. The basic idea is that there can be arbitrarily deep nesting of subdirectories under the root job configuration file directory. Each directory regardless how deep it is can have a single .properties file storing common properties that will be included when loading the job configuration files under the same directory or in any subdirectories. Below is an example directory structure. root_job_config_dir/ common.properties foo/ foo1.job foo2.job foo.properties bar/ bar1.job bar2.job bar.properties baz/ baz1.pull baz2.pull baz.properties In this example, common.properties will be included when loading foo1.job , foo2.job , bar1.job , bar2.job , baz1.pull , and baz2.pull . foo.properties will be included when loading foo1.job and foo2.job and properties set here are considered more special and will overwrite the same properties defined in common.properties . Similarly, bar.properties will be included when loading bar1.job and bar2.job , as well as baz1.pull and baz2.pull . baz.properties will be included when loading baz1.pull and baz2.pull and will overwrite the same properties defined in bar.properties and common.properties .","title":"Hierarchical Structure of Job Configuration Files"},{"location":"user-guide/Working-with-Job-Configuration-Files/#password-encryption","text":"To avoid storing passwords in configuration files in plain text, Gobblin supports encryption of the password configuration properties. All such properties can be encrypted (and decrypted) using a master password. The master password is stored in a file available at runtime. The file can be on a local file system or HDFS and has restricted access. The URI of the master password file is controlled by the configuration option encrypt.key.loc . By default, Gobblin will use org.jasypt.util.password.BasicPasswordEncryptor . If you have installed the JCE Unlimited Strength Policy , you can set encrypt.use.strong.encryptor=true which will configure Gobblin to use org.jasypt.util.password.StrongPasswordEncryptor . Encrypted passwords can be generated using the CLIPasswordEncryptor tool. $ gradle :gobblin-utility:assemble $ cd build/gobblin-utility/distributions/ $ tar -zxf gobblin-utility.tar.gz $ bin/gobblin_password_encryptor.sh usage: -f master password file file that contains the master password used to encrypt the plain password -h print this message -m master password master password used to encrypt the plain password -p plain password plain password to be encrypted -s use strong encryptor $ bin/gobblin_password_encryptor.sh -m Hello -p Bye ENC(AQWoQ2Ybe8KXDXwPOA1Ziw==) If you are extending Gobblin and you want some of your configurations (e.g. the ones containing credentials) to support encryption, you can use gobblin.password.PasswordManager.getInstance() methods to get an instance of PasswordManager . You can then use PasswordManager.readPassword(String) which will transparently decrypt the value if needed, i.e. if it is in the form ENC(...) and a master password is provided.","title":"Password Encryption"},{"location":"user-guide/Working-with-Job-Configuration-Files/#adding-or-changing-job-configuration-files","text":"The Gobblin job scheduler in the standalone deployment monitors any changes to the job configuration file directory and reloads any new or updated job configuration files when detected. This allows adding new job configuration files or making changes to existing ones without bringing down the standalone instance. Currently, the following types of changes are monitored and supported: Adding a new job configuration file with a .job or .pull extension. The new job configuration file is loaded once it is detected. In the example hierarchical structure above, if a new job configuration file baz3.pull is added under bar/baz , it is loaded with properties included from common.properties , bar.properties , and baz.properties in that order. Changing an existing job configuration file with a .job or .pull extension. The job configuration file is reloaded once the change is detected. In the example above, if a change is made to foo2.job , it is reloaded with properties included from common.properties and foo.properties in that order. Changing an existing common properties file with a .properties extension. All job configuration files that include properties in the common properties file will be reloaded once the change is detected. In the example above, if bar.properties is updated, job configuration files bar1.job , bar2.job , baz1.pull , and baz2.pull will be reloaded. Properties from bar.properties will be included when loading bar1.job and bar2.job . Properties from bar.properties and baz.properties will be included when loading baz1.pull and baz2.pull in that order. Note that this job configuration file change monitoring mechanism uses the FileAlterationMonitor of Apache's commons-io with a custom FileAlterationListener . Regardless of how close two adjacent file system checks are, there are still chances that more than one files are changed between two file system checks. In case more than one file including at least one common properties file are changed between two adjacent checks, the reloading of affected job configuration files may be intermixed and applied in an order that is not desirable. This is because the order the listener is called on the changes is not controlled by Gobblin, but instead by the monitor itself. So the best practice to use this feature is to avoid making multiple changes together in a short period of time.","title":"Adding or Changing Job Configuration Files"},{"location":"user-guide/Working-with-Job-Configuration-Files/#scheduled-jobs","text":"Gobblin ships with a job scheduler backed by a Quartz scheduler and supports Quartz's cron triggers . A job that is to be scheduled should have a cron schedule defined using the property job.schedule . Here is an example cron schedule that triggers every two minutes: job.schedule=0 0/2 * * * ?","title":"Scheduled Jobs"},{"location":"user-guide/Working-with-Job-Configuration-Files/#one-time-jobs","text":"Some Gobblin jobs may only need to be run once. A job without a cron schedule in the job configuration is considered a run-once job and will not be scheduled but run immediately after being loaded. A job with a cron schedule but also the property job.runonce=true specified in the job configuration is also treated as a run-once job and will only be run the first time the cron schedule is triggered.","title":"One Time Jobs"},{"location":"user-guide/Working-with-Job-Configuration-Files/#disabled-jobs","text":"A Gobblin job can be disabled by setting the property job.disabled to true . A disabled job will not be loaded nor scheduled to run.","title":"Disabled Jobs"},{"location":"user-guide/Working-with-the-ForkOperator/","text":"Table of Contents Table of Contents Overview of the ForkOperator Using the ForkOperator Basics of Usage Per-Fork Configuration Failure Semantics Performance Tuning Comparison with PartitionedDataWriter Writing your Own ForkOperator Best Practices Troubleshooting Example Overview of the ForkOperator The ForkOperator is a type of control operators that allow a task flow to branch into multiple streams (or forked branches) as represented by a Fork , each of which goes to a separately configured sink with its own data writer. The ForkOperator gives users more flexibility in terms of controlling where and how ingested data should be output. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers. The best practices of using the ForkOperator that we recommend, though, are discussed below. The diagram below illustrates how the ForkOperator in a Gobblin task flow allows an input stream to be forked into multiple output streams, each of which can have its own converters, quality checkers, and writers. Gobblin Task Flow Using the ForkOperator Basics of Usage The ForkOperator , like most other operators in a Gobblin task flow, is pluggable through the configuration, or more specifically , the configuration property fork.operator.class that points to a class that implements the ForkOperator interface. For instance: fork.operator.class=org.apache.gobblin.fork.IdentityForkOperator By default, if no ForkOperator class is specified, internally Gobblin uses the default implementation IdentityForkOperator with a single forked branch (although it does supports multiple forked branches). The IdentityForkOperator simply unconditionally forwards the schema and ingested data records to all the forked branches, the number of which is specified through the configuration property fork.branches with a default value of 1. When an IdentityForkOperator instance is initialized, it will read the value of fork.branches and use that as the return value of getBranches . The expected number of forked branches is given by the method getBranches of the ForkOperator . This number must match the size of the list of Boolean s returned by forkSchema as well as the size of the list of Boolean s returned by forkDataRecords . Otherwise, ForkBranchMismatchException will be thrown. Note that the ForkOperator itself is not making and returning a copy for the input schema and data records, but rather just providing a Boolean for each forked branch telling if the schema or data records should be in each particular branch. Each forked branch has a branch index starting at 0. So if there are three forked branches, the branches will have indices 0, 1, and 2, respectively. Branch indices are useful to tell which branch the Gobblin task flow is in. Each branch also has a name associated with it that can be specified using the configuration property fork.branch.name. branch index . Note that the branch index is added as a suffix to the property name in this case. More on this later. If the user does not specify a name for the branches, the names in the form fork_ branch index will be used. The use of the ForkOperator with the possibility that the schema and/or data records may be forwarded to more than one forked branches has some special requirement on the input schema and data records to the ForkOperator . Specifically, because the same schema or data records may be forwarded to more than branches that may alter the schema or data records in place, it is necessary for the Gobblin task flow to make a copy of the input schema or data records for each forked branch so any modification within one branch won't affect any other branches. To guarantee that it is always able to make a copy in such a case, Gobblin requires the input schema and data records to be of type Copyable when there are more than one forked branch. Copyable is an interface that defines a method copy for making a copy of an instance of a given type. The Gobblin task flow will check if the input schema and data records are instances of Copyable and throw a CopyNotSupportedException if not. This check is performed independently on the schema first and data records subsequently. Note that this requirement is enforced if and only if the schema or data records are to be forwarded to more than one branches , which is the case if forkSchema or forkDataRecord returns a list containing more than one TRUE . Having more than one branch does not necessarily mean the schema and/or data records need to be Copyable . Gobblin ships with some built-in Copyable implementations, e.g., CopyableSchema and CopyableGenericRecord for Avro's Schema and GenericRecord . Per-Fork Configuration Since each forked branch may have it's own converters, quality checkers, and writers, in addition to the ones in the pre-fork stream (which does not have a writer apparently), there must be a way to tell the converter, quality checker, and writer classes of one branch from another and from the pre-fork stream. Gobblin uses a pretty straightforward approach: if a configuration property is used to specify something for a branch in a multi-branch use case, the branch index should be appended as a suffix to the property name. The original configuration name without the suffix is generally reserved for the pre-fork stream . For example, converter.classes.0 and converter.classes.1 are used to specify the list of converter classes for branch 0 and 1, respectively, whereas converter.classes is reserved for the pre-fork stream. If there's only a single branch (the default case), then the index suffix is not applicable. Without being a comprehensive list, the following groups of built-in configuration properties may be used with branch indices as suffices to specify things for forked branches: Converter configuration properties: configuration properties whose names start with converter . Quality checker configuration properties: configuration properties whose names start with qualitychecker . Writer configuration properties: configuration properties whose names start with writer . Failure Semantics In a normal task flow where the default IdentityForkOperator with a single branch is used, the failure of the single branch also means the failure of the task flow. When there are more than one forked branch, however, the failure semantics are more involved. Gobblin uses the following failure semantics in this case: The failure of any forked branch means the failure of the whole task flow, i.e., the task succeeds if and only if all the forked branches succeed. A forked branch stops processing any outstanding incoming data records in the queue if it fails in the middle of processing the data. The failure and subsequent stop/completion of any forked branch does not prevent other branches from processing their copies of the ingested data records. The task will wait until all the branches to finish, regardless if they succeed or fail. The commit of output data of forks is determined by the job commit policy (see JobCommitPolicy ) specified. If JobCommitPolicy.COMMIT_ON_FULL_SUCCESS (or full in short) is used, the output data of the entire job will be discarded if any forked branch fails, which will fail the task and consequently the job. If instead JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS (or successful in short) is used, output data of tasks whose forked branches all succeed will be committed. Output data of any task that has at least one failed forked branch will not be committed since the the task is considered failed in this case. This also means output data of the successful forked branches of the task won't be committed either. Performance Tuning Internally, each forked branch as represented by a Fork maintains a bounded record queue (implemented by BoundedBlockingRecordQueue ), which serves as a buffer between the pre-fork stream and the forked stream of the particular branch. The size if this bounded record queue can be configured through the property fork.record.queue.capacity . A larger queue allows for more data records to be buffered therefore giving the producer (the pre-fork stream) more head room to move forward. On the other hand, a larger queue requires more memory. The bounded record queue imposes a timeout time on all blocking operations such as putting a new record to the tail and polling a record off the head of the queue. Tuning the queue size and timeout time together offers a lot of flexibility and a tradeoff between queuing performance vs. memory consumption. In terms of the number of forked branches, we have seen use cases with a half dozen forked branches, and we are anticipating uses cases with much larger numbers. Again, when using a large number of forked branches, the size of the record queues and the timeout time need to be carefully tuned. The BoundedBlockingRecordQueue in each Fork keeps trach of the following queue statistics that can be output to the logs if the DEBUG logging level is turned on. Those statistics provide good indications on the performance of the forks. Queue size, i.e., the number of records in queue. Queue fill ratio, i.e., a ratio of the number of records in queue over the queue capacity. Put attempt rate (per second). Total put attempt count. Get attempt rate (per second). Total get attempt count. Comparison with PartitionedDataWriter Gobblin ships with a special type of DataWriter s called PartitionedDataWriter that allow ingested records to be written in a partitioned fashion using a WriterPartitioner into different locations in the same sink. The WriterPartitioner determines the specific partition for each data record. So there's certain overlap in terms of functionality between the ForkOperator and PartitionedDataWriter . The question is which one should be used under which circumstances? Below is a summary of the major differences between the two operators. The ForkOperator requires the number of forked branches to be known and returned through getBranches before the task starts, whereas the PartitionedDataWriter does not have this requirement. The PartitionedDataWriter writes each data record to a single partition, whereas the ForkOperator allows data records to be forwarded to any number of forked branches. The ForkOperator allows the use of additional converters and quality checkers in any forked branches before data gets written out. The PartitionedDataWriter is the last operator in a task flow. Use of the ForkOperator allows data records to be written to different sinks, whereas the PartitionedDataWriter is not capable of doing this. The PartitionedDataWriter writes data records sequentially in a single thread, whereas use of the ForkOperator allows forked branches to write independently in parallel since Fork s are executed in a thread pool. Writing your Own ForkOperator Since the built-in default implementation IdentityForkOperator simply blindly forks the input schema and data records to every branches, it's often necessary to have a custom implementation of the ForkOperator interface for more fine-grained control on the actual branching. Checkout the interface ForkOperator for the methods that need to be implemented. You will also find the ForkOperatorUtils to be handy when writing your own ForkOperator implementations. Best Practices The ForkOperator can have many potential use cases and we have seen the following common ones: Using a ForkOperator to write the same ingested data to multiple sinks, e.g., HDFS and S3, possibly in different formats. This kind of use cases is often referred to as \"dual writes\", which are generally NOT recommended as \"dual writes\" may lead to data inconsistency between the sinks in case of write failures. However, with the failure semantics discussed above, data inconsistency generally should not happen with the job commit policy JobCommitPolicy.COMMIT_ON_FULL_SUCCESS or JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS . This is because a failure of any forked branch means the failure of the task and none of the forked branches of the task will have its output data committed, making inconsistent output data between different sinks impossible. Using a ForkOperator to process ingested data records in different ways conditionally. For example, a ForkOperator may be used to classify and write ingested data records to different places on HDFS depending on some field in the data that serves as a classifier. Using a ForkOperator to group ingested data records of a certain schema type in case the incoming stream mixes data records of different schema types. For example, we have seen a use case in which a single Kafka topic is used for records of various schema types and when data gets ingested to HDFS, the records need to be written to different paths according to their schema types. Generally, a common use case of the ForkOperator is to route ingested data records so they get written to different output locations conditionally . The ForkOperator also finds common usage for \"dual writes\" to different sinks potentially in different formats if the job commit policy JobCommitPolicy.COMMIT_ON_FULL_SUCCESS (or full in short) or JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS (or successful in short) is used, as explained above. Troubleshooting 1) When using Forks with jobs defined as Hocon, you may encounter an error like: com.typesafe.config.ConfigException$BugOrBroken: In the map, path 'converter.classes' occurs as both the parent object of a value and as a value. Because Map has no defined ordering, this is a broken situation. at com.typesafe.config.impl.PropertiesParser.fromPathMap(PropertiesParser.java:115) at com.typesafe.config.impl.PropertiesParser.fromPathMap(PropertiesParser.java:82) at com.typesafe.config.impl.ConfigImpl.fromAnyRef(ConfigImpl.java:260) at com.typesafe.config.impl.ConfigImpl.fromPathMap(ConfigImpl.java:200) at com.typesafe.config.ConfigFactory.parseMap(ConfigFactory.java:855) at com.typesafe.config.ConfigFactory.parseMap(ConfigFactory.java:866) at gobblin.runtime.embedded.EmbeddedGobblin.getSysConfig(EmbeddedGobblin.java:497) at gobblin.runtime.embedded.EmbeddedGobblin.runAsync(EmbeddedGobblin.java:442) This is because in Hocon a key can have only a single type (see: https://github.com/lightbend/config/blob/master/HOCON.md#java-properties-mapping). To solve this, try writing your config like: ``` converter.classes.ROOT_VALUE=\"...\" ... converter.classes.0=\"...\" ... converter.classes.1=\"...\" ``` Example Let's take a look at one example that shows how to work with the ForkOperator for a real use case. Say you have a Gobblin job that ingests Avro data from a data source that may have some sensitive data in some of the fields that need to be purged. Depending on if data records have sensitive data, they need to be written to different locations on the same sink, which we assume is HDFS. So essentially the tasks of the job need a mechanism to conditionally write ingested data records to different locations depending if they have sensitive data. The ForkOperator offers a way of implementing this mechanism. In this particular use case, we need a ForkOperator implementation of two branches that forwards the schema to both branches but each data record to only one of the two branches. The default IdentityForkOperator cannot be used since it simply forwards every data records to every branches. So we need a custom implementation of the ForkOperator and let's simply call it SensitiveDataAwareForkOperator under the package gobblin.example.fork . Let's also assume that branch 0 is for data records with sensitive data, whereas branch 1 is for data records without. Below is a brief sketch of how the implementation looks like: public class SensitiveDataAwareForkOperator implements ForkOperator Schema, GenericRecord { private static final int NUM_BRANCHES = 2; @Override public void init(WorkUnitState workUnitState) { } @Override public int getBranches(WorkUnitState workUnitState) { return NUM_BRANCHES; } @Override public List Boolean forkSchema(WorkUnitState workUnitState, Schema schema) { // The schema goes to both branches. return ImmutableList.of(Boolean.TRUE, Boolean.TRUE); } @Override public List Boolean forkDataRecord(WorkUnitState workUnitState, GenericRecord record) { // Data records only go to one of the two branches depending on if they have sensitive data. // Branch 0 is for data records with sensitive data and branch 1 is for data records without. // hasSensitiveData checks the record and returns true of the record has sensitive data and false otherwise. if (hasSensitiveData(record)) { return ImmutableList.of(Boolean.TRUE, Boolean.FALSE) } return ImmutableList.of(Boolean.FALSE, Boolean.TRUE); } @Override public void close() throws IOException { } } To make the example more concrete, let's assume that the job uses some converters and quality checkers before the schema and data records reach the SensitiveDataAwareForkOperator , and it also uses a converter to purge the sensitive fields and a quality checker that makes sure some mandatory fields exist for purged data records in branch 0. Both branches will be written to the same HDFS but into different locations. fork.operator.class=org.apache.gobblin.example.fork.SensitiveDataAwareForkOperator # Pre-fork or non-fork-specific configuration properties converter.classes= Converter classes used in the task flow prior to OutlierAwareForkOperator qualitychecker.task.policies=org.apache.gobblin.policies.count.RowCountPolicy,org.apache.gobblin.policies.schema.SchemaCompatibilityPolicy qualitychecker.task.policy.types=OPTIONAL,OPTIONAL data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher # Configuration properties for branch 0 converter.classes.0=org.apache.gobblin.example.converter.PurgingConverter qualitychecker.task.policies.0=org.apache.gobblin.example,policies.MandatoryFieldExistencePolicy qualitychecker.task.policy.types.0=FAILED writer.fs.uri.0=hdfs:// namenode host : namenode port / writer.destination.type.0=HDFS writer.output.format.0=AVRO writer.staging.dir.0=/gobblin/example/task-staging/purged writer.output.dir.0=/gobblin/example/task-output/purged data.publisher.final.dir.0=/gobblin/example/job-output/purged # Configuration properties for branch 1 writer.fs.uri.1=hdfs:// namenode host : namenode port / writer.destination.type.1=HDFS writer.output.format.1=AVRO writer.staging.dir.1=/gobblin/example/task-staging/normal writer.output.dir.1=/gobblin/example/task-output/normal data.publisher.final.dir.1=/gobblin/example/job-output/normal","title":"Fork Operator"},{"location":"user-guide/Working-with-the-ForkOperator/#table-of-contents","text":"Table of Contents Overview of the ForkOperator Using the ForkOperator Basics of Usage Per-Fork Configuration Failure Semantics Performance Tuning Comparison with PartitionedDataWriter Writing your Own ForkOperator Best Practices Troubleshooting Example","title":"Table of Contents"},{"location":"user-guide/Working-with-the-ForkOperator/#overview-of-the-forkoperator","text":"The ForkOperator is a type of control operators that allow a task flow to branch into multiple streams (or forked branches) as represented by a Fork , each of which goes to a separately configured sink with its own data writer. The ForkOperator gives users more flexibility in terms of controlling where and how ingested data should be output. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers. The best practices of using the ForkOperator that we recommend, though, are discussed below. The diagram below illustrates how the ForkOperator in a Gobblin task flow allows an input stream to be forked into multiple output streams, each of which can have its own converters, quality checkers, and writers. Gobblin Task Flow","title":"Overview of the ForkOperator"},{"location":"user-guide/Working-with-the-ForkOperator/#using-the-forkoperator","text":"","title":"Using the ForkOperator"},{"location":"user-guide/Working-with-the-ForkOperator/#basics-of-usage","text":"The ForkOperator , like most other operators in a Gobblin task flow, is pluggable through the configuration, or more specifically , the configuration property fork.operator.class that points to a class that implements the ForkOperator interface. For instance: fork.operator.class=org.apache.gobblin.fork.IdentityForkOperator By default, if no ForkOperator class is specified, internally Gobblin uses the default implementation IdentityForkOperator with a single forked branch (although it does supports multiple forked branches). The IdentityForkOperator simply unconditionally forwards the schema and ingested data records to all the forked branches, the number of which is specified through the configuration property fork.branches with a default value of 1. When an IdentityForkOperator instance is initialized, it will read the value of fork.branches and use that as the return value of getBranches . The expected number of forked branches is given by the method getBranches of the ForkOperator . This number must match the size of the list of Boolean s returned by forkSchema as well as the size of the list of Boolean s returned by forkDataRecords . Otherwise, ForkBranchMismatchException will be thrown. Note that the ForkOperator itself is not making and returning a copy for the input schema and data records, but rather just providing a Boolean for each forked branch telling if the schema or data records should be in each particular branch. Each forked branch has a branch index starting at 0. So if there are three forked branches, the branches will have indices 0, 1, and 2, respectively. Branch indices are useful to tell which branch the Gobblin task flow is in. Each branch also has a name associated with it that can be specified using the configuration property fork.branch.name. branch index . Note that the branch index is added as a suffix to the property name in this case. More on this later. If the user does not specify a name for the branches, the names in the form fork_ branch index will be used. The use of the ForkOperator with the possibility that the schema and/or data records may be forwarded to more than one forked branches has some special requirement on the input schema and data records to the ForkOperator . Specifically, because the same schema or data records may be forwarded to more than branches that may alter the schema or data records in place, it is necessary for the Gobblin task flow to make a copy of the input schema or data records for each forked branch so any modification within one branch won't affect any other branches. To guarantee that it is always able to make a copy in such a case, Gobblin requires the input schema and data records to be of type Copyable when there are more than one forked branch. Copyable is an interface that defines a method copy for making a copy of an instance of a given type. The Gobblin task flow will check if the input schema and data records are instances of Copyable and throw a CopyNotSupportedException if not. This check is performed independently on the schema first and data records subsequently. Note that this requirement is enforced if and only if the schema or data records are to be forwarded to more than one branches , which is the case if forkSchema or forkDataRecord returns a list containing more than one TRUE . Having more than one branch does not necessarily mean the schema and/or data records need to be Copyable . Gobblin ships with some built-in Copyable implementations, e.g., CopyableSchema and CopyableGenericRecord for Avro's Schema and GenericRecord .","title":"Basics of Usage"},{"location":"user-guide/Working-with-the-ForkOperator/#per-fork-configuration","text":"Since each forked branch may have it's own converters, quality checkers, and writers, in addition to the ones in the pre-fork stream (which does not have a writer apparently), there must be a way to tell the converter, quality checker, and writer classes of one branch from another and from the pre-fork stream. Gobblin uses a pretty straightforward approach: if a configuration property is used to specify something for a branch in a multi-branch use case, the branch index should be appended as a suffix to the property name. The original configuration name without the suffix is generally reserved for the pre-fork stream . For example, converter.classes.0 and converter.classes.1 are used to specify the list of converter classes for branch 0 and 1, respectively, whereas converter.classes is reserved for the pre-fork stream. If there's only a single branch (the default case), then the index suffix is not applicable. Without being a comprehensive list, the following groups of built-in configuration properties may be used with branch indices as suffices to specify things for forked branches: Converter configuration properties: configuration properties whose names start with converter . Quality checker configuration properties: configuration properties whose names start with qualitychecker . Writer configuration properties: configuration properties whose names start with writer .","title":"Per-Fork Configuration"},{"location":"user-guide/Working-with-the-ForkOperator/#failure-semantics","text":"In a normal task flow where the default IdentityForkOperator with a single branch is used, the failure of the single branch also means the failure of the task flow. When there are more than one forked branch, however, the failure semantics are more involved. Gobblin uses the following failure semantics in this case: The failure of any forked branch means the failure of the whole task flow, i.e., the task succeeds if and only if all the forked branches succeed. A forked branch stops processing any outstanding incoming data records in the queue if it fails in the middle of processing the data. The failure and subsequent stop/completion of any forked branch does not prevent other branches from processing their copies of the ingested data records. The task will wait until all the branches to finish, regardless if they succeed or fail. The commit of output data of forks is determined by the job commit policy (see JobCommitPolicy ) specified. If JobCommitPolicy.COMMIT_ON_FULL_SUCCESS (or full in short) is used, the output data of the entire job will be discarded if any forked branch fails, which will fail the task and consequently the job. If instead JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS (or successful in short) is used, output data of tasks whose forked branches all succeed will be committed. Output data of any task that has at least one failed forked branch will not be committed since the the task is considered failed in this case. This also means output data of the successful forked branches of the task won't be committed either.","title":"Failure Semantics"},{"location":"user-guide/Working-with-the-ForkOperator/#performance-tuning","text":"Internally, each forked branch as represented by a Fork maintains a bounded record queue (implemented by BoundedBlockingRecordQueue ), which serves as a buffer between the pre-fork stream and the forked stream of the particular branch. The size if this bounded record queue can be configured through the property fork.record.queue.capacity . A larger queue allows for more data records to be buffered therefore giving the producer (the pre-fork stream) more head room to move forward. On the other hand, a larger queue requires more memory. The bounded record queue imposes a timeout time on all blocking operations such as putting a new record to the tail and polling a record off the head of the queue. Tuning the queue size and timeout time together offers a lot of flexibility and a tradeoff between queuing performance vs. memory consumption. In terms of the number of forked branches, we have seen use cases with a half dozen forked branches, and we are anticipating uses cases with much larger numbers. Again, when using a large number of forked branches, the size of the record queues and the timeout time need to be carefully tuned. The BoundedBlockingRecordQueue in each Fork keeps trach of the following queue statistics that can be output to the logs if the DEBUG logging level is turned on. Those statistics provide good indications on the performance of the forks. Queue size, i.e., the number of records in queue. Queue fill ratio, i.e., a ratio of the number of records in queue over the queue capacity. Put attempt rate (per second). Total put attempt count. Get attempt rate (per second). Total get attempt count.","title":"Performance Tuning"},{"location":"user-guide/Working-with-the-ForkOperator/#comparison-with-partitioneddatawriter","text":"Gobblin ships with a special type of DataWriter s called PartitionedDataWriter that allow ingested records to be written in a partitioned fashion using a WriterPartitioner into different locations in the same sink. The WriterPartitioner determines the specific partition for each data record. So there's certain overlap in terms of functionality between the ForkOperator and PartitionedDataWriter . The question is which one should be used under which circumstances? Below is a summary of the major differences between the two operators. The ForkOperator requires the number of forked branches to be known and returned through getBranches before the task starts, whereas the PartitionedDataWriter does not have this requirement. The PartitionedDataWriter writes each data record to a single partition, whereas the ForkOperator allows data records to be forwarded to any number of forked branches. The ForkOperator allows the use of additional converters and quality checkers in any forked branches before data gets written out. The PartitionedDataWriter is the last operator in a task flow. Use of the ForkOperator allows data records to be written to different sinks, whereas the PartitionedDataWriter is not capable of doing this. The PartitionedDataWriter writes data records sequentially in a single thread, whereas use of the ForkOperator allows forked branches to write independently in parallel since Fork s are executed in a thread pool.","title":"Comparison with PartitionedDataWriter"},{"location":"user-guide/Working-with-the-ForkOperator/#writing-your-own-forkoperator","text":"Since the built-in default implementation IdentityForkOperator simply blindly forks the input schema and data records to every branches, it's often necessary to have a custom implementation of the ForkOperator interface for more fine-grained control on the actual branching. Checkout the interface ForkOperator for the methods that need to be implemented. You will also find the ForkOperatorUtils to be handy when writing your own ForkOperator implementations.","title":"Writing your Own ForkOperator"},{"location":"user-guide/Working-with-the-ForkOperator/#best-practices","text":"The ForkOperator can have many potential use cases and we have seen the following common ones: Using a ForkOperator to write the same ingested data to multiple sinks, e.g., HDFS and S3, possibly in different formats. This kind of use cases is often referred to as \"dual writes\", which are generally NOT recommended as \"dual writes\" may lead to data inconsistency between the sinks in case of write failures. However, with the failure semantics discussed above, data inconsistency generally should not happen with the job commit policy JobCommitPolicy.COMMIT_ON_FULL_SUCCESS or JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS . This is because a failure of any forked branch means the failure of the task and none of the forked branches of the task will have its output data committed, making inconsistent output data between different sinks impossible. Using a ForkOperator to process ingested data records in different ways conditionally. For example, a ForkOperator may be used to classify and write ingested data records to different places on HDFS depending on some field in the data that serves as a classifier. Using a ForkOperator to group ingested data records of a certain schema type in case the incoming stream mixes data records of different schema types. For example, we have seen a use case in which a single Kafka topic is used for records of various schema types and when data gets ingested to HDFS, the records need to be written to different paths according to their schema types. Generally, a common use case of the ForkOperator is to route ingested data records so they get written to different output locations conditionally . The ForkOperator also finds common usage for \"dual writes\" to different sinks potentially in different formats if the job commit policy JobCommitPolicy.COMMIT_ON_FULL_SUCCESS (or full in short) or JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS (or successful in short) is used, as explained above.","title":"Best Practices"},{"location":"user-guide/Working-with-the-ForkOperator/#troubleshooting","text":"1) When using Forks with jobs defined as Hocon, you may encounter an error like: com.typesafe.config.ConfigException$BugOrBroken: In the map, path 'converter.classes' occurs as both the parent object of a value and as a value. Because Map has no defined ordering, this is a broken situation. at com.typesafe.config.impl.PropertiesParser.fromPathMap(PropertiesParser.java:115) at com.typesafe.config.impl.PropertiesParser.fromPathMap(PropertiesParser.java:82) at com.typesafe.config.impl.ConfigImpl.fromAnyRef(ConfigImpl.java:260) at com.typesafe.config.impl.ConfigImpl.fromPathMap(ConfigImpl.java:200) at com.typesafe.config.ConfigFactory.parseMap(ConfigFactory.java:855) at com.typesafe.config.ConfigFactory.parseMap(ConfigFactory.java:866) at gobblin.runtime.embedded.EmbeddedGobblin.getSysConfig(EmbeddedGobblin.java:497) at gobblin.runtime.embedded.EmbeddedGobblin.runAsync(EmbeddedGobblin.java:442) This is because in Hocon a key can have only a single type (see: https://github.com/lightbend/config/blob/master/HOCON.md#java-properties-mapping). To solve this, try writing your config like: ``` converter.classes.ROOT_VALUE=\"...\" ... converter.classes.0=\"...\" ... converter.classes.1=\"...\" ```","title":"Troubleshooting"},{"location":"user-guide/Working-with-the-ForkOperator/#example","text":"Let's take a look at one example that shows how to work with the ForkOperator for a real use case. Say you have a Gobblin job that ingests Avro data from a data source that may have some sensitive data in some of the fields that need to be purged. Depending on if data records have sensitive data, they need to be written to different locations on the same sink, which we assume is HDFS. So essentially the tasks of the job need a mechanism to conditionally write ingested data records to different locations depending if they have sensitive data. The ForkOperator offers a way of implementing this mechanism. In this particular use case, we need a ForkOperator implementation of two branches that forwards the schema to both branches but each data record to only one of the two branches. The default IdentityForkOperator cannot be used since it simply forwards every data records to every branches. So we need a custom implementation of the ForkOperator and let's simply call it SensitiveDataAwareForkOperator under the package gobblin.example.fork . Let's also assume that branch 0 is for data records with sensitive data, whereas branch 1 is for data records without. Below is a brief sketch of how the implementation looks like: public class SensitiveDataAwareForkOperator implements ForkOperator Schema, GenericRecord { private static final int NUM_BRANCHES = 2; @Override public void init(WorkUnitState workUnitState) { } @Override public int getBranches(WorkUnitState workUnitState) { return NUM_BRANCHES; } @Override public List Boolean forkSchema(WorkUnitState workUnitState, Schema schema) { // The schema goes to both branches. return ImmutableList.of(Boolean.TRUE, Boolean.TRUE); } @Override public List Boolean forkDataRecord(WorkUnitState workUnitState, GenericRecord record) { // Data records only go to one of the two branches depending on if they have sensitive data. // Branch 0 is for data records with sensitive data and branch 1 is for data records without. // hasSensitiveData checks the record and returns true of the record has sensitive data and false otherwise. if (hasSensitiveData(record)) { return ImmutableList.of(Boolean.TRUE, Boolean.FALSE) } return ImmutableList.of(Boolean.FALSE, Boolean.TRUE); } @Override public void close() throws IOException { } } To make the example more concrete, let's assume that the job uses some converters and quality checkers before the schema and data records reach the SensitiveDataAwareForkOperator , and it also uses a converter to purge the sensitive fields and a quality checker that makes sure some mandatory fields exist for purged data records in branch 0. Both branches will be written to the same HDFS but into different locations. fork.operator.class=org.apache.gobblin.example.fork.SensitiveDataAwareForkOperator # Pre-fork or non-fork-specific configuration properties converter.classes= Converter classes used in the task flow prior to OutlierAwareForkOperator qualitychecker.task.policies=org.apache.gobblin.policies.count.RowCountPolicy,org.apache.gobblin.policies.schema.SchemaCompatibilityPolicy qualitychecker.task.policy.types=OPTIONAL,OPTIONAL data.publisher.type=org.apache.gobblin.publisher.BaseDataPublisher # Configuration properties for branch 0 converter.classes.0=org.apache.gobblin.example.converter.PurgingConverter qualitychecker.task.policies.0=org.apache.gobblin.example,policies.MandatoryFieldExistencePolicy qualitychecker.task.policy.types.0=FAILED writer.fs.uri.0=hdfs:// namenode host : namenode port / writer.destination.type.0=HDFS writer.output.format.0=AVRO writer.staging.dir.0=/gobblin/example/task-staging/purged writer.output.dir.0=/gobblin/example/task-output/purged data.publisher.final.dir.0=/gobblin/example/job-output/purged # Configuration properties for branch 1 writer.fs.uri.1=hdfs:// namenode host : namenode port / writer.destination.type.1=HDFS writer.output.format.1=AVRO writer.staging.dir.1=/gobblin/example/task-staging/normal writer.output.dir.1=/gobblin/example/task-output/normal data.publisher.final.dir.1=/gobblin/example/job-output/normal","title":"Example"}]}