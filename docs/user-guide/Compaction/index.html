<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Apache Software Foundation">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Compaction - Apache Gobblin</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Compaction";
    var mkdocs_page_input_path = "user-guide/Compaction.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-74333035-1', 'gobblin.readthedocs.org');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Apache Gobblin</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="/">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../Powered-By/">Companies Powered By Gobblin</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../Getting-Started/">Getting Started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../Gobblin-Architecture/">Architecture</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Working-with-Job-Configuration-Files/">Job Configuration Files</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-Deployment/">Deployment</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-as-a-Library/">Gobblin as a Library</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-CLI/">Gobblin CLI</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-Compliance/">Gobblin Compliance</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-on-Yarn/">Gobblin on Yarn</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Compaction</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#table-of-contents">Table of Contents</a></li>
    

    <li class="toctree-l3"><a href="#mapreduce-compactor">MapReduce Compactor</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#example-use-case">Example Use Case</a></li>
        
            <li><a class="toctree-l4" href="#basic-usage">Basic Usage</a></li>
        
            <li><a class="toctree-l4" href="#non-deduping-compaction-via-map-only-jobs">Non-deduping Compaction via Map-only Jobs</a></li>
        
            <li><a class="toctree-l4" href="#handling-late-records">Handling Late Records</a></li>
        
            <li><a class="toctree-l4" href="#verifying-data-completeness-before-compaction">Verifying Data Completeness Before Compaction</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#hive-compactor">Hive Compactor</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#basic-usage_1">Basic Usage</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#running-a-compaction-job">Running a Compaction Job</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../State-Management-and-Watermarks/">State Management and Watermarks</a>
                </li>
                <li class="">
                    
    <a class="" href="../Working-with-the-ForkOperator/">Fork Operator</a>
                </li>
                <li class="">
                    
    <a class="" href="../Configuration-Properties-Glossary/">Configuration Glossary</a>
                </li>
                <li class="">
                    
    <a class="" href="../Source-schema-and-Converters/">Source schema and Converters</a>
                </li>
                <li class="">
                    
    <a class="" href="../Partitioned-Writers/">Partitioned Writers</a>
                </li>
                <li class="">
                    
    <a class="" href="../Monitoring/">Monitoring</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-template/">Template</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-Schedulers/">Schedulers</a>
                </li>
                <li class="">
                    
    <a class="" href="../Job-Execution-History-Store/">Job Execution History Store</a>
                </li>
                <li class="">
                    
    <a class="" href="../Building-Gobblin/">Building Gobblin</a>
                </li>
                <li class="">
                    
    <a class="" href="../Gobblin-genericLoad/">Generic Configuration Loading</a>
                </li>
                <li class="">
                    
    <a class="" href="../Hive-Registration/">Hive Registration</a>
                </li>
                <li class="">
                    
    <a class="" href="../Config-Management/">Config Management</a>
                </li>
                <li class="">
                    
    <a class="" href="../Docker-Integration/">Docker Integration</a>
                </li>
                <li class="">
                    
    <a class="" href="../Troubleshooting/">Troubleshooting</a>
                </li>
                <li class="">
                    
    <a class="" href="../FAQs/">FAQs</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Sources</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../sources/AvroFileSource/">Avro files</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/CopySource/">File copy</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/QueryBasedSource/">Query based</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/RestApiSource/">Rest Api</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/GoogleAnalyticsSource/">Google Analytics</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/GoogleDriveSource/">Google Drive</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/GoogleWebmaster/">Google Webmaster</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/HadoopTextInputSource/">Hadoop Text Input</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/HelloWorldSource/">Hello World</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/HiveAvroToOrcSource/">Hive Avro-to-ORC</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/HivePurgerSource/">Hive compliance purging</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/SimpleJsonSource/">JSON</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/KafkaSource/">Kafka</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/MySQLSource/">MySQL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/OracleSource/">Oracle</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/SalesforceSource/">Salesforce</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/SftpSource/">SFTP</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/SqlServerSource/">SQL Server</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/TeradataSource/">Teradata</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sources/WikipediaSource/">Wikipedia</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Sinks (Writers)</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../sinks/AvroHdfsDataWriter/">Avro HDFS</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/ParquetHdfsDataWriter/">Parquet HDFS</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/SimpleBytesWriter/">HDFS Byte array</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/ConsoleWriter/">Console</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/CouchbaseWriter/">Couchbase</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/Http/">HTTP</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/Gobblin-JDBC-Writer/">JDBC</a>
                </li>
                <li class="">
                    
    <a class="" href="../../sinks/Kafka/">Kafka</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Gobblin Adaptors</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../adaptors/Gobblin-Distcp/">Gobblin Distcp</a>
                </li>
                <li class="">
                    
    <a class="" href="../../adaptors/Hive-Avro-To-ORC-Converter/">Hive Avro-To-Orc Converter</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Case Studies</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../case-studies/Kafka-HDFS-Ingestion/">Kafka-HDFS Ingestion</a>
                </li>
                <li class="">
                    
    <a class="" href="../../case-studies/Publishing-Data-to-S3/">Publishing Data to S3</a>
                </li>
                <li class="">
                    
    <a class="" href="../../case-studies/Writing-ORC-Data/">Writing ORC Data</a>
                </li>
                <li class="">
                    
    <a class="" href="../../case-studies/Hive-Distcp/">Hive Distcp</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Gobblin Data Management</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../data-management/Gobblin-Retention/">Retention</a>
                </li>
                <li class="">
                    
    <a class="" href="../../data-management/DistcpNgEvents/">Distcp-NG events</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Gobblin Metrics</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../metrics/Gobblin-Metrics/">Quick Start</a>
                </li>
                <li class="">
                    
    <a class="" href="../../metrics/Existing-Reporters/">Existing Reporters</a>
                </li>
                <li class="">
                    
    <a class="" href="../../metrics/Metrics-for-Gobblin-ETL/">Metrics for Gobblin ETL</a>
                </li>
                <li class="">
                    
    <a class="" href="../../metrics/Gobblin-Metrics-Architecture/">Gobblin Metrics Architecture</a>
                </li>
                <li class="">
                    
    <a class="" href="../../metrics/Implementing-New-Reporters/">Implementing New Reporters</a>
                </li>
                <li class="">
                    
    <a class="" href="../../metrics/Gobblin-Metrics-Performance/">Gobblin Metrics Performance</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Developer Guide</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../developer-guide/Customization-for-New-Source/">Customization for New Source</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/Customization-for-Converter-and-Operator/">Customization for Converter and Operator</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/CodingStyle/">Code Style Guide</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/Gobblin-Compliance-Design/">Gobblin Compliance Design</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/IDE-setup/">IDE setup</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/Monitoring-Design/">Monitoring Design</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/Documentation-Architecture/">Documentation Architecture</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/Contributing/">Contributing</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/GobblinModules/">Gobblin Modules</a>
                </li>
                <li class="">
                    
    <a class="" href="../../developer-guide/HighLevelConsumer/">High Level Consumer</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Project</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../project/Feature-List/">Feature List</a>
                </li>
                <li class="">
                    
    <a class="" href="/people">Contributors and Team</a>
                </li>
                <li class="">
                    
    <a class="" href="../../project/Talks-and-Tech-Blogs/">Talks and Tech Blog Posts</a>
                </li>
                <li class="">
                    
    <a class="" href="../../project/Posts/">Posts</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Miscellaneous</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../miscellaneous/Camus-to-Gobblin-Migration/">Camus to Gobblin Migration</a>
                </li>
                <li class="">
                    
    <a class="" href="../../miscellaneous/Exactly-Once-Support/">Exactly Once Support</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Apache Gobblin</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>User Guide &raquo;</li>
        
      
    
    <li>Compaction</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/apache/incubator-gobblin/edit/master/docs/user-guide/Compaction.md"> Edit on Gobblin</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="table-of-contents">Table of Contents</h2>
<div class="toc">
<ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#mapreduce-compactor">MapReduce Compactor</a><ul>
<li><a href="#example-use-case">Example Use Case</a></li>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#non-deduping-compaction-via-map-only-jobs">Non-deduping Compaction via Map-only Jobs</a></li>
<li><a href="#handling-late-records">Handling Late Records</a></li>
<li><a href="#verifying-data-completeness-before-compaction">Verifying Data Completeness Before Compaction</a></li>
</ul>
</li>
<li><a href="#hive-compactor">Hive Compactor</a><ul>
<li><a href="#basic-usage_1">Basic Usage</a><ul>
<li><a href="#global-config-properties-example-compactionproperties">Global Config Properties (example: compaction.properties)</a></li>
<li><a href="#job-config-properties-example-jobconftask1conf">Job Config Properties (example: jobconf/task1.conf)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#running-a-compaction-job">Running a Compaction Job</a></li>
</ul>
</div>
<p>Compaction can be used to post-process files pulled by Gobblin with certain semantics. Deduplication is one of the common reasons to do compaction, e.g., you may want to</p>
<ul>
<li>deduplicate on all fields of the records.</li>
<li>deduplicate on key fields of the records, keep the one with the latest timestamp for records with the same key.</li>
</ul>
<p>This is because duplicates can be generated for multiple reasons including both intended and unintended:</p>
<ul>
<li>For ingestion from data sources with mutable records (e.g., relational databases), instead of ingesting a full snapshot of a table every time, one may wish to ingest only the records that were changed since the previous run (i.e., delta records), and merge these delta records with previously generated snapshots in a compaction. In this case, for records with the same primary key, the one with the latest timestamp should be kept.</li>
<li>The data source you ingest from may have duplicate records, e.g., if you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during the replication. In some data sources duplicate records may also be produced by the data producer.</li>
<li>In rare circumstances, Gobblin may pull the same data twice, thus creating duplicate records. This may happen if Gobblin publishes the data successfully, but for some reason fails to persist the checkpoints (watermarks) into the state store.</li>
</ul>
<p>Gobblin provides two compactors out-of-the-box, a MapReduce compactor and a Hive compactor.</p>
<h1 id="mapreduce-compactor">MapReduce Compactor</h1>
<p>The MapReduce compactor can be used to deduplicate on all or certain fields of the records. For duplicate records, one of them will be preserved; there is no guarantee which one will be preserved.</p>
<p>A use case of MapReduce Compactor is for Kafka records deduplication. We will use the following example use case to explain the MapReduce Compactor.</p>
<h2 id="example-use-case">Example Use Case</h2>
<p>Suppose we ingest data from a Kafka broker, and we would like to publish the data by hour and by day, both of which are deduplicated:</p>
<ul>
<li>Data in the Kafka broker is first ingested into an <code>hourly_staging</code> folder, e.g., <code>/data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29/08...</code></li>
<li>A compaction with deduplication runs hourly, consumes data in <code>hourly_staging</code> and publish data into <code>hourly</code>, e.g., <code>/data/kafka_topics/PageViewEvent/hourly/2015/10/29/08...</code></li>
<li>A non-deduping compaction runs daily, consumes data in <code>hourly</code> and publish data into <code>daily</code>, e.g., <code>/data/kafka_topics/PageViewEvent/daily/2015/10/29...</code></li>
</ul>
<h2 id="basic-usage">Basic Usage</h2>
<p><code>MRCompactor.compact()</code> is the entry point for MapReduce-based compaction. The compaction unit is <a href="https://github.com/apache/incubator-gobblin/blob/master/gobblin-compaction/src/main/java/org/apache/gobblin/compaction/dataset/Dataset.java"><code>Dataset</code></a>. <code>MRCompactor</code> uses a <a href="https://github.com/apache/incubator-gobblin/blob/master/gobblin-compaction/src/main/java/org/apache/gobblin/compaction/dataset/DatasetsFinder.java"><code>DatasetsFinder</code></a> to find all datasets eligible for compaction. Implementations of <code>DatasetsFinder</code> include <a href="https://github.com/apache/incubator-gobblin/blob/master/gobblin-compaction/src/main/java/org/apache/gobblin/compaction/dataset/SimpleDatasetsFinder.java"><code>SimpleDatasetsFinder</code></a> and <a href="https://github.com/apache/incubator-gobblin/blob/master/gobblin-compaction/src/main/java/org/apache/gobblin/compaction/dataset/TimeBasedSubDirDatasetsFinder.java"><code>TimeBasedSubDirDatasetsFinder</code></a>.</p>
<p>In the above example use case, for hourly compaction, each dataset contains an hour's data in the <code>hourly_staging</code> folder, e.g., <code>/data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29/08</code>; for daily compaction, each dataset contains 24 hourly folder of a day, e.g., <code>/data/kafka_topics/PageViewEvent/hourly/2015/10/29</code>. In hourly compaction, you may use the following config properties:</p>
<pre><code>compaction.datasets.finder=org.apache.gobblin.compaction.dataset.TimeBasedSubDirDatasetsFinder
compaction.input.dir=/data/kafka_topics
compaction.dest.dir=/data/kafka_topics
compaction.input.subdir=hourly_staging
compaction.dest.subdir=hourly
compaction.folder.pattern=YYYY/MM/dd
compaction.timebased.max.time.ago=3h
compaction.timebased.min.time.ago=1h
compaction.jobprops.creator.class=org.apache.gobblin.compaction.mapreduce.MRCompactorTimeBasedJobPropCreator
compaction.job.runner.class=org.apache.gobblin.compaction.mapreduce.avro.MRCompactorAvroKeyDedupJobRunner (if your data is Avro)
</code></pre>

<p>If your data format is not Avro, you can implement a different job runner class for deduplicating your data format. </p>
<h2 id="non-deduping-compaction-via-map-only-jobs">Non-deduping Compaction via Map-only Jobs</h2>
<p>There are two types of Non-deduping compaction.</p>
<ul>
<li><strong>Type 1</strong>: deduplication is not needed, for example you simply want to consolidate files in 24 hourly folders into a single daily folder.</li>
<li><strong>Type 2</strong>: deduplication is needed, i.e., the published data should not contain duplicates, but the input data are already deduplicated. The daily compaction in the above example use case is of this type.</li>
</ul>
<p>Property <code>compaction.input.deduplicated</code> specifies whether the input data are deduplicated (default is false), and property <code>compaction.output.deduplicated</code> specifies whether the output data should be deduplicated (default is true). For type 1 deduplication, set both to false. For type 2 deduplication, set both to true.</p>
<p>The reason these two types of compaction need to be separated is because of late data handling, which we will explain next.</p>
<h2 id="handling-late-records">Handling Late Records</h2>
<p>Late records are records that arrived at a folder after compaction on this folder has started. We explain how Gobblin handles late records using the following example.</p>
<p>In this use case, both hourly compaction and daily compaction need a mechanism to handle late records. For hourly compaction, late records are records that arrived at an <code>hourly_staging</code> folder after the hourly compaction of that folder has started. It is similar for daily compaction.</p>
<p><strong>Compaction with Deduplication</strong></p>
<p>For a compaction with deduplication (i.e., hourly compaction in the above use case), there are two options to deal with late data:</p>
<ul>
<li><strong>Option 1</strong>: if there are late data, re-do the compaction. For example, you may run the hourly compaction multiple times per hour. The first run will do the normal compaction, and in each subsequent run, if it detects late data in a folder, it will re-do compaction for that folder.</li>
</ul>
<p>To do so, set <code>compaction.job.overwrite.output.dir=true</code> and <code>compaction.recompact.from.input.for.late.data=true</code>.</p>
<p>Please note the following when you use this option: (1) this means that your already-published data will be re-published if late data are detected; (2) this is potentially dangerous if your input folders have short retention periods. For example, suppose <code>hourly_staging</code> folders have a 2-day retention period, i.e., folder <code>/data/kafka_topics/PageViewEvent/hourly_staging/2015/10/29</code> will be deleted on 2015/10/31. If, after 2015/10/31, new data arrived at this folder and you re-compact this folder and publish the data to <code>hourly</code>, all original data will be gone. To avoid this problem you may set <code>compaction.timebased.max.time.ago=2d</code> so that compaction will not be performed on a folder more than 2 days ago. However, this means that if a late record is late for more than 2 days, it will never be published into <code>hourly</code>.</p>
<ul>
<li><strong>Option 2</strong>: (this is the default option) if there are late data, copy the late data into a <code>[output_subdir]/_late</code> folder, e.g., for hourly compaction, late data in <code>hourly_staging</code> will be copied to <code>hourly_late</code> folders, e.g., <code>/data/kafka_topics/PageViewEvent/hourly_late/2015/10/29...</code>. </li>
</ul>
<p>If re-compaction is not necessary, this is all you need to do. If re-compaction is needed, you may schedule or manually invoke a re-compaction job which will re-compact by consuming data in both <code>hourly</code> and <code>hourly_late</code>. For this job, you need to set <code>compaction.job.overwrite.output.dir=true</code> and <code>compaction.recompact.from.dest.paths=true</code>.</p>
<p>Note that this re-compaction is different from the re-compaction in Option 1: this re-compaction consumes data in output folders (i.e., <code>hourly</code>) whereas the re-compaction in Option 1 consumes data in input folders (i.e., <code>hourly_staging</code>).</p>
<p><strong>Compaction without Deduplication</strong></p>
<p>For a compaction without deduplication, if it is type 2, the same two options above apply. If it is type 1, late data will simply be copied to the output folder.</p>
<p><strong>How to Determine if a Data File is Late</strong></p>
<p>Every time a compaction finishes (except the case below), Gobblin will create a file named <code>_COMPACTION_COMPLETE</code> in the compaction output folder. This file contains the timestamp of when the compaction job starts. All files in the input folder with earlier modification timestamps have been compacted. Next time the compaction runs, files in the input folder with later timestamps are considered late data.</p>
<p>The <code>_COMPACTION_COMPLETE</code> file will be only be created if it is a regular compaction that consumes input data (including compaction jobs that just copy late data to the output folder or the <code>[output_subdir]/_late</code> folder without launching an MR job). It will not be created if it is a re-compaction that consumes output data. This is because whether a file in the input folder is a late file depends on whether it has been compacted or moved into the output folder, which is not affected by a re-compaction that consumes output data.</p>
<p>One way of reducing the chance of seeing late records is to verify data completeness before running compaction, which will be explained next.</p>
<h2 id="verifying-data-completeness-before-compaction">Verifying Data Completeness Before Compaction</h2>
<p>Besides aborting the compaction job for a dataset if new data in the input folder is found, another way to reduce the chance of seeing late events is to verify the completeness of input data before running compaction. To do so, set <code>compaction.completeness.verification.enabled=true</code>, extend <code>DataCompletenessVerifier.AbstractRunner</code> and put in your verification logic, and pass it via <code>compaction.completeness.verification.class</code>.</p>
<p>When data completeness verification is enabled, <code>MRCompactor</code> will verify data completeness for the input datasets, and meanwhile speculatively start the compaction MR jobs. When the compaction MR job for a dataset finishes, if the completeness of the dataset is verified, its compacted data will be published, otherwise it is discarded, and the compaction MR job for this dataset will be launched again with a reduced priority.</p>
<p>It is possible to control which topics should or should not be verified via <code>compaction.completeness.verification.whitelist</code> and <code>compaction.completeness.verification.blacklist</code>. It is also possible to set a timeout for data completeness verification via <code>compaction.completeness.verification.timeout.minutes</code>. A dataset whose completeness verification timed out can be configured to be either compacted anyway or not compacted.</p>
<h1 id="hive-compactor">Hive Compactor</h1>
<p>The Hive compactor can be used to merge a snapshot with one or multiple deltas. It assumes the snapshot and the deltas meet the following requirements:</p>
<ol>
<li>Snapshot and all deltas are in Avro format.</li>
<li>Snapshot and all deltas have the same primary key attributes (they do not need to have the same schema).</li>
<li>Snapshot is pulled earlier than all deltas. Therefore if a key appears in both snapshot and deltas, the one in the snapshot should be discarded.</li>
<li>The deltas are pulled one after another, and ordered in ascending order of pull time. If a key appears in both the ith delta and the jth delta (i &lt; j), the one in the jth delta survives.</li>
</ol>
<p>The merged data will be written to the HDFS directory specified in <code>output.datalocation</code>, as one or more Avro files. The schema of the output data will be the same as the schema of the last delta (which is the last pulled data and thus has the latest schema).</p>
<p>In the near future we also plan to support selecting records by timestamps (rather than which file they appear). This is useful if the snapshot and the deltas are pulled in parallel, where if a key has multiple occurrences we should keep the one with the latest timestamp.</p>
<p>Note that since delta tables don't have information of deleted records, such information is only available the next time the full snapshot is pulled.</p>
<h2 id="basic-usage_1">Basic Usage</h2>
<p>A Hive Compactor job consists of one global configuration file which refers to one or more job configuration(s).  </p>
<h3 id="global-config-properties-example-compactionproperties">Global Config Properties (example: compaction.properties)</h3>
<p>(1) Required:</p>
<ul>
<li><em><strong>compaction.config.dir</strong></em></li>
</ul>
<p>This is the the compaction jobconfig directory. Each file in this directory should be a jobconfig file (described in the next section).</p>
<p>(2) Optional:</p>
<ul>
<li><em><strong>hadoop.configfile.</strong>*</em></li>
</ul>
<p>Hadoop configuration files that should be loaded
(e.g., hadoop.configfile.coresite.xml=/export/apps/hadoop/latest/etc/hadoop/core-site.xml)</p>
<ul>
<li><em><strong>hdfs.uri</strong></em></li>
</ul>
<p>If property <code>fs.defaultFS</code> (or <code>fs.default.name</code>) is specified in the hadoop config file, then this property is not needed. However, if it is specified, it will override <code>fs.defaultFS</code> (or <code>fs.default.name</code>).</p>
<p>If <code>fs.defaultFS</code> or <code>fs.default.name</code> is not specified in the hadoop config file, and this property is also not specified, then the default value "hdfs://localhost:9000" will be used.</p>
<ul>
<li><em><strong>hiveserver.version</strong></em> (default: 2)</li>
</ul>
<p>Either 1 or 2.</p>
<ul>
<li>
<p><em><strong>hiveserver.connection.string</strong></em></p>
</li>
<li>
<p><em><strong>hiveserver.url</strong></em></p>
</li>
<li>
<p><em><strong>hiveserver.user</strong></em> (default: "")</p>
</li>
<li>
<p><em><strong>hiveserver.password</strong></em> (default: "")</p>
</li>
</ul>
<p>If <code>hiveserver.connection.string</code> is specified, it will be used to connect to hiveserver.</p>
<p>If <code>hiveserver.connection.string</code> is not specified but <code>hiveserver.url</code> is specified, then it uses (<code>hiveserver.url</code>, <code>hiveserver.user</code>, <code>hiveserver.password</code>) to connect to hiveserver.</p>
<p>If neither <code>hiveserver.connection.string</code> nor <code>hiveserver.url</code> is specified, then embedded hiveserver will be used (i.e., <code>jdbc:hive://</code> if <code>hiveserver.version=1</code>, <code>jdbc:hive2://</code> if <code>hiveserver.version=2</code>)</p>
<ul>
<li><em><strong>hivesite.dir</strong></em></li>
</ul>
<p>Directory that contains hive-site.xml, if hive-site.xml should be loaded.</p>
<ul>
<li><em><strong>hive.</strong>*</em></li>
</ul>
<p>Any hive config property. (e.g., <code>hive.join.cache.size</code>). If specified, it will override the corresponding property in hive-site.xml.</p>
<h3 id="job-config-properties-example-jobconftask1conf">Job Config Properties (example: jobconf/task1.conf)</h3>
<p>(1) Required:</p>
<ul>
<li><em><strong>snapshot.pkey</strong></em></li>
</ul>
<p>comma separated primary key attributes of the snapshot table</p>
<ul>
<li><em><strong>snapshot.datalocation</strong></em></li>
</ul>
<p>snapshot data directory in HDFS</p>
<ul>
<li><em><strong>delta.i.pkey</strong></em> (i = 1, 2...)</li>
</ul>
<p>the primary key of ith delta table
(the primary key of snapshot and all deltas should be the same)</p>
<ul>
<li><em><strong>delta.i.datalocation</strong></em> (i = 1, 2...)</li>
</ul>
<p>ith delta table's data directory in HDFS</p>
<ul>
<li><em><strong>output.datalocation</strong></em></li>
</ul>
<p>the HDFS data directory for the output
(make sure you have write permission on this directory)</p>
<p>(2) Optional:</p>
<ul>
<li><em><strong>snapshot.name</strong></em> (default: randomly generated name)</li>
</ul>
<p>prefix name of the snapshot table. The table name will be snapshot.name + random suffix</p>
<ul>
<li><em><strong>snapshot.schemalocation</strong></em></li>
</ul>
<p>snapshot table's schema location in HDFS. If not specified, schema will be extracted from the data.</p>
<ul>
<li><em><strong>delta.i.name</strong></em> (default: randomly generated name)</li>
</ul>
<p>prefix name of the ith delta table. The table name will be delta.i.name + random suffix</p>
<ul>
<li><em><strong>delta.i.schemalocation</strong></em></li>
</ul>
<p>ith delta table's schema location in HDFS. If not specified, schema will be extracted from the data.</p>
<ul>
<li><em><strong>output.name</strong></em> (default: randomly generated name)</li>
</ul>
<p>prefix name of the output table. The table name will be output.name + random suffix</p>
<ul>
<li><em><strong>hive.db.name</strong></em> (default: default)</li>
</ul>
<p>the database name to be used. This database should already exist, and you should have write permission on it.</p>
<ul>
<li><em><strong>hive.queue.name</strong></em> (default: default)</li>
</ul>
<p>queue name to be used.</p>
<ul>
<li><em><strong>hive.use.mapjoin</strong></em> (default: if not specified in the global config file, then false)</li>
</ul>
<p>whether map-side join should be turned on. If specified both in this property and in the global config file (hive.*), this property takes precedences. </p>
<ul>
<li><em><strong>hive.mapjoin.smalltable.filesize</strong></em> (default: if not specified in the global config file, then use Hive's default value)</li>
</ul>
<p>if hive.use.mapjoin = true, mapjoin will be used if the small table size is smaller than hive.mapjoin.smalltable.filesize (in bytes).
If specified both in this property and in the global config file (hive.*), this property takes precedences. </p>
<ul>
<li><em><strong>hive.tmpschema.dir</strong></em> (default: the parent dir of the data location dir where the data is used to extract the schema)</li>
</ul>
<p>If we need to extract schema from data, this dir is for the extracted schema.
Note that if you do not have write permission on the default dir, you must specify this property as a dir where you do have write permission.</p>
<ul>
<li><em><strong>snapshot.copydata</strong></em> (default: false)</li>
</ul>
<p>Set to true if you don't want to (or are unable to) create external table on snapshot.datalocation. A copy of the snapshot data will be created in <code>hive.tmpdata.dir</code>, and will be removed after the compaction.</p>
<p>This property should be set to true if either of the following two situations applies:</p>
<p>(i) You don't have write permission to <code>snapshot.datalocation</code>. If so, once you create an external table on <code>snapshot.datalocation</code>, you may not be able to drop it. This is a Hive bug and for more information, see <a href="https://issues.apache.org/jira/browse/HIVE-9020">this page</a>, which includes a Hive patch for the bug.</p>
<p>(ii) You want to use a certain subset of files in <code>snapshot.datalocation</code> (e.g., <code>snapshot.datalocation</code> contains both .csv and .avro files but you only want to use .avro files)</p>
<ul>
<li><em><strong>delta.i.copydata</strong></em> (i = 1, 2...) (default: false)</li>
</ul>
<p>Similar as <code>snapshot.copydata</code></p>
<ul>
<li><em><strong>hive.tmpdata.dir</strong></em> (default: "/")</li>
</ul>
<p>If <code>snapshot.copydata</code> = true or <code>delta.i.copydata</code> = true, the data will be copied to this dir. You should have write permission to this dir.</p>
<ul>
<li><em><strong>snapshot.dataformat.extension.name</strong></em> (default: "")</li>
</ul>
<p>If <code>snapshot.copydata</code> = true, then only those data files whose extension is <code>snapshot.dataformat</code> will be moved to <code>hive.tmpdata.dir</code>.</p>
<ul>
<li><em><strong>delta.i.dataformat.extension.name</strong></em> (default: "")</li>
</ul>
<p>Similar as <code>snapshot.dataformat.extension.name</code>. </p>
<ul>
<li><em><strong>mapreduce.job.num.reducers</strong></em></li>
</ul>
<p>Number of reducers for the job.</p>
<ul>
<li><em><strong>timing.file</strong></em> (default: time.txt)</li>
</ul>
<p>A file where the running time of each compaction job is printed.</p>
<h1 id="running-a-compaction-job">Running a Compaction Job</h1>
<p>Both the MapReduce and Hive-based compaction configurations can be executed with <code>bin/gobblin-compaction.sh</code> .</p>
<p>The usage is as follows:</p>
<pre><code>gobblin-compaction.sh [OPTION] --type &lt;compaction type: hive or mr&gt; --conf &lt;compaction configuration file&gt;
Where OPTION can be:
  --projectversion &lt;version&gt;    Gobblin version to be used. If set, overrides the distribution build version
  --logdir &lt;log dir&gt;            Gobblin's log directory: if not set, taken from ${GOBBLIN_LOG_DIR} if present. 
  --help                        Display this help and exit
</code></pre>

<p>Example:</p>
<pre><code>cd gobblin-dist
bin/gobblin-compaction.sh --type hive --conf compaction.properties
</code></pre>

<p>The log4j configuration is read from <code>conf/log4j-compaction.xml</code>.
Please note that in case of a Hive-compaction for drop table queries (<code>DROP TABLE IF EXISTS &lt;tablename&gt;</code>), the Hive JDBC client will throw <code>NoSuchObjectException</code> if the table doesn't exist. This is normal and such exceptions should be ignored.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../State-Management-and-Watermarks/" class="btn btn-neutral float-right" title="State Management and Watermarks">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Gobblin-on-Yarn/" class="btn btn-neutral" title="Gobblin on Yarn"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../Gobblin-on-Yarn/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../State-Management-and-Watermarks/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../js/extra.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
